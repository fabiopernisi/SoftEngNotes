\newpage
\section{Floating-point arithmetic}

\subsection{Real Numbers}

\subsubsection{How do we represent non-integers?}

Keeping in mind:
\begin{itemize}
    \item If we consider $n$ bits of memory,
    \begin{itemize}
        \item their values can take $2^n$ combinations
        \item so we can represent $2^n$ numbers at best with those $n$ bits
    \end{itemize}
    \item We have a finite amount of memory,
    \begin{itemize}
        \item so we cannot represent all real numbers
    \end{itemize}
    \item We (typically) want fast operations,
    \begin{itemize}
        \item so (ideally) we need hardware to perform them.
        \item Hardware has tight limits on the number of logic gates available
        \item meaning we use very few bits (say 16, 32 or 64, like for integers)
        \item \ldots further restricting how many real numbers we can represent
    \end{itemize}
\end{itemize}

\subsubsection{Practical limitations}

\begin{itemize}
  \item Integer are restricted in one way:
  \begin{itemize}
    \item their range (e.g. [\texttt{INT\_MIN}, \texttt{INT\_MAX}])
  \end{itemize}
  \item Reals are restricted in two ways:
  \begin{itemize}
    \item their range (e.g. [$-10^{308}$, $10^{308}$])
    \item the number of reals we can represent in that range (e.g. \{..., $0, 10^{-200}, 2 \times 10^{-200}, ...$\})\\
    i.e. their precision
  \end{itemize}
\end{itemize}

\subsection{Fixed-point arithmetic}

\subsubsection{Decimal example}

Instead of computing money values in \texteuro, we could use c:

e.g.  29.99\texteuro{} = 2999c\\
then use integer operations.

\begin{itemize}
  \item This is fixed-point arithmetic
  \item specifically, with 2 decimal places reserved for the fractional part.
\end{itemize}

% Note: LaTeX does not have a built-in command for the Euro symbol by default. 
% The \texteuro command used in this transcription can come from a package like "eurosym" 
% which provides the Euro currency symbol. If that package is not used, the Euro symbol 
% will not appear correctly in the resulting document unless an alternative method is implemented.


% Since the LaTeX document setup (such as \documentclass) is not requested, we begin directly with the content of the slide.
% Note that the LaTeX code provided here is not a standalone document, and a proper preamble would be needed for compilation.
% Replace images or non-textual elements with [TBD].

If $+, -, \times, /$ are the elementary integer operations:

\begin{itemize}
	\item $euro\_to\_cent(e) \mathrel{::=} e \times 100$
	\begin{itemize}
		\item $euro\_to\_cent (5 \texteuro) = 500 \textcent$
	\end{itemize}
	\item $cent\_to\_euro(a) \mathrel{::=} a/100$
	\begin{itemize}
		\item $cent\_to\_euro(700 \textcent) = 7 \texteuro$
	\end{itemize}
	\item $cent\_add(a, b) \mathrel{::=} a+b$
	\begin{itemize}
		\item $cent\_add(700 \textcent, 500 \textcent) = 1200 \textcent$
	\end{itemize}
	\item $cent\_sub(a, b) \mathrel{::=} a-b$
	\begin{itemize}
		\item $cent\_sub(700 \textcent, 500 \textcent) = 200 \textcent$
	\end{itemize}
	\item $cent\_mul(a, b) \mathrel{::=} (a \times b)/100$
	\begin{itemize}
		\item $5 \texteuro \times 7: cent\_mul(500 \textcent, 700 \textcent) = 500 \times 700 / 100 = 3500 \textcent$
	\end{itemize}
	\item $cent\_div(a, b) \mathrel{::=} (a \times 100)/b$
	\begin{itemize}
		\item $8 \texteuro / 4: cent\_div(800 \textcent, 400 \textcent) = (800 \times 100) / 400 = 200 \textcent$
	\end{itemize}
\end{itemize}

\subsubsection{Binary fixed-point arithmetic}

\begin{itemize}
	\item There is no universally accepted standard for fixed-point arithmetic
	\item But there is no real need for one:
	\begin{itemize}
		\item Only two parameters:
		\begin{description}
			\item[$n$:] total number of bits
			\item[$p$:] number of bits after the decimal point
		\end{description}
		\item All the operations are just integer operations
		\begin{itemize}
			\item For $mul$ and $div$, two integer operations each
		\end{itemize}
	\end{itemize}
\end{itemize}

\paragraph{Binary example}
\begin{itemize}
    \item A 64-bit integer can be used for fixed-point arithmetic by dividing it into two parts: a 32-bit integer part and a 32-bit fractional part.
    \item The conversion and arithmetic operations are defined as follows:
\end{itemize}


\begin{verbatim}
i64_to_fix(e) := e × 2^32
fix_to_i64(a) := a / 2^32
fix_add(a, b) := a + b
fix_sub(a, b) := a - b
fix_mul(a, b) := (a × b) / 2^32
fix_div(a, b) := (a × 2^32) / b
\end{verbatim}

The function $i64_to_fix()$ is taking an integer $e$ of type $i64$ (which is a 64-bit integer) and converting it into a fixed-point representation of type \textit{fix}. The multiplication by $2^32$ effectively shifts the integer value $e$ into the higher 32 bits of the 64-bit representation, leaving the lower 32 bits for the fractional part. This way,  it is possible to perform fixed-point arithmetic with fix types while maintaining precision for the fractional component of the values.\\

These operations are implemented using integer arithmetic, providing precise control over the scaling and range of the values.

\paragraph{Implementation in C}

Fixed-point arithmetic operations can be implemented in C using a typedef for a 64-bit integer to represent fixed-point numbers. The integer part occupies the upper 32 bits and the fractional part occupies the lower 32 bits. 

\begin{itemize}
    \item \textbf{Conversion to Fixed-Point:} An integer is converted to fixed-point format by shifting it left by 32 bits, moving the integer part to the upper 32 bits.
    \item \textbf{Conversion from Fixed-Point:} A fixed-point number is converted back to an integer by shifting it right by 32 bits, discarding the fractional part.
    \item \textbf{Addition and Subtraction:} These operations are performed directly on the fixed-point representations, with the result being in fixed-point format.
    \item \textbf{Multiplication:} Multiplication requires casting to a wider integer (128-bit) to accommodate the intermediate result before shifting right by 32 bits to maintain the correct scale.
    \item \textbf{Division:} Division is handled by first shifting the dividend left by 32 bits (to align the integer part correctly) before performing the division.
\end{itemize}

Each operation maintains the scale of fixed-point representation and ensures that the integer and fractional parts are processed correctly. Care is taken during multiplication and division to avoid overflow and to maintain precision.


\paragraph{Fixed-point arithmetic, Pros}
\begin{itemize}
    \item fast, no need for extra hardware
    \item easy to understand and study (predictible):
    \begin{itemize}
        \item uniform absolute precision (e.g. $2^{-32}$ over whole range)
    \end{itemize}
\end{itemize}

% The following is an enumeration list for "Cons" of fixed-point arithmetic
\paragraph{Fixed-point arithmetic, Cons}
\begin{itemize}
    \item limited range (e.g. $[-2147483648.999, 2147483647.999]$)
    \item limited precision (e.g. $2^{-32} \approx 0.000000002328$)
\end{itemize}

% Potential improvements that could be made to fixed-point arithmetic
\textbf{Possible improvements:}
\begin{itemize}
    \item larger range
    \item better absolute precision around zero
    \item lower absolute precision for big numbers
\end{itemize}

% The next line would suggest the topic of the following section or lecture part.
\subsection{Floating-point Arithmetic}


% Transcription of the image content into LaTeX
% Explanatory comments and LaTeX commands are included as % comments for clarity

\subsubsection{Scientific notation}

Take the number $-2147483648.999$: 
\begin{align*}
-2147483648.999 &= -2.147483648999 \times 10^{9} \\
&= -2.147483648999e+9
\end{align*}

Similarly, take the number $0.0000000002328$:
\begin{align*}
0.0000000002328 &= 2.328 \times 10^{-10} \\
&= 2.328e-10
\end{align*}

\subsubsection{Scientific Notation}
Scientific notation is a way to express numbers that are too large or too small to be conveniently written in decimal form. It is commonly used in calculations and by scientists, mathematicians, and engineers.

\begin{itemize}
    \item In scientific notation, numbers are written as a product of two numbers: a coefficient and \(10\) raised to a power, for example, \(-2.147483648999 \times 10^9\).
    \item The coefficient must be a number between \(1\) and \(9\), followed by a decimal point and the rest of the significant digits.
    \item The exponent is written as a power of \(10\), indicating how many places the decimal point should be moved to convert the number into a standard decimal number.
\end{itemize}

\subsubsection{Binary Floating-Point Numbers}
In computing, binary floating-point numbers are a way to represent real numbers in a binary system, typically using three components: the sign, exponent, and mantissa.

\begin{itemize}
    \item The sign determines if the number is positive or negative.
    \item The mantissa, or significand, is composed of the significant digits of the number.
    \item The exponent indicates where the decimal (or binary) point is placed relative to the beginning of the mantissa.
\end{itemize}

Binary floating-point numbers allow for a wide range of values to be represented in a standardized way, which is why they are commonly used in computer systems.


\subsubsection{Pros and Cons of Binary Floating-Point Numbers}
\textbf{Pros:}
\begin{itemize}
    \item Efficient to process with modern hardware.
    \item Predictable precision and representation across different systems.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item Limited precision, which can lead to rounding errors in calculations.
    \item Limited range, meaning extremely large or small numbers may not be representable.
\end{itemize}


\subsubsection{Floating-point standard}
\begin{itemize}
    \item In 1985, the Institute of Electrical and Electronics Engineers publishes standard \#754 about floating-point arithmetic (IEEE-754)
    \item The standard \#754 defines the number of bits used for the sign, exponent, and mantissa. This standard ensures that floating-point operations behave consistently across different computing systems.
    \item Most hardware makers adopt the standard very quickly thereafter (Intel 80387, launched in 1987, is fully compliant)
    \item x86\_64 natively supports binary32 and binary64 formats
    \item AArch64 natively supports binary16, binary32 and binary64 formats
\end{itemize}

\begin{tabular}{lccc}
    component & binary16 & binary32 & binary64 \\
    \hline
    $\pm$ sign bit & 1 & 1 & 1 \\
    $mmmmm...$ mantissa bits & 10 & 23 & 52 \\
    $xxxx...$ exponent bits & 5 & 8 & 11 \\
    exponent range & -14..15 & -126...127 & -1022...1023 \\
\end{tabular}

\subsubsection{Precision}

\textbf{Absolute and Relative Precision}
\begin{itemize}
    \item \textbf{Absolute precision:} For a given \(x\), the smallest \( \varepsilon \) such that
    \[
        fl(x + \varepsilon) \neq fl(x)
    \]

    \item \textbf{Relative precision:} For a given \(x\),
    \[
        \varepsilon := \frac{\varepsilon}{x}
    \]
\end{itemize}

\subsubsection{Binary64 vs. Fixed-Point 32+32 Comparison}

\textbf{Precision at Different Scales}
\begin{itemize}
    \item The fixed-point 32+32 format provides a consistent absolute precision across different magnitudes, beneficial for values around 1 or below.
    \item Floating-point binary64 format, based on IEEE-754, offers varying precision, with relative precision being consistent but absolute precision varying with the value's magnitude.
\end{itemize}

\textbf{Range}
\begin{itemize}
    \item Fixed-point 32+32 has a limited range up to \(|x| \leq 2.15 \times 10^9\), making it unsuitable for very large values.
    \item Binary64 supports a significantly larger range up to \(|x| \leq 1.80 \times 10^{308}\), accommodating both very large and very small values.
\end{itemize}

\begin{tabular}{l|cc|cc}
    \hline
     & \multicolumn{2}{c|}{fixed-point 32+32} & \multicolumn{2}{c}{floating-point binary64} \\
     \hline
     & absolute & relative & absolute & relative \\
    \hline
    precision at \(10^{-9}\) & \(2.33 \times 10^{-10}\) & \(0.0233\) & \(2.07 \times 10^{-25}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{-6}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-5}\) & \(2.12 \times 10^{-22}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{-3}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-8}\) & \(2.17 \times 10^{-19}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(1\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-11}\) & \(2.22 \times 10^{-16}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{3}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-14}\) & \(1.14 \times 10^{-13}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{6}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-17}\) & \(1.16 \times 10^{-10}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{9}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-20}\) & \(1.19 \times 10^{-7}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{16}\) & \textcolor{red}{\textbf{X}} &  & \(2.00\) & \(2.22 \times 10^{-16}\) \\
    \hline
    range & \(|x| \leq 2.15 \times 10^{9}\) &  & \(|x| \leq 1.80 \times 10^{308}\) &  \\
    \hline
\end{tabular}

\subsubsection{The Floating-Point Number Line}

\begin{itemize}
    \item The floating-point number line is non-linear, with denser representation of numbers near zero and sparser as the magnitude increases.
    \item This reflects the constant relative precision of floating-point numbers.
\end{itemize}

\subsubsection{Programming Languages and IEEE-754 Standard}

\begin{itemize}
    \item Several programming languages mandate compliance with the IEEE-754 standard for floating-point arithmetic, ensuring consistency across computing platforms.
    \item Languages like C (since C99) and C++ (since C++03) offer types like \texttt{float} for binary32 and \texttt{double} for binary64.
    \item Modern languages such as Python and JavaScript also adhere to this standard, with Python using binary64 precision by default.
\end{itemize}


\begin{tabular}{lccc}
\textbf{language} & \textbf{since} & \textbf{binary32} & \textbf{binary64} \\
C & C99 & float & double \\
C++ & C++03 & float & double \\
Fortran & Fortran 2003 & real & double \\
Rust & & f32 & f64 \\
Python & & & \checkmark \\
JavaScript & & & \checkmark \\
\end{tabular}




\subsubsection{Inaccuracy in Base 10 and Base 2}
In floating-point arithmetic, inaccuracy can arise due to the way numbers are represented in base 10 and base 2. For example, fractions like \( \frac{1}{3} \) and \( \frac{2}{3} \) do not have exact representations in base 10 and result in repeating decimals. This inaccuracy is also present in binary (base 2), where numbers like 0.1 cannot be precisely represented.

\subsubsection{Numerical Instability}
Numerical instability refers to errors that can grow when applying mathematical operations, especially in iterative calculations or when dealing with very large or small numbers.\\

\textbf{Derivative Approximation}\\
A common operation in numerical analysis is the approximation of derivatives using finite differences:
\[ \frac{df(x)}{dx} \approx \frac{f(x + \delta) - f(x)}{\delta} \]
However, choosing a very small \( \delta \) can lead to significant inaccuracies due to the limited precision of floating-point numbers.

\subsubsection{Example of Numerical Instability}
Consider computing the derivative of \( f(x) = x \) at large values of \( x \) using a small \( \delta \). When \( x \) is much larger than \( \delta \), the subtraction \( (f(x + \delta) - f(x)) \) can result in loss of precision, and the computed derivative will deviate from the expected value of 1.

\subsubsection{What is happening?}
\begin{itemize}
    \item At $x = 10^{+5}$, we first compute $(10^{+5} + 10^{-6})$
    \begin{itemize}
        \item which is a big number, close to $10^{+5}$
        \item floating-point numbers have a good \textit{relative} accuracy everywhere, $\sim 2.22 \times 10^{-16}$
        \item but at $10^{+5}$, the \textit{absolute} accuracy is not great, $\sim 1.91 \times 10^{-6}$
        \item so the result of $(10^{+5} + 10^{-6})$ may be off by roughly $1.91 \times 10^{-6}$
    \end{itemize}
    \item We then subtract $10^{+5}$.
    \begin{itemize}
        \item If we were using exact arithmetic, we would get $10^{-6}$ exactly,
        \item but we are using floating-point arithmetic,
        \item so we get something close to $10^{-6}$...
        \item ... but potentially off by roughly $1.91 \times 10^{-6}$
    \end{itemize}
    \item We divide by $10^{-6}$,
    \begin{itemize}
        \item and get a number in $[1 - 1.91, 1 + 1.91]$
    \end{itemize}
\end{itemize}
The deviations we just discussed arise because floating-point numbers have more relative precision near zero, and less absolute precision as their magnitude increases.

Therefore,
\begin{itemize}
    \item floating-point accuracy is often great
    \item but some algorithms are \textbf{unstable}
    \item we need to be extremely careful before trusting floating-point results
\end{itemize}




\subsubsection{Never do exact comparisons}

\begin{verbatim}
>>> 0.1 + 0.2 == 0.3
False
\end{verbatim}

\begin{verbatim}
>>> 1.0 - 1e-16 == 1.0
True
\end{verbatim}

\subsubsection{So how do we do comparisons?} % Lecture slide heading

\begin{itemize}
  \item If exact comparisons are important, \textbf{do not use floating-point arithmetic}.
  \item If we care about speed and can tolerate some errors...
\end{itemize}

% Example of a proper way to do comparisons with tolerance in floating point arithmetic
\begin{verbatim}
>>> 0.1 + 0.2 == 0.3
False
\end{verbatim}

becomes

\begin{verbatim}
>>> tolerance = 1e-10
>>> abs( (0.1 + 0.2) - 0.3 ) < tolerance
True
\end{verbatim}

% Another example of a proper way to do comparisons with tolerance
\begin{verbatim}
>>> x = 0.0
\end{verbatim}

becomes

\begin{verbatim}
>>> x > -tolerance
\end{verbatim}



\subsection{Rounding Floating-Point Numbers}
When a real number cannot be represented exactly by a floating-point number, we approximate it by "rounding" to the nearest floating-point number. The IEEE-754 standard outlines several rounding modes to handle such cases:

\begin{itemize}
    \item \textbf{Round to Nearest, Ties to Even:} The default rounding mode which rounds to the nearest value; if equidistant, it rounds to the nearest even number in the floating-point representation.
    \item \textbf{Round to Nearest, Ties Away from Zero:} Rounds to the nearest value; if equidistant, it rounds away from zero.
    \item \textbf{Round Toward Zero:} Always rounds towards zero, truncating the extra digits.
    \item \textbf{Round Toward \(+\infty\):} Always rounds up.
    \item \textbf{Round Toward \(-\infty\):} Always rounds down.
\end{itemize}

\subsubsection{Determinism in Floating-Point Arithmetic}
While floating-point arithmetic may be inaccurate due to rounding, it is deterministic:

\begin{itemize}
    \item The result of most operations is precisely defined by the standard.
    \item We can predict the result of operations bit-for-bit, ensuring reproducible calculations.
\end{itemize}

\subsubsection{IEEE-754 Rounding Rules}
Let us denote by \(fl(x)\) the floating-point representation of the real number $x \in \mathbb{R}$.\\
The IEEE-754 standard mandates correct rounding as per the selected rounding mode for various operations:

\begin{itemize}
    \item \textbf{Addition, Subtraction:} \( x + y \) results in \( fl(x + y) \).
    \item \textbf{Multiplication, Division:} \( x / y \) results in \( fl(x / y) \).
    \item \textbf{Square Root:} \( \sqrt{x} \) results in \( fl(\sqrt{x}) \).
    \item \textbf{Fused Multiply-Add:} \( fma(x, y, z) \) results in \( fl(x \times y + z) \).
\end{itemize}






\subsubsection{Division example}

When executing

\[ z = x / y \]

\begin{itemize}
  \item we first take the floating-point numbers $x$ and $y$, and consider them as if the were (exact, infinite-precision) real numbers
  \item we compute the (exact, infinite-precision) real quotient $x / y$.
  \item we round the result according to the current \textbf{rounding mode}: fl($x / y$)
  \item we store the \textbf{rounded} floating-point value into $z$
\end{itemize}

% End of LaTeX code


% Note that this LaTeX code represents the textual content of the slide.
% Depending on the document class and preamble you choose to wrap this with,
% you may need to include packages such as amsmath for math typesetting.

\subsubsection{Expression example}
Consider the expression:
\[
\frac{y \times (x + 4.0)}{z - 3.0}
\]
In a floating-point computation, due to rounding errors, this expression gives:
\[
fl\left( fl(y \times fl(x + 4)) \div fl(z - 3) \right)
\]
where \( fl \) denotes the floating-point representation of a number, highlighting the sequential rounding that occurs at each step.

\subsubsection{About fused multiply-add}
The fused multiply-add operation is a common computation in many numerical algorithms. It combines multiplication and addition but must be treated with caution:

\textbf{Beware:} The fused multiply-add function \( fma(x, y, z) \) does not simply equate to \( x \times y + z \).

\textbf{Indeed:}
\begin{itemize}
    \item \( fma(x, y, z) \) computes \( fl(x \times y + z) \), providing a more accurate result by combining the operations into a single step to minimize rounding errors.
    \item However, separately computing \( x \times y + z \) results in \( fl(fl(x \times y) + z) \), where each operation introduces potential rounding errors.
\end{itemize}

\subsubsection{More floating-point non-identities}
Certain algebraic identities do not hold for floating-point numbers due to rounding errors and finite precision:
\begin{itemize}
    \item Associativity does not hold: \( x + (y + z) \neq (x + y) + z \).
    \item Distributivity does not hold: \( x \times (y + z) \neq x \times y + x \times z \).
\end{itemize}


The IEEE-754 standard defines the representation and behavior of floating-point numbers:

\begin{itemize}
    \item It mandates correct rounding for basic operations such as \( + \), \( - \), \( \times \), \( \div \), \( \sqrt{} \), and \( fma() \).
    \item However, it does not mandate correct rounding for more complex functions like trigonometric, hyperbolic, exponential, and logarithmic functions. This can lead to discrepancies in different implementations and requires careful handling.
\end{itemize}

\subsubsection{Floating-point and compilers}

\begin{itemize}
  \item C99 and C++03 mandate IEEE-754
  \item which in turn mandates correct rounding for $+, -, \times, \div, \sqrt{()}, \text{fma}()$.
  \item However, if we do not specify a C or C++ standard (e.g., \texttt{-std=c17} or \texttt{-std=c++20}), \\
  gcc and clang do not follow IEEE-754
  \begin{itemize}
    \item they will happily exploit associativity and distributivity
    \item they will replace $x \times y + z$ by fma$(x, y, z)$
  \end{itemize}
\end{itemize}

\subsubsection{Why does correct rounding matter?}

\begin{itemize}
  \item (generally) not because of accuracy
  \item but because for any real number $x$, there is exactly one correct rounding
  \item as a result, there is no ambiguity:
  \begin{itemize}
    \item given a set of floating-point numbers
    \item given any expression involving those numbers and $+, -, \times, \div, \sqrt{()}, \text{fma}()$
    \item there is exactly one correct answer
    \item which is precisely specified by IEEE-754, down to its bit representation
  \end{itemize}
\end{itemize}



% This LaTeX code is designed to replicate the text content of a presentation slide. 
% Direct LaTeX commands are used to format the structure of the text.
% Sections, items, and emphasis are arranged to reflect the visual layout of the slide.

\subsubsection{What happens without correct rounding?} % Section title
\textbf{Results can change when:} % Bold statement

\begin{itemize} % Unordered list of items
  \item we change architecture
  \item we change compiler
  \item we change the standard C library
  \item we change the version of the compiler
  \item we change the version of the standard C library
  \item we change our code (even a completely unrelated part)
\end{itemize}

\textbf{Note:} If we use $\sin$, $\cos$, $\log$, $\exp$, ..., which are not correctly rounded, 
then we are exposed to result \textit{changes} 
whenever we change the version of the standard C library 
(which could be dynamically linked!)

\subsection{Beyond floating-point Arithmetic} % Section title with greater emphasis

\subsubsection{Interval arithmetic} % Subsection title
Interval arithmetic is an approach to encompass the uncertainty and rounding errors in numerical computations.

\begin{itemize}
    \item Every real number \( x \in \mathbb{R} \) is represented by an interval \([l, u]\) using a pair of floating-point numbers, ensuring that \( x \) lies within this interval.
    \item This method employs rounding modes, specifically \textit{Round toward \( +\infty \)} and \textit{Round toward \( -\infty \)}, to compute the accurate interval for each operation.
\end{itemize}

\textbf{Advantages:}
\begin{itemize}
    \item It is a fast method.
    \item It provides knowledge of how accurate a result is.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item The interval \([l, u]\) can become excessively large quickly, often yielding overly pessimistic bounds.
\end{itemize}

\subsubsection{Unum}
Unum arithmetic is a relatively new system, introduced to improve the precision and reliability of computations:
\begin{itemize}
    \item It was introduced in 2015, with the latest revision in 2017.
    \item Unum claims to allocate precision better within a given fixed bit width and offers optional interval arithmetic.
    \item However, it has seen very limited adoption, mainly because there is no hardware support on mainstream platforms as of the last update.
\end{itemize}

\subsubsection{The GNU Multi-Precision Library (GMP)}
The GNU Multi-Precision Library is an important tool for computations requiring high precision:

\begin{itemize}
    \item GMP is a C library designed to provide support for variable-width integers and arbitrary-size rational numbers.
    \item A rational number is expressed as a fraction \( \frac{\text{numerator}}{\text{denominator}} \), where the greatest common divisor (gcd) of the numerator and denominator is 1, ensuring the fraction is in its simplest form.
    \item More information about GMP can be found at \url{http://gmplib.org}.
\end{itemize}


\subsubsection{The GNU MPFR library}
The GNU MPFR library enhances the GNU Multi-Precision Library (GMP) by providing a facility for arbitrary-precision floating-point computation.

\begin{verbatim}
double x = 22.0 / 7.0;
printf("%.20f\n", x);

mpfr_t x;
mpfr_init2(x, 512);             // initialize x with 512-bit mantissa
mpfr_set_ui(x, 22, MPFR_RNDN);  // set x to value 22, round-to-nearest
mpfr_div_ui(x, x, 7, MPFR_RNDN); // divide x to 7, round-to-nearest
mpfr_printf("%.20Rf\n", x);     // print x
mpfr_clear(x);                  // free memory
\end{verbatim}

For more details, visit \url{http://mpfr.org}.

\subsubsection{Python fractions}
Python supports arbitrary-precision integers and provides a module for exact rational number arithmetic.

\begin{verbatim}
>>> -2 ** 65
-36893488147419103232   # correct result, no overflow

import fractions
a = fractions.Fraction(numerator, denominator)
\end{verbatim}


\subsubsection{Why don't we always use exact rational numbers?}
While exact rational numbers provide precision, their use is not always practical in computing due to several factors:

\textbf{Convenience:}
\begin{itemize}
    \item Using GMP in C or importing fractions in Python may not always be straightforward.
\end{itemize}

\textbf{Memory:}
\begin{itemize}
    \item The size of the numerator and denominator can become very large, especially in iterative algorithms, even after gcd reductions.
\end{itemize}

\textbf{Speed:}
\begin{itemize}
    \item Operations on arbitrary-sized integers or exact rationals are slower (often by an order of magnitude) compared to fixed-size native types, due to lack of native hardware support.
\end{itemize}

\subsubsection{The Case for Exact Rational Numbers}
Should we use exact rational numbers more often in computations? Yes, particularly when:
\begin{itemize}
    \item Exactness is crucial to the application.
    \item Computational speed is not a limiting factor.
\end{itemize}

Exact rational numbers provide unparalleled precision and are essential in applications where accuracy cannot be compromised, such as in cryptography or symbolic mathematics.

\subsubsection{Symbolic computations}
Symbolic computation systems are powerful tools that allow for manipulation and solving of algebraic expressions symbolically rather than numerically. This means that the expressions are kept in their algebraic form, allowing for exact solutions and manipulations.\\

In a symbolic algebra system:
\begin{itemize}
    \item The square root of 2, denoted as \(\sqrt{2}\), is never approximated to a decimal such as 1.4142. It is kept in the root form for exact calculations. For instance, in SageMath:
    \begin{verbatim}
    sage: sqrt(8)
    2*sqrt(2)
    \end{verbatim}

    \item Variables can be defined without assigning them specific values, which is useful for carrying out algebraic manipulations. For example:
    \begin{verbatim}
    sage: x, y, z = var('x y z')
    sage: sqrt(8) * x
    2*sqrt(2)*x
    \end{verbatim}

    \item Symbolic computations allow solving algebraic problems, such as equations, symbolically. For instance:
    \begin{verbatim}
    sage: x, b, c = var('x b c')
    sage: solve([x^2 + b*x + c == 0], x)
    [x == -1/2*b - 1/2*sqrt(b^2 - 4*c), x == -1/2*b + 1/2*sqrt(b^2 - 4*c)]
    \end{verbatim}
\end{itemize}

\subsubsection{Symbolic Algebra Systems}
Several systems are available for performing symbolic computations:

\begin{itemize}
    \item SageMath is an open-source mathematical software system with a syntax similar to Python.
    \item Maple is a commercial computer algebra system.
    \item Wolfram Mathematica is a widely used system that offers powerful symbolic computation capabilities.
\end{itemize}

WolframAlpha, a web application based on Mathematica, provides step-by-step solutions and is accessible at \href{http://www.wolframalpha.com}{WolframAlpha}.
