\newpage
\section{Lecture 20}

\subsection{Abstract Data Types and Data Structures}

\begin{itemize}
    \item An abstract data type
    \begin{itemize}
        \item Specifies supported operations
    \end{itemize}
    \item A data structure is an implementation of an abstract data type
    \begin{itemize}
        \item Specifies data layout in memory
        \item Specifies algorithms for operations
    \end{itemize}
\end{itemize}

\subsubsection{Lists}

\begin{itemize}
    \item support storing multiple elements together
    \item and optionally append, insert, delete, random access, \ldots
    \item implementations:
    \begin{itemize}
        \item dynamic arrays
        \begin{itemize}
            \item everything \(O(1)\) in practice except insert/delete \(O(n)\)
        \end{itemize}
        \item linked lists
        \begin{itemize}
            \item everything \(O(1)\) (but slower than arrays) except random access \(O(n)\)
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Stacks / LIFO}

\begin{itemize}
    \item A stack is an ordered collection of elements
    \item supports two operations:
    \begin{itemize}
        \item ``push'': add an element
        \item ``pop'': retrieve-and-remove the last-added element
        \begin{itemize}
            \item $\Rightarrow$ last in, first out (LIFO)
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Static array implementation of a stack}

\begin{itemize}
    \item Useful only when there is a hard limit on the number of elements
    \item We maintain a static array
    \item and a stack pointer (or top index)
    \item this is how ``the'' stack is implemented \\
    (for storing function arguments, local variables and return addresses)
\end{itemize}

\paragraph{Example}

The images illustrate this process:
\begin{enumerate}
    \item Initially, the stack is empty, and the stack pointer points to the base of the stack.

    \item As elements A, B, and C are pushed onto the stack, the stack pointer is incremented after each push to point to the next empty slot.

    \item When elements are popped from the stack, the stack pointer is decremented, and the elements are removed in the reverse order they were added (Last In, First Out - LIFO).

    \item If elements are pushed after popping, the stack pointer moves upwards again to fill the spaces that were previously occupied.
\end{enumerate}

\subsubsection{Linked list implementation of a stack}
\begin{itemize}
    \item \textbf{Pro:} No hard limit on number of elements
    \item \textbf{Con:}
    \begin{itemize}
        \item Memory allocation for every \texttt{push}
        \item Memory freed for every \texttt{pop}
    \end{itemize}
\end{itemize}
\subsubsection{Dynamic array implementation of a stack}

\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item No hard limit on number of elements
        \item Memory management overhead is small
    \end{itemize}
    \item \textbf{Con:}
    \begin{itemize}
        \item No pointer stability
    \end{itemize}
\end{itemize}

\paragraph{Example}
In a dynamic array stack, managing element pointers becomes a non-trivial task due to possible reallocations. Here's an insight into pointer stability during stack operations:

\begin{itemize}
    \item The stack begins empty. We then push elements A, B, C, D, and E onto the stack, which increases in size dynamically. The stack pointer marks the current top of the stack.
    
    \item Taking a pointer `p` to an element, say C, allows direct manipulation of its value. But this raises a concern: what happens to `p` when the stack grows?
    
    \item If new pushes cause the stack to exceed its capacity, it must expand, often necessitating a copy of elements to a new memory block. This reallocation invalidates previous element pointers, such as `p` pointing to C, as they now reference the old memory location.
    
    \item The visual sequence illustrates changing C to C' via pointer `p`. After stack expansion due to additional pushes, if the elements are reallocated, `p` may still point to where C used to be, not to C' in the new stack memory layout.
    
    \item The final state, after expansion and modification through `p`, should reflect the changed value of C' at the correct position in the stack, ensuring that `p` is updated to point to the new location of C'.
\end{itemize}

\subsubsection{Arena}

\begin{itemize}
  \item Known as arena allocator, region-based allocator, zone-based allocator, obstack
  \item Implemented as a list of static array stacks
\end{itemize}

\paragraph{Further explanation}
An arena allocator, also known as a region-based allocator, zone-based allocator, or obstack, is a memory management model particularly suitable for situations where objects are allocated and deallocated in a predictable pattern.

\begin{itemize}
    \item The arena allocator manages memory in large blocks or "arenas." 
    \item Each arena can contain a stack or a pool of objects.
    \item Memory within an arena is allocated sequentially, and objects are typically deallocated all at once.
    \item This model is highly efficient when objects have similar lifetimes, as it minimizes fragmentation and overhead associated with individual deallocation.
\end{itemize}

The images illustrate the process of allocating and deallocating objects within an arena. The operations shown include pushing elements onto the stack and then popping them off, which corresponds to allocating and deallocating memory in the arena.

\begin{itemize}
    \item Initially, the stack pointer points to the base of the arena, indicating that no objects are allocated.
    \item As objects are pushed onto the stack, the stack pointer moves up, marking the end of the used space.
    \item When objects are popped off the stack, the stack pointer moves down, indicating that space is now available for future allocations.
    \item The allocation is fast and efficient since it involves only incrementing or decrementing the stack pointer.
\end{itemize}

Memory allocation in an arena is thus done in a LIFO order, and all objects within an arena can be deallocated at once by resetting the stack pointer to the base of the arena.

\subsubsection{Stack implementations}

\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Implementation} & \textbf{Size} & \textbf{Requires allocations} & \textbf{Pointer stability} \\
\hline
static array & constant & no & yes \\
\hline
dynamic array & can grow & when growing & no \\
\hline
linked list & can grow & every push & yes \\
\hline
arena & can grow & when growing & yes \\
\hline
\end{tabular}

\subsubsection{More options}


Combination of other data structures and indirection can be used, depending on desired properties.

\subsection{queues / FIFO}

A queue is an ordered collection of elements
\begin{itemize}
    \item supports two operations:
    \begin{itemize}
        \item \texttt{enqueue}: add an element
        \item \texttt{dequeue}: retrieve-and-remove the earliest-added element
    \end{itemize}
    \item $\Rightarrow$ first in, first out (FIFO)
\end{itemize}

\subsubsection{Ring buffer implementation of a queue}
\begin{itemize}
    \item Useful only when there is a hard limit on the number of elements
    \item We maintain a static array
    \item and two pointers/indices: head and tail
\end{itemize}

\paragraph{Example}
These operations are illustrated through the following example:

\begin{enumerate}
    \item Start with an empty queue.
    \item \textbf{Enqueue A:} The queue is now: A.
    \item \textbf{Enqueue B:} The queue is now: A, B.
    \item \textbf{Dequeue:} Remove A. The queue is now: B.
    \item \textbf{Enqueue C:} The queue is now: B, C.
    \item Continue this process, adding to the tail and removing from the head.
\end{enumerate}

As we enqueue elements, they take their place at the tail end of the queue. When we dequeue, we remove the element from the head, adhering to the FIFO principle. 

\subsubsection{Implementation detail}

\noindent When head == tail:

\noindent\begin{tabular}{ll}
    \text{the queue is empty?} & \text{or the queue is full?} \\
\end{tabular}

\noindent\begin{itemize}
    \item maintain a variable with the number of elements currently in the queue
    \item or keep incrementing head and tail, and index the static array as
        \begin{align*}
            \text{array[head \% size]} && \text{and} \\
            \text{array[tail \% size]}
        \end{align*}
\end{itemize}

\subsubsection{Applications of ring buffers}

\begin{itemize}
	\item Audio playback/recording devices
	\item Video capture devices
	\item Special case (double-buffering, i.e. size = 2) for computer graphics
	\item Network devices (routers, switches)
\end{itemize}

\subsubsection{More options}

\begin{itemize}
	\item use dynamic arrays
	\item use linked lists
	\item use indirection
	\item \ldots
	\item depending on specific needs
\end{itemize}

\subsection{Priority queues}

\begin{itemize}
    \item A priority queue is a collection of elements, each with an associated priority
    \item supports two operations:
    \begin{itemize}
        \item ``push'': add an element-priority tuple
        \item ``pop'': retrieve-and-remove the highest-priority element
    \end{itemize}
\end{itemize}

\subsubsection{Implementation of a priority queue}

\begin{itemize}
    \item Store element-priority tuples in an array or in a linked list
    \item ``push'': \(O(1)\) of the underlying data structure
    \item ``pop'': scan all elements, find max priority, \(O(n)\)
\end{itemize}

\subsubsection{Binary heap implementation of a priority queue}

% [TBD] replaces the tree diagram from the image
[TBD]

\begin{itemize}
    \item Binary heaps represent a priority queue as
    \begin{itemize}
        \item a binary tree (every node has at most two children)
        \item that is complete (every level full, except possibly the deepest)
    \end{itemize}
    \item Every node is labeled by the corresponding element’s priority
    \item Tree has the heap property:
    \begin{itemize}
        \item Priority of any node \( \geq \) priority of its children
        \item \( \Leftrightarrow \) Priority of any node \( \geq \) priority of all its descendants
    \end{itemize}
\end{itemize}

\subsubsection{Binary heap push}

\begin{itemize}
  \item Step 0: Add new element at the first free slot on the deepest level
  \item Step 1:
  \begin{itemize}
    \item If its priority is \textit{not higher} than its parent's,
    \begin{itemize}
      \item the heap property is \textit{satisfied}, we are done
    \end{itemize}
    \item If its priority is \textit{higher} than its parent's,
    \begin{itemize}
      \item swap them,
      \item go back to Step 1, looking at the pushed element's new position
    \end{itemize}
  \end{itemize}
\end{itemize}


\subsubsection{Binary heap pop}

% The tree is not transcribed as it requires specific drawing packages.
% Tree representation: 
%       30
%     /    \
%    25    15
%   /  \
%  5   18 
%         \
%         9
[TBD]

\begin{itemize}
    \item Step 0: Replace root with last element (on deepest level)
    \item Step 1:
    \begin{itemize}
        \item If its priority is \textbf{not lower} than its children’s,
        \begin{itemize}
            \item the heap property is \textbf{satisfied}, we are done
        \end{itemize}
        \item If its priority is \textbf{lower} than one of its children’s,
        \begin{itemize}
            \item swap with the highest-priority child,
            \item go back to Step 1, looking at the pushed element’s new position
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Binary heap operations}
\begin{itemize}
    \item Push: \(O(\log_2(n))\)
    \item Find max: \(O(1)\)
    \item Pop: \(O(\log_2(n))\)
\end{itemize}

\subsubsection{Complete binary data structure}
\begin{itemize}
    \item Binary heaps are complete binary trees
    \item We can avoid allocation for every ``push'' by storing nodes in an array
    \item Depth \(\ell\) of the tree has at most \(2^\ell\) nodes, \(\forall \ell\)
    \item Depth \(\ell\) of the tree has exactly \(2^\ell\) nodes, except for the deepest level
    \begin{center}
        \begin{tabular}{|c|ccccccccccccccc|}
        \hline
        depth & 0 & 1 & 1 & 2 & 2 & 2 & 2 & 3 & 3 & 3 & 3 & 3 & 3 & 3 \\
        index & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 \\
        \hline
        \end{tabular}
    \end{center}
    \item There are exactly \( (2^\ell - 1) \) nodes of with depth \( < \ell \)
\end{itemize}



\subsubsection{Storage scheme}

[TBD]

\begin{tabular}{cccccccccccccccc}
depth & 0 & 1 & 1 & 2 & 2 & 2 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 \\
index & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 \\
\end{tabular}

\begin{itemize}
  \item If a node has index \( j \)
  \item its children are stored at indices \( 2j + 1 \) and \( 2j + 2 \)
  \item its parent is stored at index \( \left\lfloor (j - 1) / 2 \right\rfloor \)
\end{itemize}

\subsubsection{Binary heap with array storage}

\begin{itemize}
    \item Superior to in-memory tree (with pointers)
    \begin{itemize}
        \item We avoid allocation for every ``push''
        \item We avoid data dependencies (load node data to get pointer to parent/children)
    \end{itemize}
    \item Still,
    \begin{itemize}
        \item Push and pop operations are tough for branch predictor
        \item Jumps to indices $(2j + 1)$, $(2j + 2)$ or $\lfloor (j - 1) / 2 \rfloor$ are not cache-friendly for large $j$
    \end{itemize}
\end{itemize}


\subsubsection{Priority queue: special case}

\begin{itemize}
    \item Assume that
    \begin{itemize}
        \item priorities are distinct integers $p \in \{0, \ldots, P - 1\}$
        \item we always push at a priority $\leq$ current max priority
    \end{itemize}
    \item Then,
    \begin{itemize}
        \item we allocate a static array of size $P$
        \item Push: store in array at index $p$ \quad $O(1)$
        \item Pop: sweep array backwards \quad $O(P / n)$ amortized
    \end{itemize}
\end{itemize}
\paragraph{Further Explanation}
\begin{itemize}
    \item A priority queue is implemented as a static array, where the index represents the priority and the array element at that index represents the queue element at that priority level.
    \item The queue supports two main operations:
    \begin{itemize}
        \item \textbf{Push operation:} This inserts an element into the queue at the position that corresponds to its priority. The operation is constant time, \( O(1) \), because it places the element directly at the index of its priority.
        \item \textbf{Pop operation:} This removes the element with the highest priority that is less than or equal to the current max priority. To find this element, the array is swept backwards from the current max priority index. The amortized complexity of this operation is \( O(P / n) \), where \( P \) is the size of the priority range and \( n \) is the number of operations, due to the fact that each element can be popped only once and the cost is spread over multiple pop operations.
    \end{itemize}
    \item The maximum priority element is always kept updated to enable efficient pop operations. After a pop, the max priority is updated to the next highest priority that has an element in the queue.
    \item This implementation allows for efficient insertion of elements in a priority-wise fashion and also ensures that the removal of the highest priority element is done with a reasonable amortized cost.
\end{itemize}

\subsubsection{Implementation details}

\begin{itemize}
  \item good for branch predictor
  \item great for caches
  \item we can store, additionally, an array of $P$ bits (``bitmap'')
  \begin{itemize}
    \item bit $p$ set to one if there is an element with priority $p$
    \item makes ``pop'' operations essentially 64x faster
  \end{itemize}
\end{itemize}

\subsubsection{Applications of bitmap priority queues}

\begin{itemize}
  \item Linux kernel: scheduling parallel tasks
  \item Linear algebra: sparse matrices
\end{itemize}

\subsection{Sort Operations}

\subsubsection{Heap sort}
\begin{itemize}
    \item Push $n$ elements to heap: $O(n \log n)$
    \item Pop $n$ elements one by one: $O(n \log n)$
\end{itemize}

\[
\Rightarrow O(n \log n) \text{ worst case}
\]

\subsubsection{Comparison sort methods}
\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{Average} & \textbf{Worst case} & \textbf{Additional storage} \\
\hline
Quicksort & $O(n \log(n))$ & $O(n^2)$ & none \\
Merge sort & $O(n \log(n))$ & $O(n \log(n))$ & $n$ \\
Heap sort & $O(n \log(n))$ & $O(n \log(n))$ & none \\
\hline
\end{tabular}
\end{table}

\subsubsection{Special case 1}
\begin{itemize}
    \item Assume that
    \begin{itemize}
        \item we sort $n$ elements with priorities $S \subseteq \{0, \ldots, P - 1\}$
        \item no two elements have the same priority (hence $P \geq n$)
    \end{itemize}
    \item Then,
    \begin{itemize}
        \item we represent the elements as a bitmap priority queue
        \item Push: $O(n)$
        \item Pop: $O(P)$
    \end{itemize}
    \item[] $\Rightarrow O(n + P)$
\end{itemize}

\subsubsection{Special case 2: counting sort}
\begin{itemize}
    \item Assume that
    \begin{itemize}
        \item we sort $n$ elements with priorities $S \subseteq \{0, \ldots, P - 1\}$
        \item $P \leq n$ (we may have duplicates)
    \end{itemize}
    \item Then,
    \begin{itemize}
        \item we allocate a static array \texttt{count} of size $P$
        \item we allocate a static array \texttt{result} of size $n$
        \item we count the number of occurences of each priority: $O(n)$
        \item we sweep \texttt{count} backwards to determine offsets: $O(P) = O(n)$
        \item we construct the sorted \texttt{result} list: $O(n)$
    \end{itemize}
    \item[] $\Rightarrow O(n)$
\end{itemize}