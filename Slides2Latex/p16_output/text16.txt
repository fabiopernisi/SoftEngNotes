\newpage
\section{Lecture 16}

\subsection{Performance}

\subsubsection{What is performance?}

We want computers to perform required actions while minimizing their use of \textbf{resources}:
\begin{itemize}
    \item Time
    \item Power use (mobile, servers)
    \item Network use (mobile)
    \item Memory use (peak RAM usage)
    \item Storage (solid state drive / hard disk drive)
\end{itemize}

We care about those resources in proportion to their cost (financial, emotional, etc.)

\subsubsection{How do we achieve good performance?}
High level to low level:
\begin{enumerate}
    \item Pick a good algorithm
    \begin{itemize}
        \item specifically, an algorithm with low computational complexity:
        \item $O(\log(n))$ better than $O(n^2)$ better than $O(n^6)$ better than $O(2^n)$
        \item we can hope for great improvements, $100\times$ faster, $1000\times$, etc.
        \item $\rightarrow$ first thing to try!
    \end{itemize}
    \item Pick an algorithm that is fast on the computers you use and implement it well
    \begin{itemize}
        \item this course!
        \item smaller improvements: from a few percent to $50\times$ faster
    \end{itemize}
    \item Translate the implementation into efficient instructions (compilers do that well)
\end{enumerate}
\subsubsection{What hides inside the big-O?}

Let us compare two algorithms:
\begin{itemize}
    \item Algorithm A has complexity $O(n^2)$
    \item Algorithm B has complexity $O(n \log(n))$
\end{itemize}

Specifically,
\begin{itemize}
    \item Algorithm A performs $2n^2 + 16$ operations
    \item Algorithm B performs $16n \log(n) + 64$ operations
\end{itemize}

Graph showing that, for smaller n, $O(n^2)$ is be better even though it has a worse big-O scale.
\[
\begin{array}{cc}
\text{Algorithm A} & \text{Algorithm B} \\
2n^2 + 16 & 16n \log(n) + 64 
\end{array}
\]

\subsubsection{Which algorithm do we choose?}

Assume that
\begin{itemize}
    \item Algorithm A is insertion sort
    \item Algorithm B is merge sort
\end{itemize}

\subsubsection{Merge sort example}
The given charts illustrate the performance characteristics of two sorting algorithms, Algorithm A (Insertion Sort) and Algorithm B (Merge Sort), across different input sizes.

\begin{itemize}
    \item In the first chart, we see that for smaller values of \( n \), Algorithm A performs competitively, but as \( n \) increases, its performance degrades significantly compared to Algorithm B.
    \item The takeaway is that we should choose the right algorithm based on the size of the input: for smaller datasets, Algorithm A may be more efficient, whereas for larger datasets, Algorithm B is preferable.
    \item The second set of charts emphasizes that we can do even better by combining algorithms. For small datasets, we can use Algorithm A, and as the dataset grows, we can switch to Algorithm B. This combination approach leverages the strengths of both algorithms.
    \item The right chart in the second image shows that combining algorithms results in a performance that is close to the best of both worlds: the efficiency of Algorithm A for small datasets and the scalability of Algorithm B for larger ones.
\end{itemize}

This strategy is often employed in practice, such as in the implementation of sorting functions in standard libraries, which use a hybrid approach (like Timsort in Python and Java) that adapts to the nature of the dataset to provide optimal sorting performance.

\subsection{CPU Pipelines}
\subsection{Back to instructions}

Instruction decoding:

\begin{verbatim}
48 2b 06        sub    rax, QWORD PTR [rsi]
48 af           imul   rax, QWORD PTR [rsi]
48 af af 06     imul   rax, QWORD PTR [rdx]
\end{verbatim}

From \texttt{48 af af 06}, the CPU needs to understand:

\begin{itemize}
  \item that it must perform a multiplication (as opposed to, say, a subtraction)
  \item that one term is the value of a 64-bit register, \texttt{rax}
  \item that the other term comes from memory: the 64-bit value pointer to by \texttt{rsi}
\end{itemize}


\paragraph{What happens after instruction decoding?}
\texttt{48 af af 06     imul   rax, QWORD PTR [rdx]]}
\begin{itemize}
    \item the CPU has Boolean circuitry to compute multiplications
    \item it must ensure that one of the two inputs of the multiplier is \texttt{rax}
    \item the CPU has Boolean circuitry to access memory
    \item it must ensure that the input of the memory circuitry is \texttt{rsi}
    \item it sets the second input of the multiplier to the output of the memory circuitry
    \item it stores the output of the multiplier back to \texttt{rax}
\end{itemize}
\textbf{It is no longer possible to do all this in a single cycle} \\
(e.g., at 4 GHz, i.e. 4 billion cycles per second, so in 0.25 ns)
\paragraph{Imagine that it takes:}
\begin{itemize}
    \item one cycle to decode an instruction
    \item one cycle to fetch data from memory
    \item one cycle to perform arithmetic
\end{itemize}

\paragraph{Sequential Execution of CPU Instructions}

The provided diagrams illustrate the sequential execution of two assembly instructions in a non-pipelined CPU architecture. Each instruction goes through the decode, memory, and arithmetic stages one at a time without overlap. Considering one cycle for each stage, the execution timeline is as follows:

\begin{enumerate}
    \item The first instruction, \texttt{sub rax, [rdi]}, involves subtracting the value at the memory location pointed to by \texttt{rdi} from the \texttt{rax} register. The execution stages are:
    \begin{itemize}
        \item Cycle 1: Decoding the \texttt{sub} instruction.
        \item Cycle 2: Fetching the operand from memory \texttt{[rdi]}.
        \item Cycle 3: Performing the subtraction and updating the \texttt{rax} register.
    \end{itemize}
    \item After the first instruction completes all its stages, the second instruction, \texttt{imul rbx, [rsi]}, which multiplies the \texttt{rbx} register by the value at the memory location pointed to by \texttt{rsi}, begins its execution:
    \begin{itemize}
        \item Cycle 4: Decoding the \texttt{imul} instruction.
        \item Cycle 5: Fetching the operand from memory \texttt{[rsi]}.
        \item Cycle 6: Performing the multiplication and updating the \texttt{rbx} register.
    \end{itemize}
\end{enumerate}

\textbf{Each instruction takes 3 cycles}

However, in this model,
\begin{itemize}
  \item while the memory circuitry is busy fetching QWORD PTR {[rsi]}, the multiplier is idle
  \item while the multiplier computes the result, the memory is idle
  \item during instruction decoding, everything else is idle
\end{itemize}
We can exploit this!
\subsubsection{Pipelined execution}

The images demonstrate the pipelined execution of instructions in a modern CPU. This execution model allows multiple instructions to be processed concurrently at different stages of the instruction cycle. Given the scenario where each stage (decode, memory access, arithmetic operation) takes one cycle to complete, the pipeline allows for a new instruction to enter the pipeline before the previous instruction has finished, thus improving the overall throughput of the system.

\begin{itemize}
    \item Initially, the pipeline is filled with the first instruction \texttt{sub rax, [rdi]} going through the decode, memory, and arithmetic stages.
    \item As soon as the decode stage is complete for the first instruction, the second instruction \texttt{imul rbx, [rsi]} enters the decode stage while the first instruction is in the memory stage.
    \item This process continues with the third instruction \texttt{add rcx, [rbp]} entering the pipeline.
    \item Once the pipeline is full, each cycle will see an instruction at each stage of execution, effectively executing multiple instructions simultaneously.
\end{itemize}

The pipelined execution is visualized in the images, with each horizontal level representing a cycle and each stage of instruction processing moving one step down the pipeline in each subsequent cycle. The advantage of this model is that the CPU does not have to wait for one instruction to complete all its stages before starting the next one, which significantly increases the instruction throughput.

\subsubsection{Throughput vs. latency}

\begin{itemize}
    \item Latency:
    \begin{itemize}
        \item Executing each instruction still takes 3 cycles!
    \end{itemize}
    \item Throughput:
    \begin{itemize}
        \item But on average, we execute up to 1 instruction per cycle.
    \end{itemize}
\end{itemize}

\subsubsection{Data dependencies}

The images depict the execution of two assembly instructions that have a data dependency. Data dependencies occur when an instruction requires the result of a previous instruction.

\begin{enumerate}
    \item The first instruction, \texttt{mul rcx, [rdx]}, performs a multiplication operation with the value at the memory location pointed to by \texttt{rdx} and stores the result in \texttt{rcx}.
    \item The second instruction, \texttt{add rdx, [rsi]}, adds the value at the memory location pointed to by \texttt{rsi} to the register \texttt{rdx}.
\end{enumerate}

Given the following conditions:
\begin{itemize}
    \item Each stage (decode, memory, arithmetic) takes one cycle to complete.
    \item An instruction must wait for the previous instruction to complete its memory and arithmetic stages if there is a data dependency.
\end{itemize}

The execution proceeds as follows:
\begin{itemize}
    \item The \texttt{mul} instruction is decoded first, then the corresponding value is fetched from memory, and finally, the multiplication is performed.
    \item The \texttt{add} instruction must wait until the \texttt{mul} instruction has completed its arithmetic stage before it can proceed with the memory fetch, as it depends on the updated value of \texttt{rdx}.
    \item This results in a stall in the pipeline, where the \texttt{add} instruction's decoding is delayed until the \texttt{mul} instruction's result is written back, ensuring the correct value is used for the addition.
\end{itemize}

The pipelined execution is disrupted by the data dependency, causing delays or stalls, which are illustrated in the sequence of images.

\subsubsection{Conditional branching}

Conditional branching poses a unique challenge in CPU instruction pipelines. When an \texttt{if} statement is encountered, the CPU must determine which path to follow â€“ the true branch or the false branch. The provided diagrams illustrate the step-by-step decision-making process in a simplified CPU pipeline.

\begin{enumerate}
    \item Initially, the CPU decodes the comparison instruction \texttt{cmp rdi, rsi}. This instruction sets up the condition flags based on the comparison of the values in the \texttt{rdi} and \texttt{rsi} registers.
    
    \item Once the \texttt{cmp} instruction is decoded, the CPU moves on to decode the subsequent \texttt{jge} (jump if greater or equal) instruction. However, the jump cannot be performed until the result of the \texttt{cmp} instruction is known, which requires waiting for the arithmetic stage to complete.
    
    \item If the condition is true, the CPU follows the branch to the instruction labeled \texttt{.L1} (in this case, representing the code block \texttt{YYY}). Otherwise, it continues to the next instruction in sequence (\texttt{zzz}).
    
    \item During this decision phase, the pipeline may stall, waiting for the comparison result to decide the next instruction to fetch and decode. This waiting period is where branch prediction can play a significant role in maintaining pipeline efficiency.

\subsubsection{Branch prediction}

\begin{itemize}
\item in practice, the CPU will try to predict which branch will be taken (based on past choices at that specific instruction)
\item and speculatively choose that branch
\end{itemize}


\paragraph{Example}
\begin{itemize}
    \item Initially, the CPU encounters a conditional branch instruction (\texttt{jge .L1}) and speculatively decodes the subsequent instructions (YYY) assuming the branch will not be taken.
    \item Meanwhile, the comparison instruction (\texttt{cmp rdi, rsi}) is executed to evaluate the actual condition. Until this condition is evaluated, the CPU continues to decode and execute the speculatively chosen path.
    \item If the branch prediction is correct, the CPU continues execution without any interruption, leading to higher performance due to reduced waiting time.
    \item However, if the prediction is incorrect, as depicted in the final images, the CPU must discard or "flush" the speculatively executed instructions (YYY) and redirect execution to the correct path (ZZZ). This action is known as a pipeline flush, which can cause a temporary decrease in performance due to the time taken to correct the path and refill the pipeline with the correct instructions.
\end{itemize}

The diagrams illustrate a scenario where the CPU's branch predictor incorrectly anticipates the direction of a branch, resulting in several cycles of mispredicted instruction execution. Once the correct path is determined (ZZZ), the pipeline is adjusted, but the mispredicted instructions (YYY) must be invalidated, showcasing the cost of a misprediction.

\subsubsection{In practice}
\begin{itemize}
    \item When all goes perfect, processors can actually execute more than one instruction per cycle
    \item Modern processor pipelines have between 5 and 40 stages
    \item At each stage, there are multiple circuitry blocks (decoders, arithmetic and logic unit (ALU) "ports", etc.)
    \item Branch mispredict penalty is typically $\geq 10$ cycles
    \item Main memory latency is 50--200 cycles
\end{itemize}
\subsubsection{How do we write good code?}
\begin{itemize}
    \item These parameters vary widely from CPU to CPU
    \item Specific characteristics are often not public
    \item It is almost impossible to predict the number of cycles a given set of instructions will take (in the presence of branches and memory accesses)
    \item $\Rightarrow$ Qualitatively: we try to \textit{understand} the phenomena at play
    \item $\Rightarrow$ Quantitatively: We \textit{measure} at runtime
\end{itemize}

\subsubsection{Out-of-order execution}

Out-of-order execution is an advanced CPU technique that improves the utilization of execution units by allowing instructions behind a slow-running instruction to be executed in advance if the operands they require are available.

\begin{itemize}
    \item In the given example, there are three assembly instructions: two addition instructions (\texttt{add rdx, rdi} and \texttt{add rcx, rbx}) and one memory read instruction (\texttt{mov rax, [rsi]}).
    
    \item The CPU starts by decoding the first instruction. If the operands are immediately available, it may dispatch this instruction to an Arithmetic Logic Unit (ALU) for execution.
    
    \item If the subsequent instruction does not depend on the first one's result and its operands are ready, the CPU may decode and dispatch this instruction to another ALU, even if the first instruction has not completed yet.
    
    \item The memory read instruction may be dispatched to the memory unit independently if the memory address is ready, potentially allowing all three instructions to be in various stages of execution simultaneously.
    
    \item This results in improved overall throughput as the CPU does not idle waiting for instructions to complete in program order. However, the CPU must ensure that the results are written back in the correct order to preserve the program's logical flow.
\end{itemize}

\subsection{Memory}

Access to memory (``random access memory'' or RAM) is slow

On desktop computers, RAM is typically on distinct integrated circuit (IC) packages,
physically centimeters away from the CPU.

Solution: \textbf{caching}

\subsubsection{Caching}

Level 1 (``L1'') cache:
\begin{itemize}
    \item The CPU contains a \textbf{small amount of extremely fast memory}
    \item This memory \textbf{requires many of logic gates} on-package
    \item But it is \textbf{always available} (no latency)
    \item The CPU contains logic to decide which part of the main memory gets stored in its L1 cache
    \item This continuously changes over time
\end{itemize}

Level 2 (``L2'') cache:
\begin{itemize}
    \item slower than L1
    \item but requires fewer logic gates, so we can have more
\end{itemize}

Level 3 (``L3'') cache:
\begin{itemize}
    \item slower than L2
    \item but requires fewer logic gates, so we can have more
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Zen 4 Cache} & \textbf{L1I cache} & \textbf{L1D cache} & \textbf{L2 cache} & \textbf{L3 cache} \\ \hline
Cache size           & 32kB               & 32kB                & 1MB               & xxx               \\ \hline
Associativity        & 8 way              & 8 way               & 8 way             & xxx               \\ \hline
Cache line size      & 64 b               & 64 b                & 64 b              & 64 b              \\ \hline
\end{tabular}
\caption{Cache configuration for Zen 4 architecture.}
\label{table:zen4cache}
\end{table}
\subsubsection{Typical configuration}

\begin{itemize}
    \item memory transits through in units of one cache line
    \begin{itemize}
        \item 64 bytes on x86\_64
        \item 128 bytes on M1 Macs
    \end{itemize}
    \item there is no concept of locality beyond cache lines
    \item every memory access is performed through L1 cache
    \item when all cache entries are full, we need to overwrite one
    \begin{itemize}
        \item $\rightarrow$ cache eviction policies e.g. least-recently used (LRU)
    \end{itemize}
    \item pipelined CPUs feature a memory prefetcher (speculatively fills caches in advance)
\end{itemize}

\subsubsection{How do we write good code?}

\begin{itemize}
    \item Again, cache operation varies widely from CPU to CPU
    \item It is almost impossible to predict how it will behave with complex instruction streams
    \item $\Rightarrow$ Qualitatively: we try to \textbf{understand} how caches work
    \item $\Rightarrow$ Quantitatively: We \textbf{measure} at runtime
\end{itemize}