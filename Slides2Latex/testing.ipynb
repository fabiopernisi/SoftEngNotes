{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # pip install PyMuPDF\n",
    "import io\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()\n",
    "\n",
    "# Function to convert PDF to images, with two pages per image\n",
    "def convert_pdf_to_images(pdf_path, output_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # List to hold the byte data of images\n",
    "    images_bytes = []\n",
    "    \n",
    "    # Go through the pages three by one\n",
    "    for page_number in range(0, pdf_document.page_count, 3):\n",
    "        # Create a blank image with white background, three times the height of a single page\n",
    "        combined_image = Image.new('RGB', \n",
    "                                (int(pdf_document[0].rect.width), \n",
    "                                    int(pdf_document[0].rect.height * 3)), \n",
    "                                'white')\n",
    "        \n",
    "        # Draw the pages on the combined_image\n",
    "        for i in range(3):\n",
    "            if page_number + i < pdf_document.page_count:\n",
    "                page = pdf_document[page_number + i]\n",
    "                pix = page.get_pixmap()\n",
    "                img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "                # Calculate the vertical position for each page\n",
    "                vertical_position = int(pdf_document[0].rect.height) * i\n",
    "                combined_image.paste(img, (0, vertical_position))\n",
    "        \n",
    "        # Save the combined image to bytes\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        combined_image.save(img_byte_arr, format='JPEG')\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "        images_bytes.append(img_byte_arr)\n",
    "        for (i,image) in enumerate(images_bytes):\n",
    "            image_output_path=os.path.join(output_path, str(i)+'.jpg')\n",
    "            with open(image_output_path, \"wb\") as img_file:\n",
    "                img_file.write(images_bytes[i])\n",
    "\n",
    "    return images_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = #<YOUR KEY>\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request(image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Transcribe the image content exclusively into LaTeX format, suitable for immediate compilation without any modifications. The transcription should include all textual content, accurately reflecting the lecture material. Replace images or non-textual elements that cannot be represented in LaTeX with the token [TBD]. Avoid adding document creation commands or \\includegraphics; focus on replicating the lecture content cleanly and directly. Any necessary explanations or notes should be included as comments within the LaTeX code, using the '%' symbol. The goal is to provide a LaTeX transcription that consists solely of the lecture's text content, formatted and ready for direct use in a LaTeX editor. Don't use beamers of frames.\"\n",
    "            },\n",
    "            {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latex_code(response_text):\n",
    "    # Splitting the text into lines\n",
    "    lines = response_text.split(\"\\n\")\n",
    "\n",
    "    # Finding the start and end indices of the LaTeX block\n",
    "    start_index = next((i for i, line in enumerate(lines) if \"```latex\" in line), -1)\n",
    "    end_index = next((i for i, line in enumerate(lines) if line.strip() == \"```\" and i > start_index), len(lines))\n",
    "\n",
    "    # Extracting the LaTeX block\n",
    "    latex_block = lines[start_index + 1:end_index]\n",
    "\n",
    "    # Filtering out specific LaTeX commands\n",
    "    unwanted_commands = [\"\\\\includepackage\", \"\\\\begin{document}\", \"\\\\end{document}\", \"\\\\documentclass{\", \"\\\\usepackage\"]\n",
    "    latex_code = [line for line in latex_block if not any(cmd in line for cmd in unwanted_commands)]\n",
    "\n",
    "    return \"\\n\".join(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# messy structure, but it works the following structure: p01_output, p02_output, ..., p22_output (a folder for each lecture).\n",
    "# Each folder contains a variable number of jpg images, each containing 3 subsequent lecture slides (which will then be fed to GPT-4, one at a time).\n",
    "\n",
    "def main():\n",
    "    counter=0\n",
    "    # change range if needed. This selects the range of lectures (between 1 and 22)\n",
    "    for lectures in range(20, 23): \n",
    "        if len(str(lectures))==1:\n",
    "            lectures= \"0\" + str(lectures)\n",
    "\n",
    "        dir_path=\"p\"+str(lectures)+\"_output\"\n",
    "        files = os.listdir(dir_path)\n",
    "        sorted_files = sorted(files, key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in sorted_files:\n",
    "            file_path=os.path.join(dir_path, file)\n",
    "            if counter == 100:\n",
    "                break\n",
    "            response=request(file_path)\n",
    "            counter +=1\n",
    "            \n",
    "            cleaned_latex = extract_latex_code(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "            text += \"\\n\\n\\n\" + cleaned_latex\n",
    "            \n",
    "            write_path=os.path.join(dir_path, \"text.txt\")\n",
    "            with open(write_path, \"w\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            # up to 3 requests per minute are allowed\n",
    "            time.sleep(21)\n",
    "            \n",
    "        print(\"Done with lecture\", lectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"main\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
