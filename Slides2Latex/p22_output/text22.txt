\newpage
\section{Lecture 22}
\subsection{Parallel Computation}

\subsection{Paralleism that does not require programmer intervention}

\subsubsection{Pipelines}
\begin{itemize}
    \item CPU pipelines can be viewed as implementing some form of parallelism in the sense that multiple executions are being executed simultaneously.
    \item For example, one instruction’s arithmetic is performed (in an ALU) while the next is being decoded.
    \item However, from the programmer’s perspective, everything must happen as if there was no parallelism at all.
\end{itemize}

\subsubsection{Multitasking}

\begin{itemize}
    \item Multitasking allows multiple executables to run “simultaneously” (even on a single processor)
    \item Regularly, the \textbf{scheduler} (part of the OS kernel) decides which task gets to run on a processor.
\end{itemize}

\subsection{Multitasking on a single-core processor}

\begin{itemize}
    \item Multitasking on a single-core processor allows multiple tasks to be handled seemingly in parallel by quickly switching between them. This is known as context switching.
    \item A single-core CPU can only execute one task at a time. However, it gives the illusion of parallelism by rapidly alternating between tasks, which is managed by the operating system's scheduler.
    \item Tasks can be in different states, such as \textit{running}, where a task is actively using the CPU, or \textit{sleeping}, where a task is inactive and waiting for some event or the passage of a certain amount of time.
    \item When multitasking, the scheduler decides which task should be run next. This decision is based on various factors such as task priority, fairness, I/O requirements, and more.
    \item The scheduler uses a context switch to save the state of the current running task and load the state of the next task to run. The state of a task includes information like the program counter, registers, and memory allocation.
    \item This rapid switching is fast enough that users perceive multiple applications are running simultaneously, even though only one is being processed at any instant.
\end{itemize}

\begin{itemize}
  \item The scheduler is called:
  \begin{itemize}
    \item at regular intervals $f$ times per second, by default:
    \begin{itemize}
      \item Linux: $f = 1000\ \text{Hz}\ (\text{see CONFIG\_HZ})$
      \item macOS: $f = 100\ \text{Hz}\ (\text{see sysctl kern.clockrate})$
      \item Windows 10: $f = 64\ \text{Hz}\ (\text{see timeBeginPeriod()})$
    \end{itemize}
    \item when an task performs a system call (\texttt{open()}, \texttt{write()}, \texttt{exit()}, ...)
    \item when a ``hardware interrupt'' happens:
    \begin{itemize}
      \item keyboard received a keypress
      \item network device received data
      \item storage device finished writing
      \item sound/video device ready to receive next buffer
      \item \ldots
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsubsection{Preemptive multitasking}
\begin{itemize}
  \item When the scheduler decides to interrupt a running process (e.g. to run another)
  \begin{itemize}
    \item the process is said to ``preempted''
    \item it becomes ``runnable''
  \end{itemize}
  \item When a process executes a system call,
  \begin{itemize}
    \item it starts ``sleeping''
    \item after the requested operation is performed,
    \begin{itemize}
      \item in some cases, it will run again
      \item in other cases, it becomes runnable and will only run when a CPU is available
    \end{itemize}
    \item many system calls can take a long time to perform (``blocking'' system calls):
    \begin{itemize}
      \item \texttt{read()}, \texttt{write()}, \texttt{recv()}, \texttt{send()}
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item At any given time, most tasks are sleeping
  \begin{itemize}
    \item waiting for data (e.g. from network)
    \item waiting for user interaction (e.g. keyboard or touch input)
    \item waiting on a timer (tasks that run at regular interval)
  \end{itemize}
  \item The only tasks that are normally running/runnable are those performing CPU-intensive operations
  \begin{itemize}
    \item graphics rendering
    \item audio/video/data compression and decompression
    \item computations
    \item etc.
  \end{itemize}
\end{itemize}

\subsubsection{Multitasking on a multi-core processor}

\begin{itemize}
    \item Multitasking on a multi-core processor refers to the ability of the CPU to perform multiple tasks at the same time by utilizing multiple cores within a single CPU unit.
    \item Each core can independently run a task, allowing for real parallel execution of processes. This improves the overall efficiency and performance of the system, especially for multi-threaded applications.
    \item In the illustrated example, five tasks (task 0 to task 4) are distributed across four CPU cores (CPU 0 to CPU 3). This distribution allows each task to execute in parallel:
    \begin{itemize}
        \item Task 0 runs on CPU 0
        \item Task 1 runs on CPU 1
        \item Task 2 runs on CPU 2
        \item Task 3 runs on CPU 3
        \item Task 4 could either be queued or run concurrently if one of the other tasks completes or is in a waiting state (not shown in the images).
    \end{itemize}
    \item This parallelism enables a multi-core processor to handle more tasks in the same amount of time compared to a single-core processor, which must switch contexts to give the illusion of parallelism.
\end{itemize}


\begin{itemize}
    \item From a hardware perspective:
    \begin{itemize}
        \item A CPU corresponds to a single integrated circuit (``IC'') package
        \item A computer can (rarely) have multiple CPUs
        \begin{itemize}
            \item Typically only found in datacenters, rarely more than 2
        \end{itemize}
        \item Each CPU can have multiple \textit{cores}
        \begin{itemize}
            \item generally 2-8 cores on laptops
            \item up to 128 on datacenter CPUs
        \end{itemize}
    \end{itemize}
    \item From a software perspective:
    \begin{itemize}
        \item Everything that can run a task is generally called a ``CPU''
        \item Only the kernel's scheduler will (sometimes) care about CPU vs. core
        \item All other software is unaware of the difference
    \end{itemize}

    \item A CPU can have multiple copies of some logic blocks
    \item very common for arithmetic and logic units (ALUs)
\end{itemize}


\subsection{Simultaneous Multithreading {SMT}}
    \begin{itemize}
        \item From a hardware perspective:
        \begin{itemize}
            \item With Simultaneous Multithreading (SMT) (a.k.a. Hyperthreading),
            \item each core can run multiple (generally 2) tasks (``threads'')
            \item but they share many logic blocks (in particular ALUs)
            \item SMT \textit{works well} when those logic blocks would otherwise be idle
            \item SMT is \textit{ineffective} when those logic blocks are the bottleneck
        \end{itemize}
        \item From a software perspective:
        \begin{itemize}
            \item Everything that can run a task is generally called a ``CPU''
            \item Only the kernel's scheduler will (sometimes) care about CPU vs. core vs. \textit{thread}
            \item All other software is unaware of the difference
            \item ``Thread'' has a different meaning in software
        \end{itemize}
    \end{itemize}

\subsection{SIMD}
\begin{itemize}
    \item SIMD stands for Single Instruction Multiple Data
    \item new, larger registers (in addition to the general purpose ones): ``vector registers''
\end{itemize}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
bits                   & 255..224             & 223..192             & 191..160             & 159..128             & 127..96              & 95..64               & 63..32               & 31..0                \\ \hline
\multirow{2}{*}{256}   & \multicolumn{8}{c|}{ymm0}                                                                                                                                  \\ \hline
64                     & fp64 \#3             &                      & fp64 \#2             &                      & fp64 \#1             &                      & fp64 \#0             &                      \\ \hline
32                     & fp32 \#7             & fp32 \#6             & fp32 \#5             & fp32 \#4             & fp32 \#3             & fp32 \#2             & fp32 \#1             & fp32 \#0             \\ \hline
16                     & \multicolumn{2}{c|}{}                      & \multicolumn{2}{c|}{}                      & \multicolumn{2}{c|}{}                      & \multicolumn{2}{c|}{}                      \\ \hline
8                      & \multicolumn{8}{c|}{}                                                                                                                                      \\ \hline
\end{tabular}





\begin{itemize}
    \item but
    \begin{itemize}
        \item SIMD registers cannot be treated as big integers
        \item individual ``lanes'' (8-, 16-, 32- or 64-bit parts) generally cannot be accessed individually
    \end{itemize}
\end{itemize}

\subsection{SIMD registers}
\begin{itemize}
    \item On Intel (and AMD) ISAs:
    \begin{itemize}
        \item SSE (~1999): 8 128-bit registers xmm0 - xmm7
        \item AVX (~2011): 16 256-bit registers ymm0 - ymm15
        \item AVX-512 (~2016, but not yet generally available): 32 512-bit registers zmm0 - zmm31
    \end{itemize}
    \item On ARM:
    \begin{itemize}
        \item Neon (~2005): 16 128-bit registers Q0 - Q15
    \end{itemize}
\end{itemize}

\subsection{Example}

The code example illustrates the concept of SIMD (Single Instruction, Multiple Data) in the context of vectorized operations for performance optimization in computing.

\begin{itemize}
    \item SIMD is a parallel computing architecture that allows a single processor instruction to perform the same operation on multiple data points simultaneously.
    \item In the given C function \texttt{add\_one}, each element of a 4-element float array is incremented by one. This operation, although simple, can be vectorized to exploit data-level parallelism provided by SIMD.
    \item The corresponding SIMD-enabled assembly instructions perform the addition in a parallel fashion:
    \begin{itemize}
        \item The \texttt{vbroadcastss} instruction replicates a single float value into all four elements of a SIMD register (\texttt{xmm0}), preparing the value \texttt{1.0} for parallel operations.
        \item The \texttt{vaddps} instruction performs parallel addition of the four \texttt{1.0} values in \texttt{xmm0} to the four elements of the input array pointed to by \texttt{rdi}, storing the result back into \texttt{xmm0}.
        \item The \texttt{vmovups} instruction writes the result from the SIMD register back to the memory location of the array, completing the parallel increment operation.
    \end{itemize}
    \item This SIMD approach is beneficial for computational efficiency, as it minimizes the number of sequential operations and leverages the processor's ability to handle multiple data points in a single instruction cycle.
\end{itemize}

\subsection{Counter-example}

The counter-example demonstrates a scenario where SIMD (Single Instruction, Multiple Data) instructions are not sufficient to handle all operations in a vectorized manner due to the dependency of each operation on the result of the previous one.

\begin{itemize}
    \item In the C function \texttt{many\_ops}, different arithmetic operations are applied to the elements of a float array, with each operation depending on the result of the previous one, which introduces data dependencies.
    \item The assembly code shows a sequence of SIMD instructions corresponding to the C code operations. However, due to the dependencies, each operation must be completed before the next can begin, preventing the full utilization of SIMD parallelism.
    \item The instructions include \texttt{vaddss} and \texttt{vdivss} for addition and division on scalar single-precision floating point values, respectively. These operations require the results from previous instructions, hence they cannot be parallelized.
    \item This example illustrates that while SIMD is powerful for operations that can be performed in parallel without dependencies, it is not universally applicable to all types of data processing, especially those with a sequence of dependent operations.
\end{itemize}

\subsection{How to use SIMD}
\begin{itemize}
    \item Rely on compilers (\textit{``autovectorization''})
    \item Write assembly code
    \item Use compiler \textit{``intrinsics''}
    \begin{itemize}
        \item Intrinsics look like C functions
        \item but the compiler knows how to translate them to specific assembly code
        \item \textbf{Intel intrinsics guide}
        \item \textbf{ARM intrinsics}
    \end{itemize}
\end{itemize}

$\implies \text{refer to the intrinsics guide}$
\subsection{Thread-level concurrency}

\subsection{Processes and threads}
\begin{itemize}
    \item When the OS runs an executable, it gets its own \textit{process}
    \item A single executable (if run multiple times) can have multiple independent processes
    \item Memory is virtualized: each process has its own view of the memory it owns
    \item A process can create (“spawn”) multiple \textit{threads}
    \item Like processes, each thread is an individual task from the point of view of the scheduler
    \item Within a process, threads share a same view of the process memory
\end{itemize}

\begin{itemize}
    \item Pro: Communication between threads is extremely efficient
    \begin{itemize}
        \item Just write something to memory,
        \item let other threads read it through the same pointer
    \end{itemize}
    
    \item Con: Because memory is shared, synchronizing threads is \textbf{very complex}
\end{itemize}

\subsubsection{Wrong code (1)}

The code attempts to implement a single-item buffer with:
\begin{itemize}
  \item A \texttt{push} function that stores a value in the buffer and sets a \texttt{ready} flag to indicate the buffer is full.
  \item A \texttt{pop} function that retrieves a value from the buffer and clears the \texttt{ready} flag to indicate the buffer is empty.
\end{itemize}
The \texttt{ready} flag is used for synchronization, ensuring each item is only pushed when the buffer is empty and popped when full.

\textbf{The C compiler is free to reorder this:}

\begin{verbatim}
void push(int value)
{
    while (ready == 1) {
        // wait
    }
    buffer = value;
    ready = 1;
}
\end{verbatim}

\textbf{into this:}

\begin{verbatim}
void push(int value)
{
    buffer = value;
    while (ready == 1) {
        // wait
    }
    ready = 1;
}
\end{verbatim}


\textbf{The C compiler is free to infer that this loop:}

\begin{verbatim}
while (ready == 1) {
    // wait
}
\end{verbatim}

has either zero or infinitely many iterations without side effects (UB); thus remove the loop!

\subsubsection{Wrong code (2)}

The code shows a flawed implementation for synchronizing a single-producer, single-consumer scenario using a shared buffer:
\begin{itemize}
  \item The \texttt{push} function writes to the buffer if it's not marked as ready, and sets it as ready afterward.
  \item The \texttt{pop} function reads from the buffer if it's marked as ready, and resets the flag afterward.
  \item The use of \texttt{volatile} suggests an attempt to prevent compiler optimization from reordering the access to the \texttt{ready} flag.
  \item However, this code is incorrect due to race conditions caused by lack of atomicity in flag checks and updates, potentially leading to simultaneous access to the buffer by both \texttt{push} and \texttt{pop}.
\end{itemize}

\subsubsection{Solution}

\begin{itemize}
    \item low-level: compiler intrinsics for ``atomic'' operations:
    combined operations that are performed as a single unit
    no thread will every see the memory in an intermediate state
    
    \item high-level: use libraries that correctly implement some primitives:
    locks, queues, etc.
    \begin{itemize}
        \item Posix threads (``pthreads''; Linux, MacOS)
        \item OpenMP (Open Multi-Processing; portable)
    \end{itemize}
\end{itemize}

\subsection{Distributed Computing}


\begin{itemize}
  \item In distributed computing, processes do not share memory
  \item They must communicate by explicitly sending data to each other (\texttt{send()}, \texttt{recv()}, etc.) typically over the network
\end{itemize}


\begin{itemize}
  \item \textbf{Con:} Communication is much slower than multithreading
  \item \textbf{Pros:}
  \begin{itemize}
    \item Easier to implement and reason about
    \item Scales to higher levels of parallelism
    \begin{itemize}
      \item As of today, off-the-shelf computers can have up to 2 processors $\times$ 128 cores $\times$ 2 SMT threads = 512 concurrent software threads
      \item With distributed computing, networked computers can work together in parallel
    \end{itemize}
  \end{itemize}
  \item \textbf{Libraries:}
  \begin{itemize}
    \item Message Passing Interface (MPI)
    \item \ldots % This represents an ellipsis where additional libraries might be listed
  \end{itemize}
\end{itemize}

\subsection{4. Hardware Acceleration}
\subsubsection{Graphics processing units (GPUs)}

\begin{itemize}
  \item GPUs were designed to perform the same simple, repetitive operations
  \begin{itemize}
      \item on many pixels (``fragment shaders''), or
      \item on many 3D coordinates (``vertex shaders'')
  \end{itemize}
\end{itemize}

\subsubsection{Examples (GLSL)}

\begin{verbatim}
float box(in vec2 st, in vec2 size){
    size = vec2(0.5) - size*0.5;
    vec2 uv = smoothstep(size,
                         size+vec2(0.01),
                         st);
    uv *= smoothstep(size,
                     size+vec2(0.01),
                     vec2(1.0)-st);
    return uv.x*uv.y;
}
\end{verbatim}


\subsubsection{Examples (CUDA)}

\begin{verbatim}
inline __device__ float3 roundAndExpand(float3 v, ushort *w) {
    v.x = rintf(__saturatef(v.x) * 31.0f);
    v.y = rintf(__saturatef(v.y) * 63.0f);
    v.z = rintf(__saturatef(v.z) * 31.0f);
    
    *w = ((ushort)(v.x) << 11) | ((ushort)(v.y << 5)) | (ushort)v.z;
    // approximate integer bit expansion.
    v.x = 0.032258064516f;
    v.y = 0.015873015873f;
    v.z = 0.032258064516f;
    return v;
}
\end{verbatim}

\textbullet\ GPUs were designed to perform the same simple, repetitive operations
\begin{itemize}
    \item on many pixels ("fragment shaders"), or
    \item on many 3D coordinates ("vertex shaders")
\end{itemize}
\textbullet\ they generally adopt a SIMT ("single instruction, multiple threads") model
\begin{itemize}
    \item hundreds of threads working on different sets of data
    \item but running the exact same instructions
\end{itemize}
\textbullet\ good fit for long loops performing repetitive operations \\
\textbullet\ bad fit for if/then/else

\subsubsection{How do we use GPUs?}

\textbullet\ GPUs are programmed in special-purpose languages

\textbullet\ Typically, all GPU code is compiled
\begin{itemize}
    \item during application startup,
    \item by the device driver
    \item for the specific GPU device installed (amount and subdivision of threads, memory, etc.)
\end{itemize}

\textbullet\ Two dominant players in the GPU market: nVidia and AMD

\textbullet\ Three major GPU programming languages:
\begin{itemize}
    \item CUDA (nVidia, proprietary),
    \item ROCm (AMD, open-source),
    \item OpenCL (cross-platform, open-source)
\end{itemize}
\subsubsection{Matrix Multiplication Tutorial}

\paragraph{N = 8192}
8192 $\times$ 8192 matrix multiplication \\
precision: fp64 (``double") \\
CPU: AMD Ryzen 7900 x3d

\begin{tabular}{l l l}
matmul\_1 & straightforward implementation & 2932.059 s \quad 1x \\
matmul\_2 & transpose B matrix & 357.569 s \quad 8x \\
matmul\_3 & block multiply & 67.105 s \quad 44x \\
matmul\_4 & same code as matmul\_3, SIMD & 32.876 s \quad 89x \\
matmul\_5 & OpenBLAS & 15.555 s \quad 188x 1x \\
matmul\_6 & OpenBLAS, 24 threads & 1.962 s \quad 1494x 8x \\
\end{tabular}

\paragraph{N = 32768}
32768 $\times$ 32768 matrix multiplication \\
precision: fp32 (``float") - total 4 GB per matrix \\
CPU Ryzen 7900 x3d (released Feb 2023)

\begin{tabular}{l l l}
matmul\_7 & OpenBLAS, 1 thread & 550.350 s \quad 1x \\
matmul\_8 & OpenBLAS, 24 threads & 50.577 s \quad 11x \\
matmul\_9 & cuBLAS, \textit{nVidia} A100 (Apr 2021) & 13.152 s \quad 42x \\
matmul\_9 & cuBLAS, \textit{nVidia} H100 (Mar 2022) & ? s \quad 84x? \\
\end{tabular}