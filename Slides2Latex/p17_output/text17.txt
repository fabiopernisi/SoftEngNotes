\newpage
\section{Benchmarking and static instrumentation}

\subsection{More caches and pipelines}
Virtualization of memory involves translating virtual addresses to hardware addresses, which is essential for every memory access operation. This translation relies on a page table, which is itself stored in memory. This process might suggest the need for two memory accesses per memory fetch instruction, which is inefficient.

\textbf{Solution:} To optimize this, a portion of the page table is cached in the CPU in the Translation Lookaside Buffer (TLB), reducing the frequency of memory accesses.

\subsubsection{Cache latency}
Caches are implemented at various levels to mitigate access latency. Below are some typical latencies for different technologies and operations:


\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{Typical Latency} & \\
\midrule
Instruction & $\sim0.25$ ns & $0.25$ ns \\
RAM & $\sim100$ ns & $100$ ns \\
Solid State Drive (SSD) & $\sim0.2$ ms & $200,000$ ns \\
Hard Disk Drive (HDD) & $\sim2$ ms & $2,000,000$ ns \\
Wired Ethernet (round-trip) & $\sim1$ ms & $1,000,000$ ns \\
WiFi Latency (round-trip) & $\sim10$ ms & $10,000,000$ ns \\
Same-city Internet (round-trip) & $\sim5$ ms & $5,000,000$ ns \\
Same-continent Internet (round-trip) & $\sim25$ ms & $25,000,000$ ns \\
Transatlantic Internet (round-trip) & $\sim100$ ms & $100,000,000$ ns \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Examples of caches}
Caches are ubiquitous in computing and networking:

\begin{itemize}
    \item SSDs have internal RAM caches, typically ranging from 0-4 GB.
    \item Operating systems cache files in memory to speed up access.
    \item Large content providers, such as Google, Amazon, Netflix, and Cloudflare, implement extensive caching mechanisms globally.
\end{itemize}

\textbf{Real-world Caching Example}\\
Here is an example showing the output of pinging servers located in different parts of the world:

\begin{verbatim}
ping canada.ca
PING canada.ca (205.193.117.159): 56 data bytes
64 bytes from 205.193.117.159: icmp_seq=0 ttl=228 time=20 ms
... (similar lines omitted for brevity) ...
64 bytes from 205.193.117.159: icmp_seq=4 ttl=228 time=18 ms

ping google.com.au
PING google.com.au (142.251.209.3): 56 data bytes
64 bytes from mi104s04-in-f3.1e100.net (142.251.209.3): icmp_seq=0 ttl=115 time=12.2 ms
... (similar lines omitted for brevity) ...
64 bytes from mi104s04-in-f3.1e100.net (142.251.209.3): icmp_seq=4 ttl=115 time=11.8 ms
\end{verbatim}

\textbf{Key points}
\begin{itemize}
    \item Understand the role of the Translation Lookaside Buffer (TLB) in reducing memory access latency.
    \item Be able to compare the latencies of different memory and storage technologies, from CPU instructions to transatlantic internet round-trips.
    \item Recognize the impact of caching at various system levels, including hardware (like SSDs) and software (like operating system file caches).
    \item Consider the real-world implications of caching, such as the performance differences observed when accessing local vs. remote resources.
\end{itemize}


\subsubsection{Examples of pipelines}
\begin{itemize}
\item Storage devices:
\begin{itemize}
    \item SSDs typically access data in “pages” of 4096 bytes
    \item 0.2ms SSD latency would imply a max speed of 20 MB/s
    \item instead SSDs routinely read and write 500 MB/s
\end{itemize}
\item Networks:
\begin{itemize}
    \item Network packets are typically 1500 bytes
    \item 10ms WiFi latency would imply 150 KB/s
    \item instead most WiFi networks do at least 10 MB/s
\end{itemize}
\item Browsers:
\begin{itemize}
    \item Google Chrome maintains up to 6 connections per domain
\end{itemize}
\end{itemize}


% We begin with a basic structure for a LaTeX document, such as article class.
% Since the instructions specify not to add document creation commands, they are omitted here.
% We will instead focus solely on the body content.

% The slide title "BENCHMARKING" is formatted as a section.
\subsection{Benchmarking}

When profiling applications in Unix-like systems, the `time` command is used to measure the duration of program execution.\\

\textbf{Time Command Output}\\
Running the `time` command with an application gives us three key pieces of information:
\begin{itemize}
    \item \textbf{real}: This is the elapsed "real" time or wall-clock time from start to finish of the call.
    \item \textbf{user}: This time is spent in user mode, that is, the time the CPU takes to run the application code.
    \item \textbf{sys}: This is the time spent in system mode, which means the time the CPU takes to run OS kernel operations on behalf of the application.
\end{itemize}

It's important to note that the sum of user and system times (\(user + sys\)) will typically be less than the real time, as real time also includes time spent waiting for I/O or other applications.

\subsubsection{Variance in Execution Time}
Execution time can vary between runs due to a variety of factors, such as system load, I/O overhead, or CPU scheduling. Variance is observed when running the same command multiple times:

\begin{verbatim}
time ./application
\end{verbatim}

For example, running a command to read from `/dev/random` may yield different `real`, `user`, and `sys` times upon repeated executions.\\

\textbf{Examining Variance}\\
To understand the stability and performance of a system or application, examine the variance in execution time over multiple runs. For instance, running the following command multiple times:

\begin{verbatim}
time head -n 1000000 /dev/random > /dev/null
\end{verbatim}

will provide different execution times, which is an example of the inherent variance in system performance.


% subsubsection for reasons of variance
\subsubsection{Reasons for variance}
Variability in execution time can often be attributed to several factors, which include but are not limited to:

\begin{itemize}
    \item \textbf{Power and Temperature Throttling:}
    Modern CPUs can adapt their operating speed to avoid overheating, leading to performance changes.
    
    \item \textbf{Interactions with Devices:}
    The operating system has in-memory caches for files, and storage devices have internal memory caches, which can affect the timing of operations.
    
    \item \textbf{Other Processes:}
    The necessity to share system resources among multiple processes can introduce variability in the execution time of a program.
\end{itemize}


% Effect of file caches section
\subsubsection{Effect of file caches}
File caching by the operating system can have a significant effect on the performance of file operations. Below is an example showing the execution of the `md5sum` command on a large file, first without file caches and then with the file cached in memory:

\begin{verbatim}
time md5sum 2GB_file
\end{verbatim}

In this example, the second run of `md5sum` on the same file is faster due to the effect of file caching.\\

\textbf{Key Points}\\
\begin{itemize}
    \item Understand how CPU power management can affect execution times.
    \item Be aware of the role of the OS and hardware in caching data, and how it impacts performance measurements.
    \item Remember that concurrent processes compete for resources and can cause measurable differences in performance.
    \item Practice interpreting output from system monitoring tools to diagnose performance issues.
\end{itemize}

% Inaccuracies section
\subsubsection{Inaccuracies}
\begin{itemize}
    \item executable startup is slow
    \item initialization adds overhead
    \item input and output are slow
\end{itemize}

% Executable startup is slow section
\paragraph{Executable startup is slow}
The process of starting an executable can introduce overhead that affects the accuracy of performance benchmarking, especially for applications that only run for a few milliseconds.

\begin{itemize}
    \item Compiling and running an empty program in C still incurs a measurable startup time.
    \item Interpreted languages like Python also have a significant startup time even for executing a simple exit command.
\end{itemize}


\paragraph{Initialization adds overhead}
Initial setup or parsing steps in applications add overhead, which might dominate the execution time for short-running processes.

\begin{itemize}
    \item For example, timing the execution of a simplex algorithm might inadvertently measure the time taken by the file parser rather than the algorithm itself.
\end{itemize}

\paragraph{Input and output are slow}
I/O operations can be a major source of slowdown in applications.

\begin{itemize}
    \item A Python function calculating the Riemann zeta function demonstrates this when its output is printed iteratively.
    \item The execution time drastically increases when printing results inside the loop compared to a single print at the end.
\end{itemize}

\textbf{Takeaway}\\
It is essential to understand that benchmarking should focus on the algorithm or the specific task of interest, avoiding extraneous operations that do not contribute to the performance measurement of the targeted task.

\subsubsection{Recommendations for Accurate Benchmarking}
\begin{itemize}
    \item Isolate the core functionality from initialization and termination procedures when timing an application.
    \item Minimize I/O operations during timing, especially when dealing with high-performance code.
    \item Be aware of the environment and system state, as concurrent processes and system load can impact measured performance.
\end{itemize}
\subsubsection{Aggregate measures}

\begin{itemize}
    \item if we benchmark our code on different inputs, we may want to use
    \begin{itemize}
        \item total time / average time
        \item geometric mean
        \item or other aggregate measures
        \item or some visualization (bar graphs, performance profiles, etc.)
    \end{itemize}
    \item but beware: all aggregate measures are biased
\end{itemize}

\medskip
\begin{tabular}{lcccc}
    \toprule
    & Input 1 & Input 2 & Input 3 & Average \\
    \midrule
    Version A & 2530s & 2300s & 12s & 1614s \\
    Version B & \begin{tabular}[c]{@{}c@{}}2535s\\1.002x\end{tabular} & \begin{tabular}[c]{@{}c@{}}2304s\\1.002x\end{tabular} & \begin{tabular}[c]{@{}c@{}}6s\\0.5x\end{tabular} & 1615s \\
    \bottomrule
\end{tabular}





\subsection{Static instrumentation}
When it comes to performance optimization, it is crucial to focus on specific parts of the code. Static instrumentation is a method for inserting code to measure the execution time of particular segments directly.

\subsubsection{{Why Benchmark Specific Parts of Code?}}
Benchmarking specific parts of code is important for several reasons:
\begin{itemize}
    \item To bypass the time consumed by executable startup, initialization, and I/O operations, which can obscure the true performance of the code segments.
    \item To accurately measure the performance of code sections that execute quickly and might otherwise be overshadowed by the startup overhead.
    \item To identify bottlenecks, which are the parts of the code that significantly impact the overall performance.
\end{itemize}

\paragraph{{Adding Timing Instrumentation}}
To obtain accurate measurements, developers should add timing code around the functions or blocks of interest. This allows for granular analysis of where most of the execution time is being spent.


\subsubsection{About bottlenecks}

Donald Knuth famously said, "Premature optimization is the root of all evil." This highlights the importance of identifying true bottlenecks before optimization:

\begin{itemize}
    \item A small section of code could be responsible for a disproportionate amount of the execution time, which could be a target for optimization.
    \item Focusing on optimizing non-critical parts of the code without understanding the actual performance impact can lead to wasted effort and complexity.
\end{itemize}

\paragraph{Identifying Bottlenecks with Static Instrumentation}
For instance, consider a program with three functions where static instrumentation has revealed the following time consumption:

\begin{itemize}
    \item \texttt{function\_A()} takes up 12\% of the time, with 500 lines of code.
    \item \texttt{function\_B()} consumes 60\% of the time, but it only has 20 lines of code.
    \item \texttt{function\_C()} uses 18\% of the time for its 80 lines of code.
    \item All other code combined takes up the remaining 10\% of the time.
\end{itemize}

With this information, it is clear that \texttt{function\_B()} is the primary bottleneck and should be the focus of optimization efforts. Despite its small size, it has the greatest impact on performance.


\subsubsection{Using time.time()}
In Python, `time.time()` is used to get the wall-clock time at various points during execution. Here's an example of how to use it:

\begin{verbatim}
import time
t0 = time.time()
initialize()
# ... run various functions ...
cleanup()
t5 = time.time()
print(f"Total time: {t5 - t0}")
\end{verbatim}

This method is simple but has limitations due to the granularity and precision of the time measured.


\subsubsection{Using clock\_gettime()}
In C, `clock$\_$gettime()` provides a more precise measurement of execution time. It can be used like this:

\begin{verbatim}
struct timespec t0, t1, t2, t3, t4, t5;
clock_gettime(CLOCK_MONOTONIC, &t0);
initialize();
# ... run various functions ...
cleanup();
clock_gettime(CLOCK_MONOTONIC, &t5);
print_all_clocks(&t0, &t1, &t2, &t3, &t4, &t5);
\end{verbatim}

$`CLOCK_MONOTONIC`$ ensures that the time measured is not affected by discontinuous jumps in the system time.



\subsubsection{Cumulative time}
To measure cumulative time across multiple iterations or function calls, you can sum the time differences for each specific part:

\begin{verbatim}
for i in range(1000000):
    t0 = time.time()
    function_A()
    tA += time.time() - t0
    # ... similarly for functions B and C ...
\end{verbatim}

\paragraph{Caveats in Timing}
\begin{itemize}
    \item Measuring time itself takes time, which can introduce an overhead (~40 ns for `time.time()`).
    \item The actual time may fluctuate due to various system factors.
\end{itemize}

\subsubsection{Microbenchmarks}

For functions that execute very quickly, microbenchmarking can be used to get a more accurate measure by running the function many times and measuring the total time:

\begin{verbatim}
tA, tB, tC = 0
for i in range(5000000):
    t0 = time.time()
    function_A()
    tA += time.time() - t0
    # ... similarly for functions B and C ...
\end{verbatim}

\paragraph{Key Takeaways}
\begin{itemize}
    \item When benchmarking, it's important to isolate the part of the code you're interested in.
    \item Take into account the overhead introduced by the timing mechanism itself.
    \item Use cumulative timing and microbenchmarking for a more precise performance analysis, especially for quick-executing code.
\end{itemize}

\subsubsection{Microbenchmarks limitations}

\begin{itemize}
    \item It may not make sense to call \texttt{function\_A()} in isolation
    \begin{itemize}
        \item Take \texttt{sin(x)} for example: which value of x do we choose?
        \item Always the same?
        \begin{itemize}
            \item Are we sure \texttt{sin(0)} takes as much time as \texttt{sin(0.1)}?
        \end{itemize}
        \item A random value for x?
        \begin{itemize}
            \item What if generating pseudo-random values takes more time than \texttt{sin()}?
        \end{itemize}
    \end{itemize}
    \item What about caches?
    \begin{itemize}
        \item Caches will be "hot" (already filled with relevant data)
        \item Microbenchmarking presents an over-optimistic picture of memory access times
    \end{itemize}
\end{itemize}


\subsection{Automated instrumentation: profilers}

\subsubsection{gprof}
`gprof` is a profiling tool that helps identify sections of code that take up significant execution time. It automatically instruments the code, collects data on the program's performance, and generates a report.

\paragraph{Using gprof}
To use `gprof`, follow these steps:
\begin{enumerate}
    \item Compile the program with profiling enabled using `-pg` option:\\
    \texttt{gcc -O3 -o app app.c -pg}
    \item Run the application normally:\\
    \texttt{./app}
    \item Generate the performance report:\\
    \texttt{gprof app}
\end{enumerate}

The generated report includes a flat profile and a call graph that show the time spent in each function.

\paragraph{Understanding gprof Output}
The `gprof` output report contains:
\begin{itemize}
    \item A flat profile with the time spent in each function.
    \item A call graph showing the relationships between functions and the time spent in each call.
\end{itemize}

\subsubsection{Pros and Cons of gprof}
\textbf{Pros:}
\begin{itemize}
    \item It is easy to use and integrates well with existing code bases.
    \item Provides exhaustive profile information, including time spent in each function and the call hierarchy.
    \item Generally introduces low overhead to the application's runtime.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item The overhead can increase when bottlenecks are in small, short functions due to the granularity of the profiling.
    \item The accuracy of time measurement may be limited, and the actual time spent can be affected by the profiling itself.
\end{itemize}

\paragraph{Key Takeaways}
\begin{itemize}
    \item Understand how to compile and run a program with `gprof` to collect profiling data.
    \item Be familiar with interpreting the `gprof` report to identify performance bottlenecks.
    \item Recognize the advantages of using automated tools like `gprof` for profiling, as well as their limitations.
\end{itemize}