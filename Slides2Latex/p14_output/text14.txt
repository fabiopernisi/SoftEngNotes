\newpage
\section{Software engineering practices}
\subsection{Tools for Program Correctness}
Today:
\begin{enumerate}
  \item Documentation
  \item Testing
  \item Static analysis
  \item Dynamic analysis
\end{enumerate}

\begin{itemize}
  \item Each uncovers bugs
  \item For each, there are useful tools (compilers can help!)
\end{itemize}
\subsection{Documentation}

Documentation is \textbf{Good}.

\begin{itemize}
    \item Allows others to understand your code
    \item Allows yourself (in a few weeks) to understand your own code
    \item Helps make your thought process and assumptions explicit
\end{itemize}

\subsubsection{Types of documentation}

\begin{itemize}
    \item Reference manuals
    \item Tutorials
    \item Questions and answers (Q\&A)
\end{itemize}

\subsubsection{Reference manuals}
\begin{itemize}
    \item Authoritative source of information \\
    If the code does not do what the manual says, then the code is wrong.
    
    \item Must be complete
    
    \item Must use precise language \\
    Even at the cost of legibility
    
    \item Examples: ``man'' pages, C standard, IEEE-754 specifications
\end{itemize}

\subsubsection{Tutorials}
\begin{itemize}
    \item Beginner-friendly
    
    \item Usually emphasize getting things to work quickly \\
    even at the cost of completeness
    
    \item Good tutorials do not sacrifice accuracy (but many bad ones do)
    
    \item Examples: various books (K\&R C, Think Python) and intro material
\end{itemize}

\subsubsection{Questions and answers (Q\&A)}
\begin{itemize}
    \item Prioritize quick answers to frequently asked questions
    
    \item Not exhaustive
    
    \item Examples: Stack Overflow, various FAQs
\end{itemize}
\paragraph{When reading documentation:}

\begin{itemize}
  \item as a beginner, aim for \textbf{tutorials} and Q\&As
  \item as you become an expert, you need a \textbf{reference manual}.
\end{itemize}

\paragraph{When writing documentation:}

\begin{itemize}
  \item ideally, you \textbf{write all three}!
\end{itemize}

\subsubsection{Automated documentation}

Automated documentation systems

\begin{itemize}
  \item read and parse source code
  \item find functions (methods, classes, \ldots)
  \item create a (PDF or webpage) document containing function signatures
  \item specially-formatted comments in the source code are copied into the documentation along with the corresponding function signatures
\end{itemize}

\subsubsection{Automated documentation systems}

\begin{itemize}
\item General:
    \begin{itemize}
    \item doxygen
    \item sphinx
    \end{itemize}
\item Python-specific:
    \begin{itemize}
    \item pdoc
    \item PyDoc
    \item pydoctor
    \end{itemize}
\end{itemize}

Note: Some projects choose to not use automated documentation.

\subsection{Testing}
\begin{verbatim}
/*
 * This functions returns:
 * 5 if one or both of its arguments are 5
 * 0 otherwise
 */
int five_if_some_five(int a, int b)
{
    if (a == 5)
        a = 0;
    if (b == 5)
        b = 0;
    return a | b;
}

int tests()
{
    int errors = 0;
    errors += (five_if_some_five(100, 100) != 0);
    errors += (five_if_some_five(100,   5) != 5);
    return errors;
}
\end{verbatim}

\subsubsection{Test coverage}
\begin{itemize}
    \item line coverage: \\
    is every line of code covered by some test case?
    \item branch coverage: \\
    for every conditional branch, is there a test covering each of the two possibilities \\
    (taking the branch or not taking it?)
\end{itemize}

After running the provided tests using coverage analysis tools, the results are as follows:

\begin{itemize}
    \item The \texttt{clang} command with flags \texttt{-Wall -O3 --coverage} was used to compile the source file \texttt{five.c} into the object file \texttt{five.o}, and to enable coverage analysis.
    \item Executing the compiled test binary \texttt{./test} resulted in zero errors, indicating that all the tests passed successfully.
    \item Coverage analysis was performed using \texttt{gcov}. The report generated by \texttt{gcov five.c} showed that 100\% of the lines were executed, suggesting full line coverage.
    \item When running \texttt{gcov -b five.c}, which includes branch coverage information, the report indicated that all branches were executed 100\% of the time, demonstrating full branch coverage.
    \item However, further detailed analysis using \texttt{gcov} revealed that the branches within the \texttt{five\_if\_some\_five} function were not equally covered. Specifically, the condition \texttt{if (a == 5)} was never true (0\% taken), indicating that the test cases did not cover the scenario where \texttt{a} is equal to 5.
    \item On the other hand, the branch \texttt{if (b == 5)} was taken 50\% of the time, corresponding to the test case where \texttt{b} is equal to 5.
\end{itemize}

\subsubsection{Line coverage vs. branch coverage}

\begin{verbatim}
int test()
{
    int errors = 0;

    errors += (five_if_some_five(100, 100) != 0);

    return errors;
}
\end{verbatim}

\begin{tabular}{ l l }
Line coverage: & 100\% \\
Branch coverage: & 50\%
\end{tabular}
\subsubsection{How does it work?}
Line coverage measures the percentage of code lines that have been executed during a test run. Line coverage does not ensure that all edge cases or logical branches are tested; it only indicates which lines of code were run.

\subsubsection{Limitations of test coverage measures (1)}
\begin{verbatim}[language=C, basicstyle=\footnotesize\ttfamily, breaklines=true, showstringspaces=false]
/*
 This functions returns:
 5 if one or both of its arguments are 5
 0 otherwise
 */
int WRONG_five_if_some_five(int a, int b)
{
    return a | b;
}

int test()
{
    return (WRONG_five_if_some_five(0, 5) == 5);
}
\end{verbatim}

Line coverage: 100\%
Branch coverage: 100\%
\paragraph{Explanation:}
Despite achieving 100\% line and branch coverage, the test case does not effectively validate the function's correctness. The function \texttt{WRONG\_five\_if\_some\_five} uses the bitwise OR operation, which does not implement the intended logic correctly. While the test case covers the scenario where one argument is 5 (yielding the expected result due to the nature of bitwise OR with 0 and 5), it fails to catch incorrect results for other inputs, like both arguments being 5 or neither being 5.

\subsubsection{Limitations of test coverage measures (2)}

The example illustrates that even with 100\% line and branch coverage, test coverage metrics can fail to ensure the correctness of the function. The function is intended to return 5 if any of the arguments is 5, and 0 otherwise. The test cases do not cover all possible paths, specifically the case where neither argument is 5. Consequently, it does not detect the flaw in the function logic, where the function incorrectly modifies the arguments and potentially returns the wrong result.

\subsubsection{Assertions}

\begin{itemize}
    \item Assertions are used to document (and check) assumptions made in the code.
    \item An assertion failure
    \begin{itemize}
        \item should correspond to a bug in your code,
        \item triggers an immediate crash (\texttt{abort()}) of your program.
    \end{itemize}
\end{itemize}
\begin{verbatim}
#include <assert.h>

int gcd(int a, int b)
{
    if (a < b) {
        int t = a;
        a = b;
        b = t;
    }

    while (b != 0) {
        assert(a >= b); // <--- this should always be true
        int t = a % b;
        a = b;
        b = t;
    }

    return a;
}
\end{verbatim}

\subsubsection{Disabling assertions}

\begin{verbatim}
clang -DNDEBUG -Wall -O3 -o main main.c
\end{verbatim}
(equivalent to \texttt{\#define NDEBUG} at the beginning of every file)

\subsubsection{Error vs assertion failure}

\begin{itemize}
  \item an error happens when, for external reasons, your program cannot run
  \begin{itemize}
    \item examples: out of memory, file cannot be read, network unreachable
  \end{itemize}
  \item an assertion fails if a fundamental assumption in your code is violated
  \begin{itemize}
    \item indicates a bug in your code
  \end{itemize}
\end{itemize}
\subsection{Static Analysis}

\begin{itemize}
  \item \textbf{Static analysis} operates on the source code
  \begin{itemize}
    \item (before any assembly or executable code is produced)
  \end{itemize}
  \item Compilers do advanced case analysis on the code
  \begin{itemize}
    \item (in order to produce faster code)
  \end{itemize}
  \item The same analysis can be used to find (potential) bugs

  \item Not an exact science
  \begin{itemize}
    \item Relies on heuristics to detect hazardous code
    \item Suffers from false negatives and false positives
  \end{itemize}
\end{itemize}
\subsubsection{Clang's static analyzer}
If you use a Makefile, run

\begin{verbatim}
scan-build make
\end{verbatim}
\textgreater{> result}
\subsubsection{Python linters}
\begin{itemize}
\item A ``linter'' is a static analyzer
\item Typically, linters enforce a specific coding style
\end{itemize}

Examples:
\begin{itemize}
\item Pylint
\item flake8
\item mypy (adds static type checking)
\end{itemize}
\begin{verbatim}
def fib(n):
    a, b = 0, 1
    while a < n:
        yield a
        a, b = b, a+b
\end{verbatim}

\begin{verbatim}
def fib(n: int) -> Iterator[int]:
    a, b = 0, 1
    while a < n:
        yield a
        a, b = b, a+b
\end{verbatim}
\subsection{Dynamic Analysis}
\begin{itemize}
    \item Dynamic analysis operates on the running executable (during testing)
        \begin{itemize}
            \item by adding runtime checks
            \item can find more bugs than static analysis...
            \item ... but only for those bugs are triggered by some test!
        \end{itemize}
\end{itemize}
\subsubsection{Sanitizers}

With \emph{sanitizers}, runtime checks are added by the \emph{compiler}.
\paragraph{UBSan}

\begin{itemize}
    \item The ``undefined behavior sanitizer'' detects many types of undefined behavior (at runtime)
    \item triggers an immediate crash (with an explanation message)
    \item Pass \texttt{"-fsanitize=undefined"} to \texttt{gcc} or \texttt{clang}
\end{itemize}

\paragraph{Demonstration of UBSan in Action}

The code sample illustrates the usage of the Undefined Behavior Sanitizer (UBSan) by comparing the output of a program compiled without and with UBSan.

\begin{itemize}
    \item The function \texttt{f(int a, int b)} attempts to divide \texttt{a} by \texttt{b}. When compiled without UBSan and executed with \texttt{b} as zero, it results in a floating point exception. This is a runtime error that occurs due to division by zero, an undefined behavior in C.
    
    \item When compiled with UBSan using \texttt{clang -O3 -fsanitize=undefined -o timetravel timetravel.c}, the program terminates with a runtime error message indicating the cause of the error: division by zero. UBSan provides a stack trace pointing to the exact location in the code where the undefined behavior occurred.
    
    \item The output with UBSan includes details about the file name, line number, and even the assembly instruction pointer location, which are invaluable for debugging.
    
    \item UBSan causes the program to abort execution instead of continuing past the undefined behavior, which helps prevent further errors or unpredictable behavior in the program's execution.
\end{itemize}
\paragraph{Pros}

\begin{itemize}
    \item Fixes the anything-can-happen problem with undefined behavior (we get a crash with an explanation instead)
    \item No false positives
\end{itemize}

\paragraph{Cons}

\begin{itemize}
    \item Not all types of undefined behavior detected (most are)
    \item Does not always stop the compiler from exploiting undefined behavior
    \item Overhead (\textasciitilde3x slowdown)
    \item Needs good tests
\end{itemize}

\subsubsection{ASan}

\begin{itemize}
    \item The ``address sanitizer'' detects many types memory access errors (at runtime)
    \item Separate from UBSan because it uses different mechanisms
    \item triggers an immediate crash (with an explanation message)
    \item Pass ``-fsanitize=address'' to gcc or clang
\end{itemize}

\paragraph{Example}
In the function \texttt{f}, a local buffer is declared, and a string is written into it. The function then returns a pointer to this local buffer, which is a mistake because the buffer's scope ends when the function returns, rendering the pointer invalid.

When the program is compiled without sanitization, this bug may go unnoticed, potentially leading to undefined behavior at runtime. However, when compiled with ASan using \texttt{clang -O3 -fsanitize=address -o bug bug.c}, the error is caught:

\begin{itemize}
    \item ASan outputs an error message indicating that there was a read of size 1 at a memory address that is part of the stack (i.e., the local \texttt{buffer}) after the scope of the \texttt{buffer} has ended.
    \item The error message includes a stack trace that shows where the invalid read occurred, in this case, the \texttt{puts} function called from \texttt{main}, which attempts to print the contents of the now non-existent buffer.
    \item The output also pinpoints the exact location in the source code where the invalid access occurs, aiding in debugging the issue.
\end{itemize}

\paragraph{ASan detects (1)}

\begin{itemize}
  \item Out-of-bounds accesses to heap, stack and globals
    \begin{verbatim}
       int a[10];
       printf("%d\n", a[20]);
    \end{verbatim}

  \item Use-after-free
    \begin{verbatim}
       free(pointer);
       printf("%d\n", *pointer);
    \end{verbatim}
\end{itemize}
\paragraph{ASan detects (2)}

\begin{itemize}
  \item Use-after-return
    \begin{verbatim}
       int *f()
       {
         int a[10];
         return a;
       }
       
       void g()
       {
         int *pointer = f();
         printf("%d\n", pointer[0]);
       }
    \end{verbatim}

  \item Use-after-scope
    \begin{verbatim}
       void g()
       {
         int *pointer;
         
         if (1)
         {
           int a[10];
           pointer = a;
         }
          
         printf("%d\n", pointer[0]);
       }
    \end{verbatim}
\end{itemize}
\paragraph{ASan detects (3)}

\begin{itemize}
    \item Double-free, invalid free
    
    \begin{verbatim}
void *other_pointer = pointer;
free(pointer);
free(other_pointer);

int a[10];
free(a);
    \end{verbatim}
    
    \item Memory leaks
    
    \begin{verbatim}
void f()
{
    void *ptr = malloc(10);
}
    \end{verbatim}
    
\end{itemize}

\textbf{Pros}
\begin{itemize}
    \item Detects most memory issues
    \item No false positives
\end{itemize}

\textbf{Cons}
\begin{itemize}
    \item Not every memory issue detected (many are)
    \item Overhead ($\sim$2x slowdown)
    \item Needs good tests
\end{itemize}

\subsubsection{Valgrind}

\begin{itemize}
    \item Valgrind adds runtime checks on already-compiled executable.
    
    \item It is a hybrid interpreter / JIT compiler for machine code.
    
    \item It adds checks around all memory accesses.
    \begin{itemize}
        \item Detects uses of invalid pointers (incl. uninitialized memory)
        \item Detects memory leaks (at exit)
    \item Valgrind requires compiling with the ``-ggdb'' option (gcc / clang)
    \end{itemize}
\end{itemize}

\subsubsection{Pros}
\begin{itemize}
\item Detects almost all memory issues (that happen at runtime)
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
\item Large overhead ($\sim$10x slowdown)
\item Needs good tests
\end{itemize}

\subsection{Fuzzing}

\subsubsection{We need good tests}

\begin{itemize}
    \item Dynamic analysis tools are useful 
    \item but only if we have good test cases
    \item and enough of them
    \item $\Rightarrow$ How do we generate good tests?
\end{itemize}

On a basic level, a fuzzer proceeds as follows:

\begin{enumerate}
    \item take a (mostly valid) example input file
    \item run the tested program with that input file
    \item check for crashes
    \item modify the input file:
    \begin{itemize}
        \item random modifications
        \item truncations, duplications
        \item mergers with other example input files
    \end{itemize}
    \item go back to 2
\end{enumerate}

\paragraph{Advanced fuzzers}

\begin{itemize}
    \item use test coverage techniques \\
    to determine which input files are ``interesting'', \\
    in that they cover previously-uncovered source code
    \item use static analysis techniques \\
    to determine input file modifications that could trigger specific code branches
\end{itemize}

\paragraph{AFL++}
\begin{itemize}
    \item open source project (\url{https://aflplus.plus/})
    \item takes as an input a directory with many (mostly valid) example input files
    \item generates modified input files that (try to) yield crashes
\end{itemize}