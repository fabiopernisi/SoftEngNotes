\newpage
\section{Data structures: Arrays, Linkned Lists}

\subsection{Abstract Data Types and Data Strucutres}

An abstract data type (ADT) is a model for data containers that defines the type's behavior in terms of possible values, operations, and behavior of those operations. Examples include:

\begin{itemize}
    \item In Python: \texttt{list}, \texttt{dict}, \texttt{set}, etc.
    \item In C++: \texttt{std::vector}, \texttt{std::unordered\_map}, etc.
\end{itemize}

ADTs specify the operations supported but not the implementation details, such as how data is stored or how the operations are executed.

\subsubsection{Data Structure Implementation}
A data structure is a concrete implementation of an ADT that details how data is arranged in memory and the specific algorithms used for operations, which allows us to compute the computational complexity.


\subsection{Lists}
Lists represent one of the simplest forms of ADTs, essentially a collection of ordered elements that support operations like:
\begin{itemize}
    \item Storing multiple elements together.
    \item Optionally appending, inserting, or deleting elements.
    \item Accessing or modifying all elements in sequence or at a specific index (with varying efficiency).
\end{itemize}

\subsection{Arrays}
\subsubsection{Static arrays}
Static arrays are fixed-size, contiguous memory data structures that provide:

\begin{itemize}
    \item Constant-time (\(O(1)\)) access or modification of elements via an index.
    \item Linear-time (\(O(n)\)) traversal to access or modify all elements due to the direct addressing capability.
\end{itemize}

\subsubsection{Dynamic arrays}
Dynamic arrays extend static arrays to allow variable size \( n \), which introduces additional complexity:

\begin{itemize}
    \item Theoretical complexity \( O(n) \) for operations that change the size of the list.
    \item This affects the complexity of appending, inserting, or deleting elements.
\end{itemize}

\paragraph{Key Takeaways}
\begin{itemize}
    \item ADTs define the what, not the how, of data manipulation.
    \item Data structures implement the specifics of ADTs and determine the efficiency of operations.
    \item Understanding the difference between ADTs and data structures is crucial for choosing the right implementation for the needed operations, especially considering the performance implications.
\end{itemize}

\subsubsection{Size increase}

\begin{itemize}
  \item An array occupies the bytes in memory:
  \begin{itemize}
    \item from ``array\_address'' % Using double quotes for literal strings
    \item to \( \text{array\_address} + n \times \text{element\_size} - 1 \)
  \end{itemize}
  \item Increasing \( n \) has \( O(n) \) complexity, because the memory at
  \[ \text{array\_address} + n \times \text{element\_size} \]
  may be occupied by other data
  \item In that case, the dynamic array must be relocated elsewhere in memory (changing \( \text{array\_address} \))
  \item All \( n \times \text{element\_size} \) bytes must be copied to the new location, hence \( O(n) \) complexity
\end{itemize}




\subsubsection{Size decrease}

\begin{itemize}
    \item Conversely, if the memory before and/or after an array is free,
    \begin{itemize}
        \item we may want to move the array
        \item in order to create a larger block of free memory
    \end{itemize}
    \item Not doing this may cause ``memory fragmentation''
\end{itemize}

\noindent In theory:

\begin{center}
\begin{tabular}{l c}
\hline
operation & complexity \\
\hline
access/modify element at arbitrary index & $O(1)$ \\
increase $n$ & $O(n)$ \\
decrease $n$ & $O(n)$ \\
append an element & $O(n)$ \\
discard last element & $O(n)$ \\
insert an element & $O(n)$ \\
delete an element & $O(n)$ \\
\hline
\end{tabular}
\end{center}

\noindent In practice:

\begin{itemize}
    \item Almost all implementations ignore fragmentation due to shrinking \\
    (no move when decreasing $n > 0$)
\end{itemize}

\begin{center}
\begin{tabular}{l c}
\hline
operation & complexity \\
\hline
access/modify element at arbitrary index & $O(1)$ \\
increase $n$ & $O(n)$ \\
decrease $n$ & $O(1)$ \\
append an element & $O(n)$ \\
discard last element & $O(1)$ \\
insert an element & $O(n)$ \\
delete an element & $O(n)$ \\
\hline
\end{tabular}
\end{center}

\subsubsection{Over-allocation}
Over-allocation is a strategy used in managing dynamic arrays where more memory is allocated than is immediately necessary.

\begin{itemize}
    \item We distinguish between two quantities:
    \begin{itemize}
        \item The user-visible size \( n \): the number of elements the user thinks the array can hold.
        \item The allocated size \( a \): the actual amount of memory reserved for the array.
    \end{itemize}
    \item When the user requests an increase in the array size to \( n' \), if \( n' \leq a \), the dynamic array can accommodate the new size without needing additional memory allocation.
    \item The allocated size \( a \) is not incremented by small amounts (no \( a' = a + 1 \)).
    \item Instead, to optimize performance and minimize frequent allocations, \( a \) is typically increased exponentially, often doubling (\( a' = 2a \)) when the array needs to grow. This approach reduces the number of times the array must be resized and copied over the lifetime of the array.
\end{itemize}

\paragraph{Benefits of Over-Allocation}
\begin{itemize}
\item \textbf{Efficiency:} By allocating more space than immediately necessary, we reduce the frequency of memory allocation operations, which are costly in terms of performance.
\item \textbf{Growth Management:} Over-allocation manages the growth of an array more smoothly, preventing the system from reallocating memory for every single element added once the initial capacity is exceeded.
\end{itemize}

\paragraph{Implications of Over-Allocation}
\begin{itemize}
\item While over-allocation can improve efficiency, it does mean that the data structure may use more memory than what is strictly required for its current elements.
\item This trade-off between memory usage and performance is often acceptable, especially since modern computing devices typically have memory resources to spare.
\end{itemize}



\subsubsection{Exponential memory allocation}
Exponential memory allocation is a strategy used to manage the growth of data structures such as dynamic arrays efficiently. This method helps minimize the number of memory reallocations, which are computationally expensive operations.

\begin{itemize}
    \item In dynamic arrays, we differentiate between the user-visible size (\( n \)) and the allocated memory size (\( a \)).
    \item When an array's size needs to be increased from \( n \) to \( n' \), if \( n' \) is less than or equal to \( a \), no additional memory allocation is required.
    \item The allocated size (\( a \)) is not incremented linearly but rather grows exponentially, typically doubling when a size increase is necessary.
\end{itemize}

\paragraph{Illustration of Exponential Allocation}
\begin{itemize}
    \item Initially, an array may have an allocated size (\( a \)) larger than the user-visible size (\( n \)).
    \item As elements are added and \( n \) grows, if \( n \) exceeds \( a \), the array is reallocated with twice the previous allocated size (\( a' = 2a \)).
    \item This approach is visualized in the sequence of diagrams, where the allocated size (\( a \)) increases to accommodate the growing number of elements (\( n \)).
\end{itemize}

\paragraph{Advantages of Exponential Allocation}
\begin{itemize}
    \item \textbf{Efficiency:} Reduces the frequency of memory allocation calls, which can be costly in terms of performance.
    \item \textbf{Predictability:} Provides a predictable growth pattern, simplifying the management of memory for dynamic arrays.
    \item \textbf{Space Utilization:} By allocating more space upfront, the overhead of reallocation is reduced, utilizing memory more effectively in the long run.
\end{itemize}

\paragraph{Exponential Allocation in Practice}
\begin{itemize}
    \item When an array with \( n = 3 \) grows to \( n = 4 \), the previously allocated size (\( a = 4 \)) is sufficient.
    \item However, when \( n \) reaches \( 5 \), the array is reallocated with \( a = 8 \), doubling the previous allocation to maintain the exponential growth strategy.
    \item This pattern continues as \( n \) grows, with the next reallocation occurring when \( n \) exceeds \( 8 \), increasing \( a \) to \( 16 \).
\end{itemize}

\paragraph{key concepts}
\begin{itemize}
    \item Understand how exponential allocation minimizes the number of memory allocations needed as an array grows.
    \item Be prepared to explain the trade-off between immediate memory usage and long-term efficiency gains.
    \item Recognize the patterns of growth in allocated size versus user-visible size in dynamic arrays.
\end{itemize}






\subsubsection{Pros and cons of exponential allocation}

\textbf{Pros:}
\begin{itemize}
    \item Reduces the frequency of memory reallocations.
    \item Ensures that the allocated size \( a \) is always within a factor of the actual size \( n \), specifically \( a \leq 2n \).
    \item Provides an \( O(1) \) amortized time complexity for increasing the size of the array.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item Can lead to memory waste as the allocated space might not always be fully utilized.
    \item The wasted space is bound by the equation \( a = 2^{\lceil \log_2(n) \rceil} \), which can be significant for large arrays.
\end{itemize}



\subsubsection{Complexity of exponential allocation}
\begin{itemize}
    \item Starting with an empty array and incrementing its size \( n \) times leads to at most \( k = \lceil \log_2(n) \rceil \) memory allocation moves.
    \item The total cost of these moves is a power series sum: \( 1 + 2 + 4 + \ldots + 2^{k-1} = 2^k - 1 \).
    \item This sum is less than or equal to \( 2n \), ensuring that the total time complexity for \( n \) size increments is \( O(n) \).
    \item The amortized time complexity for each size increment is \( O(1) \).
\end{itemize}

\paragraph{Operational Complexities}
\begin{itemize}
    \item Access or modification at an arbitrary index: \( O(1) \).
    \item Increase or decrease \( n \): \( O(1) \) amortized.
    \item Append an element: \( O(1) \) amortized.
    \item Discard the last element: \( O(1) \).
    \item Insert or delete an element: \( O(n) \) due to the need to shift elements.
\end{itemize}

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{operation} & \textbf{complexity} \\
\hline
access/modify element at arbitrary index & $O(1)$ \\
increase $n$ & $O(1)$ amortized \\
decrease $n$ & $O(1)$ \\
append an element & $O(1)$ amortized \\
discard last element & $O(1)$ \\
insert an element & $O(n)$ \\
delete an element & $O(n)$ \\
\hline
\end{tabular}
\end{center}

\paragraph{Key concepts}
\begin{itemize}
    \item Understand the balance between efficient allocation and potential memory waste.
    \item Be able to calculate the upper bound of memory allocation and the amortized cost for array resizing.
    \item Recognize the time complexities associated with different array operations
\end{itemize}

\subsubsection{Virtual memory}
Virtual memory changes the complexity landscape for operations on dynamic arrays by allowing us to remap memory instead of physically moving it.

\paragraph{Virtual Memory in Asymptotic Complexity}
\begin{itemize}
    \item The asymptotic complexity of changing the size \( n \) of an array involves an \( O(n) \) operation due to memory moves.
    \item However, with virtualized memory, physical byte movement is unnecessary.
    \item Instead, the page table remaps the physical memory associated with a virtual address to a new virtual address, which in practice, can make memory moves essentially \( O(1) \).
\end{itemize}

\subsubsection{Remapping Memory Using the Page Table}
\begin{itemize}
    \item \textbf{Pros:} Memory move becomes \( O(1) \) in practice due to virtual memory's ability to remap addresses instead of moving bytes.
    \item \textbf{Cons:} It requires a system call to the OS kernel to change the page table, which introduces:
    \begin{itemize}
        \item A context switch, potentially polluting the CPU caches.
        \item A significant fixed cost due to the system call overhead.
    \end{itemize}
    \item This operation is therefore only performed when \( n \) grows very large, avoiding frequent system calls.
    \item To manage growth, \( a' \) is set to \( a + K \) for some large \( K \), balancing between memory efficiency and system call overhead.
\end{itemize}

For very large \( n \) (in the order of multiple megabytes), the operational complexities can be optimized:
\begin{itemize}
    \item Access or modification at an arbitrary index remains \( O(1) \).
    \item Increasing or decreasing \( n \) can be \( O(1) \) with virtual memory's remapping capabilities.
    \item Appending or discarding the last element is \( O(1) \).
    \item Inserting or deleting an element within the array still has \( O(n) \) complexity due to the need to shift elements in the array.
\end{itemize}

\begin{center}
    \begin{tabular}{ l c c }
    \hline
    \textbf{operation} & \textbf{complexity} \\
    \hline
    access/modify element at arbitrary index & \( O(1) \) \\
    increase \( n \) & \( O(1) \) \\
    decrease \( n \) & \( O(1) \) \\
    append an element & \( O(1) \) \\
    discard last element & \( O(1) \) \\
    insert an element & \( O(n) \) \\
    delete an element & \( O(n) \) \\
    \hline
    \end{tabular}
\end{center}

\paragraph{Key Concepts}
\begin{itemize}
    \item Understand the role of virtual memory in optimizing array resizing operations.
    \item Discuss the pros and cons of using virtual memory to remap addresses instead of moving data physically.
    \item Recognize the impact of system calls and context switches on the performance of dynamic arrays.
\end{itemize}


\subsection{Linked Lists}
Linked lists are fundamental data structures that implement a collection of elements, allowing for variable size \( n \).\\

\textbf{Linked lists support:}
\begin{itemize}
    \item Inserting, deleting, and modifying elements in \( O(1) \) time complexity, regardless of the position in the list.
    \item Accessing or modifying all elements in order with \( O(n) \) time complexity.
\end{itemize}

\textbf{Linked lists do not natively support:}
\begin{itemize}
    \item Accessing or modifying an element at an arbitrary index, known as "random access," which would have \( O(n) \) complexity if implemented sequentially.
\end{itemize}


\subsubsection{Doubly-linked lists}

Doubly-linked lists are an extension of linked lists where each element has a reference to both the previous and the next element.

\begin{verbatim}
struct element {
    struct payload data;
    struct element *prev;
    struct element *next;
};
\end{verbatim}

Functionality such as insertion is demonstrated in the provided code snippet, showing how new elements are linked to their predecessors and successors.

\paragraph{Comparative Operations}
\begin{tabular}{lcc}
\hline
\textbf{operation} & \textbf{dynamic array} & \textbf{doubly-linked list} \\
\hline
access/modify element at arbitrary index & \(O(1)\) & \(O(n)\) \\
increase \(n\) & \(O(1)\) & \(O(1)\) \\
decrease \(n\) & \(O(1)\) & \(O(1)\) \\
append an element & \(O(1)\) & \(O(1)\) \\
discard last element & \(O(1)\) & \(O(1)\) \\
insert an element & \(O(n)\) & \(O(1)\) \\
delete an element & \(O(n)\) & \(O(1)\) \\
\hline
\end{tabular}


\subsubsection{Memory management considerations}

Memory allocation is slow

\begin{verbatim}
struct element *x = malloc(sizeof(struct element));
\end{verbatim}

compared to dynamic arrays' fast case

\begin{verbatim}
if (new_n <= d->sz) {
    d->n = new_n;
    return SUCCESS;
}
\end{verbatim}

% The complexity table
\begin{tabular}{lcc}
\hline
\textbf{operation} & \textbf{dynamic array} & \textbf{doubly-linked list} \\
\hline
access/modify element at arbitrary index & $O(1)$ & $O(n)$ \\
increase $n$ & $O(1)$ & $O(1)$ \\
decrease $n$ & $O(1)$ & $O(1)$ \\
append an element & $O(1)$ & $O(1)$ \\
discard last element & $O(1)$ & $O(1)$ \\
insert an element & $O(n)$ & $O(1)$ \\
delete an element & $O(n)$ & $O(1)$ \\
\hline
\end{tabular}


\subsubsection{Memory caches considerations}
Cache usage is an important aspect of performance optimization. For linked lists, memory caches do not provide the same benefits as for dynamic arrays due to the non-contiguous memory allocation of elements.

\begin{itemize}
    \item With deep pipelines and good branch prediction, a processor can pre-fetch elements in a dynamic array, but it cannot do so as effectively with linked lists due to data dependencies.
\end{itemize}
While linked lists have fewer applications than one might expect, they can be extremely useful in situations where their specific advantages can be leveraged.


\subsubsection{More options}
Beyond simple linked lists, other data structures offer different performance trade-offs:

\begin{itemize}
    \item Indirection, such as a dynamic array of pointers, allows for a level of indirection which can simplify memory management.
    \item In-memory tree data structures, like binary trees or n-ary trees, provide hierarchical organization and can be more efficient for certain types of operations.
\end{itemize}

\begin{verbatim}
struct nary_node {
    struct payload data;
    struct nary_node *children[MAX_CHILDREN];
};

struct dll_node {
    struct payload data;
    struct dll_node *prev_sibling;
    struct dll_node *next_sibling;
    struct dll_node *first_child;
};
\end{verbatim}

\paragraph{Key concepts}
\begin{itemize}
    \item Understand the trade-offs between different data structures regarding memory allocation and access patterns.
    \item Recognize that linked lists can incur additional overhead due to memory allocation and may not benefit as much from cache optimization.
    \item Consider alternative data structures like indirection or in-memory trees for more complex needs or to improve performance.
\end{itemize}