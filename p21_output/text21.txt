\newpage
\section{Tries, hash tables, spatial data structures}

\subsection{Associative Arrays}
Associative arrays, also known as maps or dictionaries, are fundamental data structures in computer science and programming.

\subsection*{Definition and Operations}
\begin{itemize}
    \item Associative arrays are collections of key-value pairs or tuples.
    \item Keys can be any string of bits, such as integers or strings.
    \item Values are data associated with the keys.
    \item Operations supported by associative arrays include:
    \begin{itemize}
        \item Insertion: Adding a new key-value pair.
        \item Deletion: Removing an existing key-value pair.
        \item Lookup: Finding the value associated with a given key.
    \end{itemize}
\end{itemize}



\subsubsection{Naive implementation}
A naive implementation of an associative array can be achieved using a simple list of key-value tuples.

\subsubsection{Complexity}
The complexity of operations in a naive implementation varies based on the underlying data structure:\\

\begin{tabular}{lccc}
\hline
& Insertion & Deletion (after lookup) & Lookup \\
\hline
Linked list & $O(1)$ & $O(1)$ & $O(n)$ \\
Dynamic array & $O(1)$ & $O(n)$ & $O(n)$ \\
\hline
\end{tabular}



\subsection{Implementations using a total order on keys}
\subsubsection{Total order on keys}
\textbf{Key comparison}
\begin{itemize}
    \item Associative arrays require a mechanism to compare keys (e.g., \( key_i \leq key_j \)).
    \item In practice, key comparison is feasible by interpreting keys as large integers.
    \item Specialized comparison operators may be more efficient for constant-sized keys.
    \item Key space could potentially be infinite, accommodating arbitrary-sized keys.
\end{itemize}


\subsubsection{Sorted dynamic array of (key, value) tuples}

\begin{itemize}
    \item Sorted dynamic arrays maintain keys in a total order (e.g., \( key_0 \leq key_1 \leq \dots \leq key_n \)).
    \item Lookup operations can use binary search, leading to \( O(\log n) \) complexity.
\end{itemize}

\begin{tabular}{lccc}
\hline
\textbf{Data Structure} & \textbf{Insertion} & \textbf{Deletion (after lookup)} & \textbf{Lookup} \\
\hline
Linked list             & O(1)               & O(1)                            & O(n)             \\
Dynamic array           & O(1)               & O(n)                            & O(n)             \\
Sorted dynamic array    & O(n)               & O(n)                            & $O(\log(n))$       \\
\hline
\end{tabular}

\subsubsection{Binary search tree}

\begin{itemize}
    \item A binary search tree (BST) maintains a total order invariant within its structure.
    \item Each node \( i \) guarantees \( key_j \leq key_i \) for all \( j \) in its left subtree and \( key_j > key_i \) for all \( j \) in its right subtree.
    \item The shape of the BST can vary greatly depending on the insertion order, which can impact performance.
\end{itemize}

\subsubsection{Self-balancing binary search tree}

\begin{itemize}
    \item AVL trees
    \item Red-black trees
    \item B-trees, splay trees, treaps, \ldots
\end{itemize}

\begin{tabular}{lccc}
\hline
\textbf{Data Structure}     & \textbf{Insertion}         & \textbf{Deletion (after lookup)} & \textbf{Lookup}      \\
\hline
Linked list                 & O(1)                       & O(1)                            & O(n)                 \\
Dynamic array               & O(1)                       & O(n)                            & O(n)                 \\
Sorted dynamic array        & O(n)                       & O(n)                            & $O(\log(n))$           \\
Binary search tree          & O(n)                       & O(n)                            & O(n)                 \\
AVL tree                    & $O(\log(n))$                 & $O(\log(n))$                      & $O(\log(n))$           \\
Red-black tree              & $O(\log(n))$                 & $O(\log(n))$                      & $O(\log(n))$           \\
\hline
\end{tabular}


\begin{itemize}
    \item Cache behavior: ok, not great (similar to other binary tree structures, e.g., heap)
\end{itemize}


\subsection{Implementations using keys bits - tries}

\subsubsection{Trie}

\begin{itemize}
    \item A trie, also known as a prefix tree, is a tree structure that uses static arrays of size \(2^T\) for routing.
    \item Keys are divided into chunks or "letters" of \(T\) bits.
    \item Each chunk provides an index into a node's static arrays.
    \item The path from the root to a leaf represents the complete key.
\end{itemize}

\paragraph{Trie Operations}
A Trie is a specialized tree used to store associative arrays, where the keys are usually strings. Inserting a key into a trie involves several steps:
\begin{enumerate}
    \item A key is represented by a series of characters. In the context of a trie, each character can be thought of as a node in the tree.
    \item To insert a key, we start at the root of the trie and traverse the tree by following the path defined by the key's characters.
    \item At each step, we look at the current character in the key and move to the corresponding child node.
    \item If a child node corresponding to the current character does not exist, we create a new node.
    \item We continue this process until all characters in the key have been processed.
    \item Once we reach the end of the key, we mark the final node as an end node, which signifies the completion of a key insertion.
\end{enumerate}


\paragraph{Trie Insertion Example}
Consider the insertion of the hexadecimal keys \(0x9f2\), \(0x8cd\), and \(0x532\) with corresponding values \(V1\), \(V2\), and \(V3\) into a trie where each node represents 4 bits of the key.

\begin{enumerate}
    \item For the key \(0x9f2\) with value \(V1\), we would insert the 4-bit segments (2, f, 9) one by one, creating a path in the trie.
    \item Next, for the key \(0x8cd\) with value \(V2\), we would insert the segments (d, 8, c) into the trie, potentially sharing nodes with the previous key if any 4-bit segments are the same.
    \item Finally, for the key \(0x532\) with value \(V3\), we would insert the segments (2, 3, 5) into the trie, again sharing nodes with any previously inserted keys that have matching segments.
\end{enumerate}


\paragraph{Complexity Analysis}
\begin{itemize}
    \item Tries are highly efficient for lookups, insertions, and deletions.
    \item They provide a means to search for keys in a dataset of strings in \(O(m)\) time complexity, where \(m\) is the length of the key to be searched.
    \item They are particularly useful for implementing dictionaries with prefix-based lookups.
\end{itemize}


\subsubsection{Key space}
The key space of an associative array refers to the range of all possible keys that can be used within the array. The concept of \emph{dense} and \emph{sparse} key spaces is crucial in the context of data structure efficiency.

\begin{itemize}
    \item Let \( K \) be the set of all possible keys, and \( n \) the number of tuples in the associative array.
    \item A key space is considered \emph{sparse} if \( n \ll K \), meaning that the number of actual keys used is much less than the possible number of keys.
    \item If the number of used keys is comparable to the number of possible keys, the key space is considered \emph{dense}.
\end{itemize}

\subsubsection{``Dense'' key space}

\begin{itemize}
    \item In a \emph{dense} key space, most of the potential keys have values associated with them.
    \item Operations such as insertion, deletion, and lookup can be done in \( O(\log{T} n) \) time, where \( T \) is a factor related to the trie branching (typically the size of the alphabet or character set).
    \item For a dense key space, a static array can be more efficient as the operations become \( O(1) \).
\end{itemize}

\subsubsection{``Sparse'' key space}

\begin{itemize}
    \item Tries are particularly useful for \emph{sparse} key spaces where a static array of the entire key space would be impractically large.
    \item The complexity of trie operations in a sparse key space is not dependent on the number of entries but on the key size and the trie branching factor \( T \).
    \item A sparse key space can lead to high memory overhead in the worst case, where every leaf node might contain only a single tuple.
\end{itemize}
A trie provides a balance between the two extremes, offering more efficient memory usage in sparse key spaces without sacrificing the performance benefits of a static array in dense key spaces.

\subsection{Implementations using key bits - hash tables}
Hash tables are a way to implement associative arrays by mapping a large, potentially sparse key space into a smaller, dense index space using a hash function.

\subsubsection{Hash function}
\begin{itemize}
    \item A hash function \( h \) is a mapping from the key space \( K \) to an index space \( U \), where \( U \subseteq \mathbb{N} \) and the size of \( U \) is much smaller than \( K \).
    \item A good hash function distributes keys evenly across the index space, minimizing the chance of collisions.
    \item Examples of hash functions include taking a subset of the key bits or using modular arithmetic, such as \( h(k) = k \mod m \) for some integer \( m \).
\end{itemize}

\subsubsection{Hash table}
\begin{itemize}
    \item A hash table uses a static array of size \( |U| \) to store values.
    \item Each key-value tuple \((k, v)\) is stored in the array at the index given by \( h(k) \).
    \item Due to the surjective nature of hash functions, collisions can occur where multiple keys are hashed to the same index.
\end{itemize}

\subsubsection{Dealing with collisions}
When implementing associative arrays using hash tables, collisions occur when two different keys hash to the same index. Here are some strategies to handle collisions:

\paragraph{Collision Resolution Strategies}
\begin{itemize}
    \item The hash table can be a static array of linked lists. When a collision occurs, the colliding elements are added to the list at the hashed index.
    \item Alternatively, a static array of dynamic arrays (or vectors) can be used, where colliding elements are appended to the dynamic array at the hashed index.
\end{itemize}

\paragraph{Search Complexity in Case of Collisions}
\begin{itemize}
    \item When collisions occur, a linear search within the linked list or dynamic array at the hash index is performed.
    \item If \( c \) is the maximum number of collisions at a hash index, the worst-case search complexity is \( O(c) \).
    \item In the worst case where all elements collide at the same index, \( c \) can be equal to \( n \), making the complexity \( O(n) \).
\end{itemize}

\paragraph{Insertion and Lookup Operations}
\begin{itemize}
    \item To insert a key-value pair \((k, v)\), hash the key to find the index and add the pair to the appropriate data structure at that index.
    \item To lookup a value for a key, hash the key to find the index, and then search through the data structure at that index to find the value.
\end{itemize}




\subsubsection{Open addressing as a collision resolution method}
Open addressing is a collision resolution method in hash tables. When a collision occurs, it finds another place within the hash table.

\paragraph{Insertion Process}
\begin{enumerate}
    \item Compute the hash index \( i = h(\text{key}) \).
    \item If \texttt{array[i]} is empty, place the \((\text{key}, \text{value})\) tuple there. Process done.
    \item Otherwise, if \texttt{array[i]} is occupied (collision), then let \( i = (i + 1) \mod |U| \), and repeat from step 2.
\end{enumerate}

\textbf{Example of Open Addressing}
\begin{lstlisting}
h(k) = k mod 16
Insert (0x9f2, V1) -> h(0x9f2) = 0x2
Insert (0x8cd, V2) -> h(0x8cd) = 0xd
Insert (0x532, V3) -> h(0x532) = 0x2

// Array after insertion
// Indices:   0 1 2 3 4 5 6 7 8 9 a b c d e f
// Values:    . . V1V3. . . . . . . . . V2. .
\end{lstlisting}

\paragraph{Handling Collisions}
\begin{itemize}
    \item If a collision is found at index \( i \), check the next index \( i+1 \).
    \item Continue checking in a circular manner until an empty slot is found.
    \item If the table is full, the insertion fails (or rehashing is needed).
\end{itemize}

\paragraph{Lookup After Collision}
\begin{itemize}
    \item To lookup a value for a key after collisions, start at \( h(\text{key}) \) and search sequentially until:
    \begin{itemize}
        \item The key is found, or
        \item An empty slot is encountered, indicating the key is not in the table.
    \end{itemize}
\end{itemize}


\paragraph{Lookup in Hash Tables with Open Addressing}
The lookup process for retrieving the value associated with a key in a hash table that uses open addressing is as follows:

\begin{enumerate}
    \item Compute the initial index \( i \) using the hash function: \( i = h(\text{key}) \).
    \item Check if the key at the computed index matches the key we are looking for:
    \begin{itemize}
        \item If \texttt{array[i]} matches the key, return \texttt{array[i]}.
        \item If \texttt{array[i]} is empty, return \textcolor{red}{\textbf{not found}}.
        \item If another key is found at \texttt{array[i]}, compute the next index \( i = (i + 1) \mod |U| \), and go back to the previous step.
    \end{itemize}
\end{enumerate}

\subsubsection{Probing}

Probing is a technique used in hash tables to resolve collisions. It involves finding the next available slot or bucket when a collision occurs. Here's how it works during insertion:

\begin{enumerate}
    \item \textbf{Compute initial index}: Let \( h_0 = h(k) \) be the hash of key \( k \), and initialize \( j = 0 \).
    \item \textbf{Check the slot}: If the slot \( array[i(h_0, j)] \) is empty, insert \( (k, v) \) there and terminate the insertion.
    \item \textbf{Collision resolution}: If the slot is occupied, increment \( j \) and find the next slot using a probing function \( i(h_0, j) \), then repeat step 2.
\end{enumerate}

The probing function \( i(h_0, j) \) can vary:
\begin{itemize}
    \item \textbf{Linear probing}: \( i(h_0, j) = (h_0 + j) \mod |U| \).
    \item \textbf{Quadratic probing}: \( i(h_0, j) = (h_0 + Kj + Lj^2) \mod |U| \) for some constants \( K \) and \( L \).
\end{itemize}


\subsubsection{Good hash functions}
\begin{itemize}
    \item Naive hash functions can lead to high collision rates, even with random keys.
    \item Effective hash functions take a non-uniform distribution of keys over the key space \( K \) and map it to a uniform-looking distribution over a set \( U \), minimizing collisions.
    \item Examples of good hash functions are Fowler–Noll–Vo (FNV), djb2, SipHash.
    \item These functions usually output 32-, 64-, or 128-bit numbers, which are then mapped to the table size using modulo operation: \( h(k) = h_0(k) \mod |U| \).
\end{itemize}

\subsubsection{Complexity of hash table operations}

Performance is influenced by many factors, including:
\begin{itemize}
    \item The density \( \frac{n}{|U|} \), key distribution, hash function choice, and probing method all affect performance.
    \item As table density approaches 1, it's often necessary to increase \( |U| \) and rebuild the hash table, known as "rehashing".
\end{itemize}

\subsubsection{In practice}
\begin{itemize}
    \item Keeping the collision rate low ensures that operations like insert, delete, and lookup can remain \( O(1) \) in average case.
    \item The initial access to a hash table often results in a cache miss, which impacts performance.
    \item With open addressing, collisions can affect the constant time performance due to the need for probing.
\end{itemize}

\subsection{Associative Arrays: performance}
\textbf{Choice of Data Structure:}
\begin{itemize}
    \item There is no universally superior data structure for all scenarios; the choice between self-balancing trees, tries, and hash tables is highly dependent on the specific data and application requirements.
    \item It is often beneficial to perform benchmarking to determine the most appropriate data structure.
\end{itemize}

\paragraph{Hash Tables}
\begin{itemize}
    \item Hash tables may perform better when:
    \begin{itemize}
        \item Hashing operations are low-cost.
        \item Collisions are infrequent.
        \item The scale of the dataset (\( n \)) is predictable.
    \end{itemize}
\end{itemize}

\paragraph{Self-balancing Trees}
\begin{itemize}
    \item Self-balancing trees provide:
    \begin{itemize}
        \item Robustness with better worst-case non-amortized complexity, particularly during operations like rehashing.
    \end{itemize}
\end{itemize}

\paragraph{Tries}
\begin{itemize}
    \item Tries can be more efficient when working with keys that have a structured pattern, such as:
    \begin{itemize}
        \item Virtual address translation in page tables.
        \item IP address routing for networks.
        \item Tokenizers in natural language processing (e.g., GPT-type models).
    \end{itemize}
\end{itemize}

\subsubsection{Combinations of Data Structures}
\begin{itemize}
    \item Combining data structures can yield the benefits of multiple approaches and is common practice. Examples include:
    \begin{itemize}
        \item Hash tables implemented as a static array of self-balancing trees.
        \item Depth-k tries with self-balancing trees at the leaf nodes.
    \end{itemize}
\end{itemize}



\subsection{Spatial data structures}
Spatial data structures are designed to manage multidimensional data, such as vectors in $\mathbb{R}^m$. They support various operations:

\begin{itemize}
    \item Insertion: Adding a vector $\mathbf{x} \in \mathbb{R}^m$.
    \item Deletion: Removing a vector.
    \item Finding the closest vector to a given vector $\mathbf{y} \in \mathbb{R}^m$.
    \item Locating nearest neighbors for each inserted vector.
    \item Identifying $k$ nearest neighbors for each inserted vector.
    \item Discovering all vectors within a certain distance $d$ from each inserted vector.
\end{itemize}

\subsubsection{The problem}
Given a set of vectors, the goal is to find all pairs of vectors that are within a distance $d$ from each other. A naive approach to this problem has a time complexity of $O(n^2)$, described by the following pseudocode:

\begin{verbatim}
R := {}
for i = 0 to n - 1:
    for j = i + 1 to n - 1:
        if ||x^i - x^j|| <= d:
            R := R union {(i, j)}
\end{verbatim}


\subsubsection{Grids}
A grid-based approach divides the space into a finite number of cells, which can significantly reduce the number of comparisons needed.

\subsubsection*{Pros and Cons of Using Grids}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item Quadratic complexity is limited to within individual grid cells.
    \end{itemize}
    \item \textbf{Cons:}
    \begin{itemize}
        \item Requires finite bounds $L \leq \mathbf{x}_i \leq U$ for all vectors.
        \item Fixed cell size, which can lead to:
        \begin{itemize}
            \item A variable number of vectors in each cell.
            \item Many cells being empty, especially in sparse regions.
        \end{itemize}
    \end{itemize}
\end{itemize}



\subsubsection{Quadtrees and octrees}
Quadtrees and octrees are tree data structures which partition a two-dimensional and three-dimensional space respectively into successively smaller regions.

\begin{itemize}
    \item They adaptively divide the space based on the distribution of the objects within.
    \item Each node in a quadtree corresponds to a square region of space and has four children, which correspond to the four quadrants of the square.
    \item Similarly, each node in an octree corresponds to a cubic region and has eight children.
\end{itemize}

\subsubsection{k-d trees}
k-d trees are a binary tree used to organize points in k-dimensional space.

\begin{itemize}
    \item Nodes which are not leaves represent hyperplanes that divide the space into two parts, known as half-spaces.
    \item Points to the left of this hyperplane are represented by the left subtree of that node and points to the right by the right subtree.
    \item The tree alternates between axes used to partition the space.
\end{itemize}


\subsubsection{Quadtrees, octrees, k-d trees: Pros and Cons}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item No need for finite bounds $L \leq \mathbf{x}_i \leq U$ for all points.
        \item Cell size adapts to the distribution of the data.
    \end{itemize}
    \item \textbf{Limitations:}
    \begin{itemize}
        \item Cells have a fixed shape, which can be inefficient for high-dimensional data.
        \item As the number of dimensions grows, the number of cells can grow exponentially, which is known as the curse of dimensionality.
    \end{itemize}
\end{itemize}

\subsubsection{Binary space partitioning}
Binary Space Partitioning (BSP) is a method for recursively subdividing a space into convex sets by hyperplanes. This method is commonly used in computer graphics, computational geometry, and for developing spatial data structures.

\begin{itemize}
    \item BSP is a generic process of partitioning a space into two parts at each step.
    \item It is typically implemented as a binary tree where each node represents a subregion of space partitioned by a hyperplane.
\end{itemize}

\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item Variable cell shape, which allows for efficient space division according to the distribution of objects.
    \end{itemize}
    \item \textbf{Cons:}
    \begin{itemize}
        \item Computation of separating hyperplanes can be costly.
    \end{itemize}
    \item \textbf{Limitations:}
    \begin{itemize}
        \item Not suitable for high-dimensional data due to the curse of dimensionality.
    \end{itemize}
\end{itemize}


\subsubsection{Locality-sensitive hashing}
Locality-Sensitive Hashing (LSH) is an algorithmic technique that hashes similar input items into the same “buckets” with high probability. The buckets are used to partition the space in a way that preserves locality.

\begin{itemize}
    \item The goal is to maximize the probability that similar items map to the same bucket.
    \item LSH is used for approximate nearest neighbor search in high-dimensional spaces.
\end{itemize}

\subsection*{Designing a Locality-Sensitive Hash Function}

\begin{itemize}
    \item The hash function \( h \colon \mathbb{R}^m \to \mathbb{R} \) should ensure that if two points \( \mathbf{x}, \mathbf{y} \) are close in the input space, their hash values \( h(\mathbf{x}), h(\mathbf{y}) \) should be close as well.
    \item It is often used in conjunction with other data structures to handle high-dimensional data efficiently.
\end{itemize}