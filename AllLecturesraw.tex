\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{geometry}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\setlength\parindent{0pt}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{2}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\geometry{a4paper, margin=1in}

\begin{document}

\title{Software Engineering Course Notes}
\author{Giosuè Castellano, Fabio Pernisi}
\date{January 2024}
\maketitle

\tableofcontents























\newpage
\section{Boolean Logic and Integers}

\subsection{Boolean Logic}

\subsubsection{Boolean values}
\begin{itemize}
\item False \( = 0 \)
\item True \( = 1 \)
\end{itemize}

\subsubsection{Boolean variables:}
\[ x \in \{0, 1\} \]


\subsubsection{Boolean expressions}

% The table for Boolean operators
\begin{tabular}{l l l l l}
\textbf{Boolean operators:}         &               &       &       & \\
\hline
operator    & math      & pseudocode & C code & logic gate \\
negation    & $\neg$    & not        & !      & [TBD]      \\
conjunction & $\land, \times$  & and         & \&\&, \&     & [TBD]      \\
disjunction & $\lor, +$ & or          & \texttt{||, |}      & [TBD]      \\
\end{tabular}
\vspace{4.5mm}

\textbf{Example expression:}

\begin{equation*}
(a \text{ and } b) \text{ or } (\text{not } c)
\end{equation*}

\noindent \textbf{Example function:}

\begin{equation*}
f(a, b, c) := (a \text{ and } b) \text{ or } c
\end{equation*}

\subsubsection{Truth Tables}

\begin{itemize}
    \item The truth table for the NOT operator is as follows:

    \begin{center}
    \begin{tabular}{c|c}
    \hline
    $x$ & $\lnot x$ \\
    \hline
    0 & 1 \\
    1 & 0 \\
    \hline
    \end{tabular}
    \end{center}

    \item The truth table for the AND operator is as follows:

    \begin{center}
    \begin{tabular}{cc|c}
    \hline
    $x$ & $y$ & $x \land y$ \\
    \hline
    0 & 0 & 0 \\
    0 & 1 & 0 \\
    1 & 0 & 0 \\
    1 & 1 & 1 \\
    \hline
    \end{tabular}
    \end{center}

    \item The truth table for the OR operator is as follows:

    \begin{center}
    \begin{tabular}{cc|c}
    \hline
    $x$ & $y$ & $x \lor y$ \\
    \hline
    0 & 0 & 0 \\
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 1 \\
    \hline
    \end{tabular}
    \end{center}
\end{itemize}


\paragraph{More operators!}
\begin{itemize}
    \item Exclusive OR (XOR), NAND, and NOR are additional binary Boolean operators with their respective truth tables.
    \item Unary Boolean operators include constants (always true, always false), identity, and NOT.
\end{itemize}

\begin{minipage}{0.3\textwidth}
\centering
Truth table for XOR:

\begin{tabular}{cc|c}
$x$ & $y$ & $x \text{ xor } y$ \\
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\end{tabular}
\end{minipage}
\hfill
% NAND Operator
\begin{minipage}{0.3\textwidth}
\centering
Truth table for NAND:

\begin{tabular}{cc|c}
$x$ & $y$ & $x \text{ nand } y$ \\
\hline
0 & 0 & 1 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\end{tabular}
\end{minipage}
\hfill
% NOR Operator
\begin{minipage}{0.3\textwidth}
\centering
Truth table for NOR:

\begin{tabular}{cc|c}
$x$ & $y$ & $x \text{ nor } y$ \\
\hline
0 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
\end{tabular}
\end{minipage}

\vspace{1em}

\textbf{Example of expressions}
\begin{itemize}
    \item $(a \land b) \lor (\lnot c)$
    \item $f(a, b, c) := (a \land b) \lor c$
    \item $w := \lnot a$
    \item $z := a \land (\lnot b)$
    \item $z := (\lnot a) \lor (b \land c)$
\end{itemize}

\paragraph{Questions}
\textbf{Q: How many distinct unary Boolean operators?}\\
\textbf{A:} one? (NOT)\\

Actually, we have 4 deterministic unary operators in total (counting 3 trivial unary operators):\\

\begin{minipage}[t]{0.15\textwidth}
\centering
always false

\begin{tabular}{c|c}
$x$ & $\emptyset$ \\ % The empty set symbol is used here to denote "always false".
\hline
0 & 0 \\
1 & 0 \\
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.25\textwidth}
\centering
always true

\begin{tabular}{c|c}
$x$ & $1$ \\
\hline
0 & 1 \\
1 & 1 \\
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.25\textwidth}
\centering
identity

\begin{tabular}{c|c}
$x$ & $x$ \\
\hline
0 & 0 \\
1 & 1 \\
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.25\textwidth}
\centering
NOT

\begin{tabular}{c|c}
$x$ & $\text{not } x$ \\
\hline
0 & 1 \\
1 & 0 \\
\end{tabular}
\end{minipage}


\vspace{1em}

\subsubsection{Binary Operators}
The number of distinct binary operators corresponds to the number of different truth tables that can be constructed with two Boolean inputs. There are $2^{2^2} = 16$ possible truth tables for binary operators.

\begin{tabular}{c c c}
x & y & op(x, y) \\
\hline
0 & 0 & ? \\
0 & 1 & ? \\
1 & 0 & ? \\
1 & 1 & ? \\
\end{tabular}

\paragraph{Representing binary operators}
Common binary Boolean operators include AND, OR, and NOT. These are sufficient to represent all possible binary operations.\\
NAND and NOR are known as \textit{universal gates} because they can be used to represent any Boolean operation.\\

\textbf{Examples}\\
$x \text{ nand } y = \text{not } (x \text{ and } y)$ \\
$x \text{ xor } y = (x \text{ or } y) \text{ and } (\text{not } (x \text{ and } y))$ 

\paragraph{Proof via truth table}
To prove that two expressions are equivalent, we can show that their truth tables are identical.

\begin{center}
\begin{tabular}{cc|c|c}
$x$ & $y$ & $x \text{ XOR } y$ & $(x \lor y) \land \lnot(x \land y)$ \\
\hline
0 & 0 & 0 & 0 \\
0 & 1 & 1 & 1 \\
1 & 0 & 0 & 0 \\
1 & 1 & 1 & 1 \\
\end{tabular}
\end{center}

The expressions are equivalent because their truth tables match.


\subsubsection{Boolean identities I}

\begin{itemize}
    \item \( x \text{ and } 0 = 0 \)
    \item \( x \text{ or } 1 = 1 \)
    \item \( x \text{ and } 1 = x \)
    \item \( x \text{ or } 0 = x \)
    \item \( x \text{ or } x = x \)
    \item \( x \text{ and } x = x \)
\end{itemize}

\subsubsection{Boolean identities II}

\begin{itemize}
    \item AND is commutative:
    \[ x \text{ and } y = y \text{ and } x \]

    \item AND is associative:
    \[ x \text{ and } (y \text{ and } z) = (x \text{ and } y) \text{ and } z \]

    \item OR is commutative:
    \[ x \text{ or } y = y \text{ or } x \]

    \item OR is associative:
    \[ x \text{ or } (y \text{ or } z) = (x \text{ or } y) \text{ or } z \]
\end{itemize}

\subsubsection{Boolean identities III}

\begin{itemize}
    \item Distributivity (AND over OR):
    \[ x \text{ and } (y \text{ or } z) = (x \text{ and } y) \text{ or } (x \text{ and } z) \]
    
    \item Distributivity (OR over AND):
    \[ x \text{ or } (y \text{ and } z) = (x \text{ or } y) \text{ and } (x \text{ or } z) \]
    
    \item De Morgan's law (1):
    \[ (\text{not } x) \text{ and } (\text{not } y) = \text{not } (x \text{ or } y) \]

    \item De Morgan's law (2):
    \[ (\text{not } x) \text{ or } (\text{not } y) = \text{not } (x \text{ and } y) \]
\end{itemize}


% Note for user: This transcription assumes a LaTeX document with a suitable class already in place that supports sectioning and itemize environments.
% The "\text{}" command is used here to maintain the "and", "or", and "not" as text within the mathematical expressions for clarity, as per the image content.


\subsubsection{Satisfiability problem}
The satisfiability problem, often referred to as SAT, is a problem of determining if there exists an interpretation that satisfies a given Boolean formula. In other words, it is about finding an assignment to variables that makes the entire expression true.


\paragraph{Example Problem}
Given a Boolean expression, our goal is to find a value for each variable such that the expression evaluates to true.

For the expression \(x1 \land ((\neg x2 \lor x3) \land (\neg x3))\), the truth table is:

\begin{center}
\begin{tabular}{ccc|c}
$x1$ & $x2$ & $x3$ & $x1 \land ((\neg x2 \lor x3) \land (\neg x3))$ \\
\hline
0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 0 & 1 & 0 \\
1 & 1 & 0 & 0 \\
1 & 1 & 1 & 0 \\
\end{tabular}
\end{center}

A solution to the SAT problem for this expression is \(x1 = 1\), \(x2 = 0\), \(x3 = 0\).

\subsubsection{Definitions}
\begin{itemize}
\item \textbf{Variable:} \(x_j\) for some \(j \in J \subseteq \mathbb{N}\).
\item \textbf{Literal:} Either \(x_j\) or \(\neg x_j\) for some \(j \in J\).
\item \textbf{Disjunctive clause:} A disjunction of literals, e.g., \(\bigvee_{j \in J^0} \neg x_j \lor \bigvee_{j \in J^1} x_j\) for some \(J^0, J^1 \subseteq J\).
\item \textbf{Conjunctive clause:} A conjunction of literals, e.g., \(\bigwedge_{j \in J^0} \neg x_j \land \bigwedge_{j \in J^1} x_j\) for some \(J^0, J^1 \subseteq J\).
\end{itemize}

\subsubsection{Conjunctive normal form}

Conjunctive Normal Form is a way of structuring logical expressions as a conjunction (AND) of disjunctive clauses (ORs of literals).

\[
\bigwedge_{i \in I} \left( \bigvee_{j \in J_i^0} \neg x_j \vee \bigvee_{j \in J_i^1} x_j \right)
\]

where \( J_i^0, J_i^1 \subseteq J \) and \( I \subseteq \mathbb{N} \).

\paragraph{Examples of CNF}
\begin{itemize}
\item \((x1 \lor x2) \land (x3 \lor x4) \land (x5 \lor x6)\)
\item \((\neg x2 \lor \neg x4 \lor \neg x6) \land (x1 \lor x5 \lor x6 \lor x7 \lor x9) \land (\neg x1 \lor \neg x2 \lor \neg x3)\)
\end{itemize}

\subsubsection{Disjunctive normal form}

The disjunctive normal form (DNF) is a disjunction of conjunctive clauses:
\[
\bigvee_{i \in I} \left( \bigwedge_{j \in J_i^0} \neg x_j \land \bigwedge_{j \in J_i^1} x_j \right),
\]
where $J_i^0, J_i^1 \subseteq J \subseteq \mathbb{N}, \forall i \in I \subseteq \mathbb{N}$

\paragraph{Examples:}
\begin{itemize}
    \item $(x1 \land x2) \lor (x3 \land x4) \lor (x5 \land x6)$
    \item $(x1 \land (\neg x2)) \lor (x3 \land (\neg x4))$
\end{itemize}

\subsubsection{Theorems}

\begin{itemize}
    \item Every Boolean expression can be put into CNF
    \begin{itemize}
        \item For every Boolean expression with $n$ variables and $k$ literals using operators \{NOT, AND, OR\}, there exists an equivalent CNF with $n + k$ variables $3k$ clauses and $7k$ literals at most.
        \item Satisfiability for a CNF (``SAT'') is hard.
    \end{itemize}
    \item Every Boolean expression can be put in DNF
    \begin{itemize}
        \item For every Boolean expression with $n$ variables and $k$ literals using operators \{NOT, AND, OR\}, there exists an equivalent DNF with $n$ variables and $n \times 2^n$ literals at most.
        \item Satisfiability for a DNF is trivial.
    \end{itemize}
\end{itemize}

\paragraph{Example:}
$(x2 \land (\neg x4) \land (\neg x6)) 
\lor 
(\neg x1 \land x5 \land x6 \land x7 \land x9)
\lor 
((\neg x1) \land (\neg x2) \land (\neg x3))
\lor 
(x4 \land x5 \land x6)$

\begin{enumerate}
    \item Take any clause, e.g. $(x2 \land (\neg x4) \land (\neg x6))$.
    \item Set $x2 = 1, x4 = 0, x6 = 0$.
    \item Done.
\end{enumerate}




% The following LaTeX code is meant to transcribe the text content of the provided image.
% Since LaTeX doesn't handle images within code, [TBD] is used to represent them.

\subsection{Integer Arithmetic}

\begin{itemize}
    \item Computers are made out of Boolean gates
    \item But we want to represent numbers other than 0 and 1
    \item How do we proceed?
\end{itemize}

\begin{itemize}
    \item Consider Booleans as binary digits (\textit{bits})
    \item Group them together to form numbers in base 2
\end{itemize}

\subsubsection{Base-10 numbers}

In base 10 (decimal), we have 10 digits: \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}

Using one digit, we can count to 9:

\begin{center}
    0 \quad 1 \quad 2 \quad 3 \quad 4 \quad 5 \quad 6 \quad 7 \quad 8 \quad 9
\end{center}

Then we need more digits:

\begin{center}
    10 \quad 11 \quad 12 \quad 13 \quad 14 \quad 15 \quad 16 \quad 17 \quad 18 \quad 19
    20 \quad 21 \quad 22 \quad 23 \quad \ldots
\end{center}

\subsubsection{Base-2 numbers}

In base 2 (binary), we have 2 digits: \{0, 1\}\\
Using one digit, we can count to 1: \\
0 \quad 1\\

Then we need more digits: \\
10 \quad 11 \quad 100 \quad 101 \quad 110 \quad 111 \quad 1000 \quad 1001 \ldots\\
If we wanted to count from 0 to 15, we may decide to use 4 digits:\\
0000 \quad 0001 \quad 0010 \quad 0011 \quad 0100 \quad 0101 \quad 0110 \quad 0111\\
1000 \quad 1001 \quad 1010 \quad 1011 \quad 1100 \quad 1101 \quad 1110 \quad 1111\\

\paragraph{Example}

1001$_2$ = ?\\
$=$ $1 \times 8$ $+$ 
$0 \times 4$ $+$ 
$0 \times 2$ $+$ 
 $1$ \\
$=$ $1 \times 2^3$  $+$ 
$0 \times 2^2$ $+$ 
$0 \times 2^1$ $+$ 
 $1 \times 2^0$ 

= 9\\

Note:
\begin{itemize}
\item rightmost / least-significant bit is called bit 0
\item leftmost / most-significant bit is called bit $n - 1$
\end{itemize}


\subsubsection{Fixed bit width}
In computer systems, integers have a fixed number of bits, determining their size and the range of values they can represent.

\paragraph{Commonly Used Integer Types and Their Bit Widths}
\begin{tabular}{lll}
\textbf{bits} & \textbf{a.k.a.} & \textbf{C type} \\
\hline
8  & byte & \texttt{uint8\_t} \\
   &      & \texttt{unsigned char} \\
32 &      & \texttt{uint32\_t} \\
   &      & \texttt{unsigned int} (Windows, Linux, BSD, macOS) \\
64 &      & \texttt{uint64\_t} \\
   &      & \texttt{unsigned long} (Linux, BSD, macOS) \\
   &      & \texttt{unsigned long long} (Windows) \\
\end{tabular}


\subsubsection{Integers in hardware and in programming languages}
\begin{itemize}
    \item Most computers support 8, 16, 32, and 64-bit arithmetic natively.
    \item Arithmetic on integers larger than native size can be done in software, which is slower.
    \item In C, integer types are fixed-size, and arbitrary-sized integers require specific libraries.
    \item Python supports integers of arbitrary size, though with a performance penalty for very large numbers.
\end{itemize}

\begin{tabular}{lr}
\textbf{bits} & \textbf{largest integer = $2^{\text{bits}} - 1$} \\
\hline
8  & 255 \\
16 & 65,535 \\
32 & 4,294,967,295 \\
64 & 18,446,744,073,709,551,615 \\
128 & 340,282,366,920,938,463,463,374,607,431,768,211,455 \\
\end{tabular}



\[
\text{1 decimal digit} = \log_{2}{10} \text{ bits} \approx 3.3219 \text{ bits}
\]


\subsubsection{Operations with integers}
Integer operations in binary are analogous to the decimal operations learned in school.

\paragraph{Binary Addition Example}
\begin{lstlisting}
  0 1 0 1 0 0 1 1
+ 0 1 1 0 0 0 0 1
-----------------
  1 0 1 1 0 1 0 0
\end{lstlisting}

\paragraph{Operation complexity}
\begin{itemize}
    \item Addition and subtraction are straightforward, like their decimal counterparts.
    \item Multiplication is more complex, involving shifting and adding.
    \item Division is the most complex, requiring iterative subtraction and shifting.
\end{itemize}

\subsection{Signed integers}
Representing negative numbers in binary requires special encoding:
\subsubsection{Sign-Magnitude Representation}
\begin{itemize}
    \item One bit is reserved for the sign (most significant bit).
    \item Drawback: Zero has two representations: $+0$ and $-0$.
    \item Drawback: Arithmetic with sign-magnitude is not
\end{itemize}

\subsubsection{One's Complement Representation}
\begin{itemize}
    \item The sign bit is also the most significant bit.
    \item Negative numbers are represented by flipping all bits of the positive value.
    \item Drawback: Zero still has two representations: $+0$ and $-0$.
    \item Drawback: Arithmetic is simpler than sign-magnitude but 
\end{itemize}


\subsubsection{Signed integers: two's complement}
It is the approach adopted by most current computers.
\begin{itemize}
    \item In two's complement representation, a negative number \( x \) is represented by \( 2^n - x \) for an \( n \)-bit number.
    \item The most significant bit (MSB) is the sign bit, where \( 1 \) indicates a negative number.
    \item Drawback: Flipping the sign requires inverting all bits and adding \( 1 \).
    \item Drawback: This representation gives zero a single unique representation.
    \item Drawback: Arithmetic operations for addition and subtraction are the same as for unsigned integers.
\end{itemize}





% Start of the LaTeX content
\subsubsection{4-bit signed integers (two's complement)}

\begin{tabular}{cccccc}
b3 & b2 & b1 & b0 & unsigned & signed \\
\hline
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 1 \\
0 & 0 & 1 & 0 & 2 & 2 \\
0 & 0 & 1 & 1 & 3 & 3 \\
0 & 1 & 0 & 0 & 4 & 4 \\
0 & 1 & 0 & 1 & 5 & 5 \\
0 & 1 & 1 & 0 & 6 & 6 \\
0 & 1 & 1 & 1 & 7 & 7 \\
1 & 0 & 0 & 0 & 8 & -8 \\
1 & 0 & 0 & 1 & 9 & -7 \\
1 & 0 & 1 & 0 & 10 & -6 \\
1 & 0 & 1 & 1 & 11 & -5 \\
1 & 1 & 0 & 0 & 12 & -4 \\
1 & 1 & 0 & 1 & 13 & -3 \\
1 & 1 & 1 & 0 & 14 & -2 \\
1 & 1 & 1 & 1 & 15 & -1 \\
\end{tabular}

\subsubsection{Example:}

\begin{tabular}{lcl}
signedness & decimal & binary \\
\hline
unsigned & $2 + 11 = 13$ & $0010_b + 1011_b = 1101_b$ \\
signed & $2 + (-5) = -3$ & $0010_b + 1011_b = 1101_b$ 
\end{tabular}

\subsubsection{Range of values for different bit sizes:}

\begin{tabular}{cccc}
bits & $-2^{bits-1}$ (min) & $2^{bits-1} - 1$ (max) \\
\hline
8  & -128 & 127 \\
16 & -32768 & 32767 \\
32 & -2,147,483,648 & 2,147,483,647 \\
64 & $\approx -9.10^{18}$ & $\approx 9.10^{18}$ \\
128 & $\approx -2.10^{38}$ & $\approx 2.10^{38}$ \\
\end{tabular}














\newpage
\section{Instructions and Memory}

\subsection{Integers (Continued)}

\subsubsection{Two's complement}
Two's complement uses the binary digit with the greatest place value (i.e. what we here call $bit(n-1)$) as the sign to indicate whether the binary number is positive or negative. When the most significant bit is 1, the number is signed as negative; and when the most significant bit is 0 the number is signed as positive.\\
The remaining \(n-1\) bits indicate the magnitude of the number.
\begin{itemize}
    \item Given a single \textit{n}-bit pattern,
    \begin{itemize}
        \item let \textit{u} be its unsigned value
        \item let \textit{s} be its signed value,
    \end{itemize}
    \item If bit $(n - 1) = 0$, then the number is non-negative, and \( s = u \).
    \item If bit $(n - 1) = 1$, then the number is negative, and \( s \) is found by subtracting \( 2^n \) from \( u \), i.e., \( s = u - 2^n \).
\end{itemize}

\paragraph{In general:}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[gray]{0.9} Bit Pattern & Unsigned \( u \) & Signed \( s \) \\
\hline
0000 & 0 & 0 \\
\hline
0001 & 1 & 1 \\
\hline
... & ... & ... \\
\hline
0111 & 7 & 7 \\
\hline
1000 & 8 & -8 \\
\hline
1001 & 9 & -7 \\
\hline
... & ... & ... \\
\hline
1111 & 15 & -1 \\
\hline
\end{tabular}
\end{center}
In this example, the signed interpretation of the bit pattern changes when \(bit(n-1)\) is 1, indicating a negative number.\\

For an arbitrary \( n \)-bit number:
\begin{itemize}
    \item The range of unsigned integers is \( 0 \) to \( (2^n - 1) \).
    \item The range of signed integers in two's complement is \( -(2^{n-1}) \) to \( (2^{n-1} - 1) \).
\end{itemize}
This allows for a symmetric range around zero, facilitating arithmetic operations like addition and subtraction.


\paragraph{Conversion to Two's Complement}
To convert a positive number to negative in two's complement:
\begin{enumerate}
    \item Invert all bits of the number (one's complement).
    \item Add one to the resulting number.
\end{enumerate}
This process changes the sign of the number while maintaining the magnitude in a form that is conducive to binary arithmetic.


\paragraph{Conversely:}

\begin{itemize}
    \item if $s \geq 0$
    \begin{itemize}
        \item represent with bit pattern of $u = s$.
    \end{itemize}
    \item if $s < 0$
    \begin{itemize}
        \item represent with bit pattern of $u = 2^n - |s|$.
    \end{itemize}
    \item if $s \notin \{-(2^{n-1}), \ldots, (2^{n-1}) - 1\}$
    \begin{itemize}
        \item cannot represent, need larger $n$
    \end{itemize}
\end{itemize}

\subsubsection{Sign extension}

Let us represent \( s = -5 \) in n-bit signed binary (two's complement):

\( u = 2^n - \left| s \right| = 2^n - 5 \)

\begin{center}
\begin{tabular}{cccc}
n & s & u & bit pattern \\
\hline
4 & -5 & 11 & 1011 \\
5 & -5 & 27 & 11011 \\
6 & -5 & 59 & 111011 \\
7 & -5 & 123 & 1111011 \\
8 & -5 & 251 & 11111011 \\
9 & -5 & 507 & 111111011 \\
10 & -5 & 1019 & 1111111011 \\
11 & -5 & 2043 & 11111111011 \\
12 & -5 & 4091 & 111111111011 \\
\end{tabular}
\end{center}

\subsubsection{Increasing the number of bits}

To convert an n-bit number to an \( (n + k) \)-bit number (\( k \geq 0 \)):

\begin{itemize}
\item Unsigned:
\begin{itemize}
\item Additional high-order (leftmost) bits are set to zero
\end{itemize}
\item Signed (``sign extension''):
\begin{itemize}
\item Additional high-order (leftmost) bits are set to the value of bit \( (n - 1) \)
\end{itemize}
\end{itemize}
\subsubsection{Base 16}

Hexadecimal digits: \( 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f \)

\begin{itemize}
    \item Pros:
    \begin{itemize}
        \item Directly maps to binary numbers:
        \begin{itemize}
            \item hex 12f3 = binary 0001 0010 1111 0011
        \end{itemize}
        \item More compact than binary
        \item Directly maps to bytes:
        \begin{itemize}
            \item two hex digits = one byte
        \end{itemize}
    \end{itemize}
    \item Cons:
    \begin{itemize}
        \item Not human-friendly (esp. for arithmetic)
    \end{itemize}
\end{itemize}
\subsection{Characters and Text}

How do we map bit patterns to characters in order to form text?

\begin{itemize}
    \item Many standards
    \item Some similarities
    \item Some incompatibilities
\end{itemize}

\subsubsection{ASCII (1963-)}
\begin{itemize}
    \item American Standard Code for Information Interchange
    \item Each character stored stored in 1 byte (8 bits, 256 possible characters)
    \item 128 standardized characters
    \item Many derivatives specify the remaining 128
\end{itemize}
[TBD] % This is a placeholder for the ASCII table image

\subsubsection{Unicode (1988-)}
\begin{itemize}
    \item Associates ``code points'' (roughly, characters) to integers
    \item Up to 1,112,064 code points (currently 149,186 assigned)
    \item First 128 code points coincide with ASCII
    \item Multiple possible encodings into bytes (``transmission formats''):
    \begin{itemize}
        \item UTF-8
        \begin{itemize}
            \item First 128 code points encoded into a single byte (backward compatible with ASCII)
            \item Sets most significant bit (bit 7) to 1 to signify ``more bytes needed''
            \item Up to 4 bytes per code point
            \item Default on BSD, iOS/MacOS, Android/Linux and on most internet communications
        \end{itemize}
        \item UTF-16
        \begin{itemize}
            \item Code points are encoded by either two or four bytes
            \item Default on Windows, for Java code, and for SMS
        \end{itemize}
        \item Aims at encoding all languages:
        \begin{itemize}
            \item including extinct ones
            \item left-to-right, right-to-left or vertical
            \item and more (emojis)
        \end{itemize}
        \item Some ``characters'' require multiple code points (flag emojis, skin tone modifiers)
        \item What is even a ``character''? (code point, glyph, grapheme, cluster)
        \item Unicode is extremely complicated
        \item Latest version (v15.0.0, 2022) specification is 1,060 pages
    \end{itemize}
\end{itemize}
\subsection{Hardware}

Logic gates allow us to compute Boolean functions ``instantly'' (subject to physical limits).

But we need many logic gates, even for simple things (like 64-bit integer division).

$\rightarrow$ We break down complex algorithms into simple steps.

\subsubsection{Components in a Computer}
\begin{itemize}
    \item Logic gates
    \item A clock
    \item Memory
    \item Input and output devices
\end{itemize}
\subsubsection{A first abstraction}

\begin{itemize}
  \item Memory is $N$ bits $\in \{0, 1\}^N$ (e.g., for 16 GB, $N \approx 128 \cdot 10^9$)
  \item At every clock cycle (e.g., 1.2 GHz), we update the memory:
  \[
  x_i' \leftarrow f_i(\mathbf{x}) \quad \forall i = 0, \ldots, N
  \]
  \item Some of the memory comes from input devices
  \item Some of the memory is sent to output devices
\end{itemize}

\subsubsection{A more realistic model}

\begin{itemize}
  \item We cannot update the whole memory at every clock cycle
  \begin{itemize}
    \item That would be $128 \times 10^9 \times 1.2 \times 10^9 = 153.6 \times 10^{18}$ B/s
    \item $\approx 153,600,000,000$ GB/s
  \end{itemize}
  \item As of 2023, memory maxes out at $\approx 800$ GB/s
  \item Instead, at each cycle, we only read/write a tiny amount of memory
  \item We cannot have too many different Boolean function $f_i$
  \begin{itemize}
    \item Instead, at each cycle, the computer executes one of a limited set of \textbf{instructions} in a \textbf{processor} (a.k.a. \textsl{"Central Processing Unit", CPU}), e.g.
    \begin{itemize}
      \item memory read / write
      \item 64-bit arithmetic (+, -, $\times$, \ldots)
      \item comparison ($<$, $>$, $=$, \ldots)
      \item branch (if, while, \ldots)
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Instruction Set Architectures (ISA)}
\subsubsection{An ISA specifies:}
\begin{itemize}
    \item How the machine is organized (memory, etc.)
    \item What instructions are available
    \item How instructions are encoded into bits
\end{itemize}
More precisely,
\begin{itemize}
    \item ISA defines instructions, data types, registers, and hardware support for memory management.
    \item Fundamental features include memory consistency, addressing modes, virtual memory, and the I/O model.
\end{itemize}

\textbf{Implementation and Binary Compatibility:}
\begin{itemize}
    \item Ensures behavior of machine code is consistent across different implementations of the ISA.
    \item Provides binary compatibility, allowing hardware variation without software replacement.
    \item Supports evolution of microarchitectures while maintaining software compatibility with older generations.
\end{itemize}

\textbf{Application Binary Interface (ABI) and Operating Systems:}
\begin{itemize}
    \item A standard ABI allows software to run on future implementations of the ISA.
    \item ISA's multi-OS support doesn't ensure cross-OS compatibility for machine code without OS-level support.
\end{itemize}

\textbf{Extensions and Compatibility:}
\begin{itemize}
    \item ISAs can be extended for additional instructions, capabilities, and larger addresses/data values.
    \item Extended ISA implementations remain backward compatible with non-extended ISA machine code.
    \item Machine code using extensions only runs on supporting implementations.
\end{itemize}

\textbf{ISA vs. Microarchitecture:}
\begin{itemize}
    \item ISA is distinct from microarchitecture, which is how the instruction set is implemented in a processor.
    \item Different microarchitectures can share the same instruction set, e.g., Intel Pentium and AMD Athlon with x86.
\end{itemize}

\textbf{Virtual Machines and Bytecode:}
\begin{itemize}
    \item Some virtual machines use bytecode as their ISA and execute it via JIT compilation or interpretation.
    \item Transmeta used this approach to implement x86 instruction set on VLIW processors.
\end{itemize}

\textbf{Significance:}
\begin{itemize}
    \item Binary compatibility of ISAs is a fundamental abstraction in computing, enabling hardware and software ecosystem longevity.
\end{itemize}

\subsubsection{Two major ISAs in practice:}
\begin{itemize}
    \item \textbf{x86\_64} (a.k.a. x64, x86\_64, AMD64): Intel\textsuperscript{\textregistered} and AMD\textsuperscript{\textregistered} 64-bit CPUs
    \item \textbf{AArch64} (a.k.a. ARM64): ARM\textsuperscript{\textregistered}-based 64-bits CPUs (phones, Apple M1 \& M2)
\end{itemize}

Many older and less prominent ISAs:
\begin{itemize}
    \item x86, Itanium, ARMv7, RISC-V, PowerPC, \ldots
\end{itemize}

\paragraph{Code Example and Assembly Representation}
\texttt{ int f(int a, int b, int c)}
\begin{lstlisting}
{
    return (a * b) / c;
}
\end{lstlisting}


\subsubsection{Assembly}
\begin{itemize}
    \item Assembly language has a strong correspondence with the machine code instructions of the architecture.
    \item Generally, it follows a one-to-one relationship between statements and machine instructions.
\end{itemize}

\textbf{Architecture Specificity:}
\begin{itemize}
    \item Each assembly language is specific to its computer architecture.
    \item Because each processor type's instruction set is unique, assembly languages are necessarily different among processor types.
\end{itemize}

\textbf{Assemblers and Operating Systems:}
\begin{itemize}
    \item Assembly code is converted into executable machine code by a utility program referred to as an assembler.
    \item Multiple assemblers may exist for a single architecture.
    \item An assembler might be designed for use with specific operating systems.
    \item Assembly languages usually do not have specific syntax for operating system calls.
    \item They provide access to the real capabilities of the processor, upon which all system call mechanisms ultimately rest
\end{itemize}


\subsubsection{Instructions (x86\_64)}

\begin{lstlisting}
f:
 mov eax, edi       # 89 f8
 mov ecx, edx       # 89 d1
 imul eax, esi      # 0f af c6
 cdq                # 99
 idiv ecx           # f7 f9
 ret                # c3
\end{lstlisting}

\begin{itemize}
  \item \textbf{mov a, b} \qquad move \hfill $a \gets b$
  \item \textbf{imul a, b} \qquad signed integer multiply \hfill $a \gets a \times b$
  \item \textbf{idiv a} \qquad signed integer divide \hfill $eax \gets eax / a$
  \item \textbf{cdq} \qquad convert double-word (32 bits) to quad-word (64 bits) \hfill sign-extend $eax$ into $edx:eax$
  \item \textbf{ret} \qquad return \hfill return to calling function
\end{itemize}

\subsubsection{Instructions (AArch64)}

\begin{lstlisting}
f:
 mul w0, w0, w1     # 1b 01 7c 00
 sdiv w0, w0, w2    # 1a c2 0c 00
 ret                # d6 5f 03 c0
\end{lstlisting}

\begin{itemize}
  \item \textbf{mul a, b, c} \qquad multiply \hfill $a \gets b \times c$
  \item \textbf{sdiv a, b, c} \qquad signed integer divide \hfill $a \gets b / c$
  \item \textbf{ret} \qquad return \hfill return to calling function
\end{itemize}

\subsubsection{Registers}
Registers are mainly used for arithmetic, addressing, and control processes within the CPU.

\begin{itemize}
    \item small, fixed set of variables that can be accessed instantly
    \item 16 (x86\_64) or 31 (AArch64) general-purpose 64-bit registers
    \item special registers and flags (not accessible directly)
    \item larger registers for extended operations (e.g., non-integer numbers)
\end{itemize}
More in detail
\begin{itemize}
    \item Registers are highly accessible locations for a processor, consisting of a small amount of fast storage.
    \item Some registers have specific hardware functions and may have read-only or write-only access rights.
    \item Typically, registers are addressed uniquely, not via main memory, though exceptions exist in some architectures (e.g., DEC PDP-10, ICT 1900).
    \item Registers are essential for arithmetic, bitwise operations, and other machine instruction manipulations, with results often stored back in main memory.
    \item Modern processors use static or dynamic RAM for main memory, with dynamic RAM accessed via cache levels.
    \item The hierarchy places registers at the top, offering the quickest data access method.
    \item The term "register" usually refers to those directly encoded in an instruction, as per the instruction set.
    \item To enhance performance, CPUs may duplicate these architectural registers, facilitating parallel and speculative execution.
    \item Allocation is managed by a compiler during code generation or by an assembly language programmer.
\end{itemize}

\subsubsection{Registers (x86\_64)}

\begin{itemize}
    \item sixteen 64-bit registers:
    \begin{lstlisting}
    rax, rbx, rcx, rdx, rsp, rbp, rsi, rdi,
    r8, r9, r10, r11, r12, r13, r14, r15
    \end{lstlisting}
    
    \item we can access the lower 32 bits separately:
    \begin{lstlisting}
    eax, ebx, ecx, edx, esp, ebp, esi, edi,
    r8d, r9d, r10d, r11d, r12d, r13d, r14d, r15d
    \end{lstlisting}
    
    \item we can access the lower 16 bits separately:
    \begin{lstlisting}
    ax, bx, cx, dx, bp, sp, si, di,
    r8w, r9w, r10w, r11w, r12w, r13w, r14w, r15w
    \end{lstlisting}
    
    \item we can access the lower 8 bits separately:
    \begin{lstlisting}
    al, bl, cl, dl, bpl, spl, sil, dil,
    r8b, r9b, r10b, r11b, r12b, r13b, r14b, r15b
    \end{lstlisting}
    
    \item we can access bits 8-15 separately for some registers:
    \begin{lstlisting}
    ah, bh, ch, dh
    \end{lstlisting}
\end{itemize}

\paragraph{Example:}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{bits} & \textbf{63..56} & \textbf{55..48} & \textbf{47..40} & \textbf{39..32} & \textbf{31..24} & \textbf{23..16} & \textbf{15..8} & \textbf{7..0} \\
\hline
64 & \multicolumn{8}{c|}{rax} \\
\hline
32 & \multicolumn{4}{c|}{} & \multicolumn{4}{c|}{eax} \\
\hline
16 &  &  &  &  &  & & \multicolumn{2}{c|}{ax} \\
\hline
8 &  &  &  &  &  &  & ah & al \\
\hline
\end{tabular}
\subsubsection{Registers (AArch64)}

\begin{itemize}
    \item thirty-one 64-bit registers:
    \begin{lstlisting}
    x0, ..., x30
    \end{lstlisting}
    
    \item we can access the lower 32 bits separately:
    \begin{lstlisting}
    w0, ..., w30
    \end{lstlisting}
    
    \item register 31 (\texttt{x31}, \texttt{w31}) is read-only (zero in most cases)
\end{itemize}

\paragraph{Example:}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{bits} & 63...56 & 55...48 & 47...40 & 39...32 & 31...24 & 23...16 & 15...8 & 7...0 \\
\hline
64 &  \multicolumn{8}{|c|}{\textbf{x0}} \\
\hline
32 &  &  &  &  & \multicolumn{4}{|c|}{\textbf{w0}} \\
\hline
\end{tabular}
\subsection{Memory}
\begin{lstlisting}
int g(int *a, int *b)
{
    return *a + *b;
}
\end{lstlisting}

\paragraph{x86\_64:}
\begin{lstlisting}
g:
  mov eax, DWORD PTR [rsi]
  add eax, DWORD PTR [rdi]
  ret
\end{lstlisting}

\paragraph{AArch64:}
\begin{lstlisting}
g:
  ldr w0, [x0]
  ldr w1, [x1]
  add w0, w2, w0
  ret
\end{lstlisting}

\subsubsection{Memory}

\begin{itemize}
    \item From a process' perspective, memory is seen as a single long array of \textbf{bytes} (8 bits)
    \item Like registers, memory can be accessed in larger chunks (16, 32 or 64 bits)
    \item But the smallest addressable unit is the byte
\end{itemize}

\subsubsection{Byte ordering}
Consider the following simplified model of memory\\
\begin{tabular}{c c c c c c c c c c c c c}
    \hline
    address & 0 & 1 & 2 & 3 & ... & 239 & 240 & 241 & 242 & 243 & 244 & ... \\
    \hline
    value (hex) & ef & cd & ab & 89 & ... & ff & a0 & a1 & a2 & a3 & 42 & ... \\
    \hline
\end{tabular}

\begin{itemize}
    \item the byte at address 240 is (hex) \textbf{a0} = (decimal) 160
    \item the byte at address 241 is (hex) \textbf{a1} = (decimal) 161
    \item the byte at address 242 is (hex) \textbf{a2} = (decimal) 162
    \item the byte at address 243 is (hex) \textbf{a3} = (decimal) 163
\end{itemize}

\textbf{Q}: What is the value of the 32-bit integer at address 240?

\textbf{A}: It depends!

\subsubsection{Byte ordering / "Endianness"}
Endianness refers to the sequence of byte order within a word in memory or during data transmission.\\

\textbf{Big-endian and Little-endian Systems:}
\begin{itemize}
    \item Big-endian systems store the most significant byte at the smallest memory address.
    \item Little-endian systems store the least significant byte at the smallest memory address.
    \item Bi-endianness allows for switchable endianness and is supported by some architectures.
    \item Middle-endian and mixed-endian refer to other byte orderings.
\end{itemize}

Consider again the same simplified model:

\begin{tabular}{c c c c c c c c c c c c c}
\hline
address & 0 & 1 & 2 & 3 & ... & \textbf{239} & \textbf{240} & \textbf{241} & \textbf{242} & \textbf{243} & 244 & ... \\
\hline
value (hex) & ef & cd & ab & 89 & ... & ff & \textbf{a0} & \textbf{a1} & \textbf{a2} & \textbf{a3} & 42 & ... \\
\hline
\end{tabular}


\begin{itemize}
    \item "big-endian" (BE): 32-bit int at 240 is (hex) \textbf{a0 a1 a2 a3}
    \begin{itemize}
        \item[] = (decimal) $160 \times 2^{24} + 161 \times 2^{16} + 162 \times 2^{8} + 163$
        \item[] = (decimal) 2,694,947,491
    \end{itemize}
    \item "little-endian" (LE): 32-bit int at 240 is (hex) \textbf{a3 a2 a1 a0}
    \begin{itemize}
        \item[] = (decimal) $163 \times 2^{24} + 162 \times 2^{16} + 161 \times 2^{8} + 160$
        \item[] = (decimal) 2,745,344,416
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item \texttt{x86\_64} is LE
    \item \texttt{AArch64} is LE by default (LE-only on Windows, MacOS, Linux)
\end{itemize}
\subsubsection{Bit ordering}

Because we cannot access individual bits on a CPU (smallest chunk is a byte), 
bit ordering does not matter here.

However the same problem crops up in other contexts (USB, Ethernet, Wifi, \ldots)

\subsubsection{Memory access notation}

\begin{itemize}
\item In assembly, accessing memory is denoted using ``\texttt{[}'' and ``\texttt{]}''
\begin{itemize}
\item Moving the value 240 into a register:
\begin{lstlisting}
mov eax, 240   # eax = 240
ldr w0, 240    # w0 = 240
\end{lstlisting}

\item Moving the 4 bytes of memory at address 240 into a register:
\begin{lstlisting}
mov eax, DWORD PTR [240]   # eax = (hex) a3a2a1a0
ldr w0, [240]              # w0 = (hex) a3a2a1a0
\end{lstlisting}
\end{itemize}
\end{itemize}




















\newpage
\section{Compilation, Operating Systems, Virtual Memory}

\subsection{Compilation}
The compilation process can be divided into four steps:
\begin{itemize}
    \item Pre-processing
    \item Compiling
    \item Assembling
    \item Linking
\end{itemize}
Most compilation systems provide a \textbf{compiler driver} that invokes these four steps, as needed on behalf of the user.

\textbf{A compiler:}

\begin{itemize}
    \item reads source code,
    \item forms chunks of
    \begin{itemize}
        \item data (constants, global variables)
        \item executable machine code (functions)
    \end{itemize}
    \item associates a \textit{symbol} to each chunk (variable or function name)
    \item writes all into an ``object'' (“.o”) file (format: ELF, COFF, Mach-O)
\end{itemize}

The compiler leaves blank all \textit{references to symbols} (incl. external symbols like global variables and global functions)


\subsubsection{Linking}

A linker reads ``object'' files and writes an executable file.

\begin{itemize}
  \item it assigns a position in memory to every chunk of code and data
  \item it sets the value of the corresponding \textbf{symbol} to this position
  \item it resolves all references to \textbf{symbols}:
  replaces all references with the numeric value of the corresponding position in memory
\end{itemize}

\subsubsection{Static and dynamic linking}

\begin{itemize}
    \item Static linking is performed in order to prepare an executable (.exe, ...) file.
    \item Dynamic linking is performed every time the executable is run
    \begin{itemize}
        \item Object files built to be dynamically linked are called
        \begin{itemize}
            \item shared objects (.so, Linux, MacOS), or
            \item dynamically-linked libraries (.dll, Windows)
        \end{itemize}
        \item Typically used for
        \begin{itemize}
            \item System libraries
            \item Plugins
        \end{itemize}
    \end{itemize}
\end{itemize}

\paragraph{Why a separate linking phase?}

\begin{itemize}
    \item Separate linking simplifies compilations
    \begin{itemize}
        \item (allows the compiler to write code using functions and variables it has not seen yet)
    \end{itemize}
    \item It allows us to break down our code into multiple files...
    \begin{itemize}
        \item that can be compiled separately
    \end{itemize}
    \item It allows using code written and compiled by other people
    \begin{itemize}
        \item saves time
        \item lets us use closed-source software
    \end{itemize}
    \item Dynamic linking allows us to use system libraries without shipping them
    \item It reduces the size of executables
    \item It helps in masking some system incompatibilities
    \begin{itemize}
        \item (e.g., run the same .exe on Windows 10 and 11)
    \end{itemize}
    \item It allows updating system libraries separately
    \item The compiler does not know the code inside external object files
    \begin{itemize}
        \item it cannot check for mistakes based on that knowledge
        \item it cannot optimize code based on that knowledge (at least for dynamic linking)
    \end{itemize}
    \item Dynamically-linked libraries add complexity
    \begin{itemize}
        \item (separate installation, incompatible versions, etc.)
    \end{itemize}
\end{itemize}

% The [TBD] tokens represent non-textual elements in the original image that are not transcribed to LaTeX.




% Start of LaTeX content for transcribing the lecture notes

\subsubsection{Libraries}

Libraries are collections of functions (and data) that can be used by different executables
Examples:
\begin{itemize}
    \item \texttt{libjpeg}: read/write jpeg files
    \item \texttt{libssl}: cryptography
    \item \texttt{BLAS}: fast vector and matrix operations
    \item \texttt{Qt}: cross-platform GUI toolkit
\end{itemize}

Most languages have a \textit{standard library},  i.e. a library made available across implementations of a programming language. 
\begin{itemize}
    \item Distinct from the language itself, but usually necessary in any program
    \item The C language provides no functions. \\
    (All basic utilities (\texttt{strlen}, \texttt{printf}, \texttt{exit}) come from the standard library.)
    \item It is normally \textbf{dynamically linked}
\end{itemize}

\subsubsection{Optimizing compilers}

Optimizing compilers aim to improve the efficiency and performance of code during the compilation process. An example of such optimization is demonstrated through a simple C program:
\begin{lstlisting}
int main() {
    int r = 0;
    for (int i = 0; i < 1000000; i++) {
        r = r + 2;
    }
    return r;
}
\end{lstlisting}

Instead of executing the loop one million times at runtime, the compiler optimizes the code by pre-calculating the final value of the variable \texttt{r}. This is achieved by a technique known as \textit{constant folding}, where the compiler evaluates the result of constant expressions at compile time rather than at runtime.

\paragraph{Note}
\begin{itemize}
    \item ``Optimal'' = ``best''
    \item ``Optimizing'' = ``going towards the best possible result''
    
    \item Do not say: ``I made my code \textit{more optimal}''
    \item Do say: ``\textit{I optimized my code some more}'' \\
    or \\
    ``\textit{I made my code faster}''
\end{itemize}

% End of LaTeX content for transcribing the lecture notes




% This is a LaTeX transcription of the textual content from an image of lecture material on Operating Systems
% Please note that it is intended for direct use in a LaTeX editor and compilation without modification.
% You will need to ensure that your LaTeX environment has the appropriate packages installed to handle lists and sections.

\subsection{Operating Systems}

The operating system (OS) manages the computer and provides services to applications.

\subsubsection{Components}

\begin{itemize}
  \item The kernel handles:
  \begin{itemize}
    \item most of the boot process (what happens upon power on)
    \item memory allocation and sharing
    \item input/output devices, through ``drivers'' (often dynamically loaded)
    \item application coexistence and cooperation
  \end{itemize}
  \item Optionally:
  \begin{itemize}
    \item Standard libraries for some languages (C, C++, .NET, Swift, \ldots)
    \item Some additional common libraries
    \item User interface (UI): command-line (CLI), graphical (GUI)
    \item Some tools: CLI utilities, compilers, settings/configuration apps
  \end{itemize}
\end{itemize}
The critical code of the kernel is usually loaded into a separate area of memory, which is protected from access by application software.

\paragraph{Popular OSs:}

\begin{itemize}
  \item Windows
  \item MacOS, iOS (base OS: Darwin, kernel: XNU)
  \item Android, SteamOS (kernel: Linux)
\end{itemize}

\paragraph{Other current OSs:}

\begin{itemize}
  \item Debian, Ubuntu, Suse, Fedora, Arch, RHEL, AL2 (base OS: GNU, kernel: Linux)
  \item FreeBSD, OPNsense, TrueNAS, pfSense (base OS \& kernel: FreeBSD)
  \item OpenBSD
\end{itemize}

All the above except Windows are descendants from ``Unix''




% Start of LaTeX code


% Slide title: omitted as per the instruction
\subsubsection{Example: File Handling in C on Linux Systems}
In C programming on Linux systems, file operations are commonly handled using functions provided by the standard library. For instance, to open a file for reading, the \texttt{fopen} function is used:
\begin{lstlisting}
    FILE *f = fopen("my\_file.txt", "r");    
\end{lstlisting}

The \texttt{fopen} function is part of the standard library and performs the following actions:

\begin{itemize}
    \item Calls Unix-specific \texttt{open()} function, also in the standard library.
    \item Acts as a high-level wrapper for the \texttt{open} system call in the Linux kernel.
    \item Allocates a structure which includes buffers for data and the file descriptor.
    \item Returns a file descriptor (of type \texttt{int}) upon successful opening of the file.
\end{itemize}

The \texttt{open} system call in the Linux kernel uses the filesystem and storage device drivers, such as SSD drivers, to access the file.

\subsubsection{Levels of abstraction}
\begin{itemize}
    \item the processor only does elementary operations (move 64-bit to/from memory)
    \item the \textit{kernel} implements basic functionality (managing devices, reading data from a file)
    \item the \textit{standard library} provides more, OS-independent functionality (buffering, parsing data)
    \item other \textit{libraries} may allow even more (e.g., decompressing a video file)
\end{itemize}

% Slide title: MEMORY (AGAIN)
\subsection{Memory (again)} % creating a non-numbered section for MEMORY (AGAIN)
\subsubsection{Memory Virtualization}
Memory virtualization is a fundamental concept in modern computing systems. It allows each process to see memory as if it were the only process running on the system. This isolation is achieved through the following mechanisms:

\begin{itemize}
    \item Each time a process accesses memory, the hardware translates the virtual address provided by the process into a physical hardware address.
    \item This translation is performed using a \textit{\textbf{page table}}, which is managed by the operating system's kernel.
\end{itemize}
All virtual addresses correspond to some hardware address, but \textbf{not} all hardware addresses correspond to some virtual address (some hardware addresses are reserved for the page table itself, for instance).

\paragraph{An example}
Consider the C code example provided:

\begin{lstlisting}
#include <stdio.h>

int the_number = -1;

int main() {
    scanf("%d", &the_number);
    return 0;
}
\end{lstlisting}

The assembly instructions corresponding to the \texttt{scanf} function call might look like this:

\begin{lstlisting}
# x86_64 Assembly
mov eax, DWORD PTR [4100]
\end{lstlisting}

\paragraph{Page table (managed by the kernel)}
The page table is set up by the computer's operating system.\\
Here's how the memory virtualization works in this context:

\begin{itemize}
    \item The processor looks up virtual address 4100 in the page table.
    \item It identifies the page (in this case, page 1), finds the base hardware address for the page (20480), and adds the offset within the page (4).
    \item Consequently, the actual memory access takes place at the hardware address 20484 (20480 + 4).
\end{itemize}

The page table example shows the mapping from virtual addresses to hardware addresses:\\

\begin{tabular}{|c|l|l|}
\hline
\textbf{page} & \textbf{virtual address} & \textbf{hardware address} \\
\hline
0 & 0 -- 4095 & 65536 -- 69631 \\
1 & 4096 -- 8191 & 20480 -- 24575 \\
2 & 8192 -- 12287 & 4096 -- 8191 \\
... & ... & ... \\
\hline
\end{tabular}

\vspace{3.5mm}
For architectures like ARM (AArch64), the assembly instruction for accessing memory would look different:

\begin{lstlisting}
# AArch64 Assembly
ldr w0, [4100]
\end{lstlisting}



% For tabular data, an actual tabular environment is used to transcribe the content.


\subsubsection{Page table}
\begin{itemize}
    \item the page table itself is in memory!
    \item at a specific hardware address
    \item various techniques to make page lookup faster (it is a tree, with a cache)
\end{itemize}

\subsubsection{Memory allocation}
\begin{itemize}
    \item The kernel is responsible for finding free hardware addresses that are not in use by any process.
    \item Virtual addresses are managed by the kernel, which can:
    \begin{itemize}
        \item Allocate specific virtual addresses as requested by a process.
        \item Search for and allocate free virtual addresses that are currently unassigned.
    \end{itemize}
    \item Suitable entries are then added to the page table by the kernel.
    \item The kernel returns the virtual address to the requesting process.
\end{itemize}

\subsubsection{Virtual memory}
Virtual memory introduces several advantages and some drawbacks:

\paragraph{Cons:}
\begin{itemize}
    \item The process of translating virtual addresses to physical addresses can be slow.
    \item Initially, any memory sharing between processes requires mediation by the kernel.
\end{itemize}

\paragraph{Pros:}
\begin{itemize}
    \item Simplifies memory management for processes.
    \item Ensures process separation, preventing processes from interfering with each other.
    \item Allows quick relocation of large memory blocks by simply updating page table entries.
    \item Facilitates fast input/output operations as devices can be mapped to virtual addresses.
    \item Supports the extension of memory using various methods:
    \begin{itemize}
        \item Swap space on storage devices.
        \item Memory compression techniques.
        \item Overcommitting memory beyond the physical limits.
    \end{itemize}
\end{itemize}

\paragraph{Remarks on virtual memory}

\begin{itemize}
    \item Virtual addresses do not necessarily correspond to a hardware address; they are mapped to hardware addresses when needed.
    \item Not all hardware addresses correspond to virtual addresses; some may be reserved for the operating system or for special purposes.
    \item A process can map a virtual address to a specific location in memory, including to a cache line.
    \item Different processes can map the same virtual address to different hardware addresses, providing process isolation.
\end{itemize}


\subsubsection{Function Calls and Stack Management}

In C, functions use the stack to manage local variables and return addresses. The stack is LIFO (last-in, first-out). 

\textbf{Function f1} initializes with two \texttt{uint64\_t} variables and calls \texttt{f2} and \texttt{f3} in sequence:
\begin{lstlisting}
void f1(void) {
    uint64_t a, b;
    f2();
    f3();
}
\end{lstlisting}

\textbf{Function f2} allocates a \texttt{uint64\_t} variable and calls \texttt{f3}:
\begin{lstlisting}
void f2(void) {
    uint64_t c;
    f3();
}
\end{lstlisting}

\textbf{Function f3} may use stack space, which is reclaimed when it returns to \texttt{f2}:
\begin{lstlisting}
int f3(void) {
    // function body
}
\end{lstlisting}

\texttt{f3} executes and eventually returns to \texttt{f2}, which then returns to \texttt{f1}. Each return unwinds the stack, removing the local variables and return addresses.\\

\textbf{Stack After f3 Returns to f2}\\
After \texttt{f3}'s completion, the stack contains \texttt{f1}'s \texttt{a} and \texttt{b}, along with \texttt{f2}'s \texttt{c}.\\

\textbf{Stack After f2 Returns to f1}\\
Once \texttt{f2} has returned, the stack holds only \texttt{f1}'s variables \texttt{a} and \texttt{b}.\\

\textbf{Function f1 Calls f3 Again}\\
After the completion of \texttt{f2}, \texttt{f1} invokes \texttt{f3} once more, preparing the stack for another potential set of local variables from \texttt{f3}.\\

\textbf{Function f1 Completes}
\begin{lstlisting}
void f1(void) {
    // final actions of f1
    return;
}
\end{lstlisting}
Finally, \texttt{f1} returns, and the stack is cleared to its state prior to \texttt{f1}'s call.

The lifecycle of the stack from the initiation of \texttt{f1} through the nested calls and back illustrates the LIFO nature of the stack in function call management.


\subsubsection{Stack pointer}
\begin{itemize}
    \item x86\_64: rsp (by convention -- rsp is a general register)
    \item AArch64: sp (mandatorily -- sp is a special register)
    \item In both cases, the stack actually grows downwards
    \item Default stack size on Linux: 8 MB
    \begin{itemize}
        \item theoretical max recursion depth: 1,000,000
    \end{itemize}
\end{itemize}

\subsubsection{Stack vs. Heap}
\begin{itemize}
    \item The \textbf{stack} is used for static memory allocation, which includes local variables.
    \item The \textbf{heap} is used for dynamic memory allocation, accessed via pointers and memory allocation functions like \texttt{malloc}.
\end{itemize}

\paragraph{Code Example Analysis}
\begin{lstlisting}[language=C]
int a = 0;

int f(char *b) {
    char c[2];
    c[0] = b[a] * 3;
    return printf("%s\n", c);
}
\end{lstlisting}

\begin{itemize}
    \item "\texttt{int a}" is a global variable, not allocated  typically neither on the stack nor the heap.
    \item "\texttt{char c[2]}" in \texttt{f(char *b)} indicates local, stack-based allocation.
    \item Pointer "\texttt{char *b}" could reference stack or heap memory, depending on its initialization.
    \item Memory for "\texttt{b}" could be on the heap if allocated via \texttt{malloc} or similar.
    \item Printing "\texttt{c}" executes a formatted output function, using a stack-allocated string.
\end{itemize}


\subsection{Tools}

\begin{itemize}
    \item Windows Subsystem for Linux
    \item Basics
    \begin{itemize}
        \item \texttt{cd}, \texttt{ls}, \texttt{less}
        \item TAB completion
        \item command-line parameters, \texttt{-h}
        \item \texttt{man}
    \end{itemize}
    \item package management
    \begin{itemize}
        \item Debian/Ubuntu: \texttt{apt-get}
        \item Fedora/Suse: \texttt{dnf}
        \item MacOS: \texttt{brew}
        \item Windows: \texttt{winget}
    \end{itemize}
\end{itemize}



















\newpage
\section{Compiler invocation, IDEs, build systems}

\subsection{Compiling}

\subsubsection{Historical compilers}

\begin{itemize}
    \item Proprietary
    \begin{itemize}
        \item Intel C++ Compiler (ICC, 1970's?)
        \item Microsoft Visual C++ (MSVC, 1993)
        \item ARM Compiler (ARMCC, 2005)
        \item AMD Optimizing C/C++ Compiler (AOCC, 2017)
    \end{itemize}
    \item Open source
    \begin{itemize}
        \item GNU Compiler Collection (GCC, 1987)
        \item LLVM (2003--)
    \end{itemize}
\end{itemize}
\subsubsection{Evolution of compilers}
\begin{itemize}
    \item 2014: ARM Compiler rebased on LLVM
    \item 2017: AMD Compiler was always based on LLVM
    \item 2021: Intel C++ Compiler rebased on LLVM
\end{itemize}

\subsubsection{Current major compilers}
\begin{itemize}
    \item Microsoft Visual C++
    \begin{itemize}
        \item default on MS Windows (in MS Visual Studio)
    \end{itemize}
    \item GCC
    \begin{itemize}
        \item default on most open source OSs
    \end{itemize}
    \item LLVM (for C/C++: Clang)
    \begin{itemize}
        \item base for hardware vendor (Intel, ARM, AMD, nVidia) compilers
        \item default on MacOS, iOS (in Apple X Code)
        \item default for native applications on Android
    \end{itemize}
\end{itemize}

\subsubsection{Components of a compiler}
\begin{itemize}
    \item Front-end (parses and analyses code -- language-specific)
    \item Intermediate representation (IR) (most code optimization happens here)
    \item Back-end (writes assembly or machine code -- ISA-specific)
\end{itemize}

\paragraph{LLVM frontends:}
\begin{itemize}
    \item C and C++ (Clang), Fortran (Flang), Rust, Zig, Swift
\end{itemize}

\paragraph{LLVM backends:}
\begin{itemize}
    \item Intel/AMD/ARM compilers, nVidia CUDA compiler, AMD ROCm
\end{itemize}

\subsubsection{LLVM IR}
 gibberish lstlisting picture

\subsubsection{Compiler invocation (1)}

\begin{itemize}
  \item As usual, use \texttt{man gcc} / \texttt{man clang} for help.
  \item Compile and link:
    \begin{lstlisting}
      gcc -o executable source_code.c
    \end{lstlisting}
  \item Compile only:
    \begin{lstlisting}
      gcc -c -o file.o file.c
    \end{lstlisting}
  \item Link only:
    \begin{lstlisting}
      gcc -o executable file.o file1.o file2.o file3.o
    \end{lstlisting}
  \item Write assembly (see also: )
    \begin{lstlisting}
      gcc -S assembly.s source_code.c
    \end{lstlisting}
  \item Internally, \texttt{gcc} runs other tools (assembler: \texttt{as}, linker: \texttt{ld})
\end{itemize}

\subsubsection{Compiler invocation (2)}

\begin{itemize}
  \item Enable warnings:
    \begin{lstlisting}
      gcc -Wall -c -o file.o file.c
    \end{lstlisting}
  \item Enable optimization:
    \begin{lstlisting}
      gcc -Wall -O3 -c -o file.o file.c
    \end{lstlisting}
\end{itemize}

\subsubsection{Note for MacOS}
Install \texttt{binutils}:
\begin{itemize}
    \item from MacPorts \url{https://www.macports.org}
    \begin{lstlisting}
    port install binutils
    \end{lstlisting}
    \item or from Homebrew \url{https://brew.sh/}
    \begin{lstlisting}
    brew install binutils
    \end{lstlisting}
\end{itemize}
Utilities may be prefixed by a \texttt{g}:
\begin{itemize}
    \item \texttt{objdump} $\rightarrow$ \texttt{gobjdump}
\end{itemize}

\subsubsection{Tools}
\begin{itemize}
    \item \texttt{hexdump} dump hexadecimal representation of any file
    \begin{itemize}
        \item \texttt{hexdump -C} also print ASCII for valid ASCII bytes
        \item \texttt{hexdump -C \textbar less} ``pipe'' output to pager
        \item \texttt{hexdump -C > file.hex} write output to a file
    \end{itemize}
    \item \texttt{readelf} print symbols in ELF object file
    \begin{itemize}
        \item \texttt{readelf -a} print all object information
    \end{itemize}
    \item \texttt{objdump} dump contents of object file
    \begin{itemize}
        \item \texttt{objdump -M intel -d} disassembles object file, prints assembly code
        \item \texttt{objdump -p} similar to \texttt{readelf}
    \end{itemize}
    \item or online: \url{http://godbolt.org}
\end{itemize}

\subsection{Editing Code}

\subsubsection{Applications for writing code}

\begin{itemize}
  \item Text editors, i.e. Notepad
  \item Code editors, i.e. emacs, vim, Notepad++, VS Code
  \item Integrated development environment (IDE), i.e. Microsoft Visual Studio, Apple XCode, IDE: IntelliJ IDEA (paid)
\end{itemize}

\subsubsection{More code editors}

\begin{itemize}
    \item gedit
    \item Kate
    \item Sublime Text (paid)
    \item many more...
\end{itemize}

\subsubsection{More IDEs}
\begin{itemize}
    \item PyCharm (Python, paid)
    \item Android Studio (paid)
    \item KDevelop
    \item QtCreator
    \item Dev-C++
    \item Spyder (Python)
    \item \ldots
\end{itemize}

\subsubsection{Code editor vs. IDE}
\textbf{IDE pros:}
\begin{itemize}
    \item one-click compile
    \item IDE aware of whole project
    \begin{itemize}
        \item can suggest code completions from different files
    \end{itemize}
    \item integrated tools (e.g. debugger)
\end{itemize}

\textbf{IDE cons:}
\begin{itemize}
    \item Project setup takes time and effort
    \item ``Walled garden'' problem
    \begin{itemize}
        \item By default, anyone who wants to compile your project needs the same IDE.
    \end{itemize}
\end{itemize}

\subsection{Build Systems}

\subsubsection{How do we compile a complex project?}

\begin{itemize}
    \item Option 1:
    \begin{lstlisting}
        gcc -Wall -O3 -c -o ggml.o ggml.c
        gcc -Wall -O3 -c -o ggml-alloc.o ggml-alloc.c
        g++ -Wall -O3 -c -o llama.o llama.cpp
        g++ -Wall -O3 -c -o common/common.o common/common.c
        g++ -Wall -O3 -c -o console.o console/common/console.c
        g++ -Wall -O3 -c -o grammar-parser.o common/grammar-parser.c
        g++ -Wall -O3 -shared -fPIC -o common.so common/ggml-alloc.o llama.o \
            common.o console.o grammar-parser.o
    \end{lstlisting}
    
    \item Option 2:
    \begin{itemize}
        \item Put above commands in a "shell script" file, e.g., \texttt{compile.sh}
        
        \item Run:
        \begin{lstlisting}
            ./compile.sh
        \end{lstlisting}
        
        \item Problems:
        \begin{itemize}
            \item Difficult to modify (e.g., change compiler options)
            \item We recompile everything everytime
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Build automation}

\begin{itemize}
    \item IDE integrated:
    \begin{itemize}
        \item Visual Studio
        \item Xcode
    \end{itemize}
    
    \item Stand-alone:
    \begin{itemize}
        \item make
        \item Bazel (based on Google's internal tool Blaze) / Buck (Facebook)
        \item Ninja (Google, for Chrome)
        \item CMake (uses make, Ninja,...), qmake (uses make), Meson (uses Ninja, ...)
    \end{itemize}
\end{itemize}

\subsubsection{Make}
Example:
\begin{itemize}
    \item The \texttt{Makefile} contains rules for compiling source files and generating the shared library \texttt{libllama.so}.
    \item Compilation is performed using \texttt{gcc} for C files and \texttt{g++} for C++ files, with flags \texttt{-Wall} (enable all warnings), \texttt{-O3} (optimization level 3), and \texttt{-c} (compile without linking).
    \item The \texttt{.o} object files are created from their respective source files.
    \item The \texttt{libllama.so} shared library is created by linking the object files \texttt{gmgml.o}, \texttt{gmgml-alloc.o}, \texttt{llama.o}, \texttt{common.o}, \texttt{console.o}, and \texttt{grammar-parser.o} using \texttt{g++} with flags \texttt{-shared} and \texttt{-fPIC} (Position Independent Code).
    \item Command \texttt{make libllama.so} triggers the compilation and linking process defined in the \texttt{Makefile}, resulting in the creation of \texttt{libllama.so}.
    \item Run
    \begin{lstlisting}
    make libllama.so
    \end{lstlisting}
\end{itemize}



\subsubsection{Make rule syntax}

\begin{lstlisting}
target: source0 source1 source2 ...
       recipe
\end{lstlisting}

Whenever one of the sources was modified after the target, run the recipe (to rebuild the target).

Otherwise, consider target up-to-date and do nothing.

\subsubsection{Make variables}


\begin{itemize}
    \item \textbf{Compiler Variables:} The Makefile specifies compilers and the associated flags for compiling C and C++ files. The variables \texttt{CC} and \texttt{CXX} are used for the C and C++ compilers respectively, with \texttt{gcc} for C and \texttt{g++} for C++. The flags \texttt{-Wall} (enables all compiler's warning messages) and \texttt{-O3} (optimization level 3) are set for both compilers using \texttt{CFLAGS} and \texttt{CXXFLAGS}.
    \item \textbf{Object File Rules:} Rules for generating object files from source files are defined. Each rule follows the format \texttt{target: dependencies} followed by the command to create the target. For example, \texttt{gmgml.o} is compiled from \texttt{gmgml.c} and depends on header files \texttt{gmgml.h} and \texttt{gmgml-cuda.h}.
    \item \textbf{Use of Variables:} The \texttt{\$(...)} syntax is used to reference variables. For instance, \texttt{\$(CC)} and \texttt{\$(CFLAGS)} in a rule's command extract the values of the \texttt{CC} and \texttt{CFLAGS} variables respectively. 
    \item \textbf{Shared Library:} The target \texttt{libllama.so} demonstrates creating a shared library from multiple object files. It uses the \texttt{-shared} and \texttt{-fPIC} flags, with \texttt{-fPIC} indicating Position Independent Code, necessary for shared libraries.
\end{itemize}



\subsubsection{Special make variables}
\begin{itemize}
    \item \verb|$(@)| the target of the current rule
    \item \verb|$<)| the first source of the current rule
    \item \verb|$(^)| all the sources of the current rule
\end{itemize}

\subsubsection{Static pattern rules}
\begin{itemize}
    \item Static pattern syntax:
    \begin{lstlisting}
target0 target1 target2 ... : target-pattern : source-pattern
    recipe
    \end{lstlisting}
    
    \item Target pattern contains \%, which will match anything
    \item Source pattern also contains \%, which is replaced by the match in target
    \item Example:
    \begin{lstlisting}
some_file.o other_file.o third_file.o : %.o : %.c
    recipe
    \end{lstlisting}
    is equivalent to:
    \begin{lstlisting}
some_file.o: some_file.c
    recipe
other_file.o: other_file.c
    recipe
third_file.o: third_file.c
    recipe
    \end{lstlisting}
\end{itemize}
[two big lstlisting]

\subsubsection{Phony and default targets}

A "phony" target does not necessarily correspond to a file name:

\begin{lstlisting}
.PHONY: clean

clean:
    rm libllama.so
\end{lstlisting}

If no target is provided to the make command, the default target is the first one. A common pattern is:

\begin{lstlisting}
.PHONY: default

default: libllama.so
\end{lstlisting}

\begin{itemize}
    \item \textbf{.PHONY Target:}
    \begin{itemize}
        \item The `.PHONY` target is a built-in Make feature that helps to define targets that are not associated with files. Targets under `.PHONY` are always considered out of date and Make executes their recipes every time they are invoked, irrespective of any file with the same name as the target.
        \item In the example Makefile, `.PHONY` is used to declare `default` and `clean` as phony targets. This ensures that `make default` will build the `libllama.so` library and `make clean` will execute the cleaning commands even if files named `default` or `clean` exist in the directory.
    \end{itemize}
    
    \item \textbf{Usage of .PHONY in the Example:}
    \begin{itemize}
        \item \texttt{default}: This phony target is usually the first in a Makefile and therefore the default when `make` is executed without arguments. It depends on \texttt{\$(LIBTARGET)}, which causes the build of the `libllama.so` shared library.
        \item \texttt{clean}: This target is responsible for removing all the build artifacts, such as object files and the shared library, thus cleaning the build directory. The command `rm -f \$(COBJS) \$(CXOBJS) \$(LIBTARGET)` is invoked, which forces removal without prompting, even if there are no files to remove (the `-f` option).
    \end{itemize}
\end{itemize}

\subsubsection{Using shell commands}

The syntax is:

\begin{lstlisting}
$(shell any-shell-command)
\end{lstlisting}

For example:

\begin{lstlisting}
TODAY := $(shell date)
CFILES := $(shell ls *.c)
\end{lstlisting}




\subsubsection{String replacement in variables}

\begin{itemize}
  \item The syntax is:
  \begin{lstlisting}
    $(variable:pattern=replacement)
  \end{lstlisting}

  \item The pattern contains \%, which will match any substring
  \item The replacement may contain \%, which will be replaced by the matched substring
  \item For example:
  \begin{lstlisting}
    C_FILES := $(shell ls *.c)
    O_FILES := $(C_FILES:%.c=%.o)
  \end{lstlisting}
\end{itemize}

\subsubsection{For more about make}

\begin{lstlisting}
# Using make
man make

# Writing Makefiles
info make
\end{lstlisting}





















\newpage
\section{Programming Languages}

\subsection{Compilers vs Interpreters}
\subsubsection{Parsing}

Parsing is the process of taking the source code and creating the corresponding abstract syntax tree (AST).

Example:

\begin{lstlisting}
t = 3 * ((y * w) + x)
\end{lstlisting}


\subsubsection{Compiler vs. interpreter}

\begin{itemize}
  \item A compiler:
  \begin{itemize}
    \item parses the source code into an AST
    \item takes the AST and [...] writes the corresponding assembly / machine code
  \end{itemize}
  \item An interpreter:
  \begin{itemize}
    \item parses the source code into an AST
    \item takes the AST and performs the corresponding operations
  \end{itemize}
\end{itemize}

\subsubsection{Pros and cons}
Programming languages can be implemented either through compilation or interpretation, though this is not an inherent property of the language itself. Below are the advantages and disadvantages of interpreters, along with examples and explanations of how languages compile.

\paragraph{Advantages of interpreters:}
\begin{itemize}
    \item Eliminates the compilation step, simplifying the execution process.
    \item Provides portability across different platforms without the need for recompilation.
\end{itemize}


\paragraph{Disadvantages of interpreters:}
\begin{itemize}
    \item Requires the presence of the interpreter on the user's machine.
    \item Typically executes code slower than compiled native machine code.
\end{itemize}

Compiled or interpreted is \textit{not an inherent property} of a language.

\paragraph{Example: Python}
Python, often used as a prime example of an interpreted language due to its reference implementation (CPython), showcases the flexibility of language implementations. CPython compiles Python code to an intermediate form known as bytecode, which it then interprets. However, other implementations like PyPy use Just-In-Time compilation techniques to improve performance, reflecting the language's versatility.\\
Python exemplifies a hybrid approach. While CPython produces bytecode, translating Python code to an intermediate language, it also interprets the bytecode. This intermediate step adds a layer of abstraction that allows Python to be platform-independent while also incurring the typical overhead of interpretation. Moreover, alternative implementations like Cython can compile Python code into optimized C, potentially transforming Python scripts into high-performance executables.

\textbf{Preferred Execution Model:}
Despite the flexibility, languages tend to be associated with a standard execution model based on their common implementations—compiled languages like C and interpreted ones like Python.





\paragraph{Compiled languages:}
\begin{itemize}
  \item C, C++
  \item Rust, Go, Zig
  \item Pascal, Fortran, COBOL
\end{itemize}

\paragraph{Interpreted languages:}
\begin{itemize}
  \item Python, Javascript, Lua
  \item Lisp, Perl, PHP, R, Ruby, VBScript
\end{itemize}

\subsubsection{Compilation Targets and Strategies}

Understanding the compilation process involves knowing not only how source code is transformed but also what it is compiled into.

\paragraph{Compilation Targets}

Different languages choose different compilation targets:

\begin{itemize}
  \item \textbf{Nim:} Compiles to C code, which is then compiled again to machine code, leveraging the efficiency of C compilers.
  \item \textbf{Dart:} Targets JavaScript, allowing the compiled code to be interpreted by JavaScript engines in web browsers.
  \item \textbf{Java:} Compiles to bytecode for the Java Virtual Machine (JVM), which serves as an abstract processor architecture. The bytecode is portable but requires users to have the JVM for execution.
  \item \textbf{Python:} The CPython interpreter compiles Python into bytecode, which is immediately interpreted. This approach grants Python scripts a level of portability across platforms.
\end{itemize}

\paragraph{Pros and Cons of JVM}

The JVM introduces both advantages and drawbacks:

\begin{itemize}
  \item \textbf{Advantage:} JVM bytecode is platform-independent.
  \item \textbf{Drawback:} It necessitates that the JVM interpreter be installed on the user's machine.
\end{itemize}

\subsubsection{Just-in-Time Compilation}

An alternative approach to traditional compilation is Just-in-Time (JIT) compilation:

\begin{itemize}
  \item JIT avoids long compilation times by compiling code on-the-fly, as it is needed, at run time.
  \item This strategy can result in execution that is both portable and fast.
  \item By compiling section-by-section—be it a file, function, or block of code—JIT compilers can optimize at runtime, providing speed without sacrificing flexibility.
  \item JIT compilation may consist of source code translation, but is more commonly bytecode translation to machine code, which is then executed directly. 
\end{itemize}

JIT compilation offers a balance, aiming to combine the best aspects of interpretation (portability and platform independence) and compilation (speed).

\subsubsection{Languages with JIT compilation}

\begin{itemize}
    \item Julia
    \item C\#
    \item Java (source code compiled to JVM code; JVM code JIT compiled to native code)
    \item PyPy (Python)
    \item LuaJIT (Lua)
\end{itemize}



\subsubsection{Pros and cons (summary)}

\begin{tabular}{lcccc}
\toprule
 & Compiled & Interpreted & Compiled to VM & Just-in-time \\
\midrule
Needs compilation step & yes & no & yes & no \\
Needs interpreter / VM & no & yes & yes & yes \\
Portable & no & yes & yes & yes \\
Speed & fast & slow & in-between & slow at first, then fast \\
\bottomrule
\end{tabular}

\subsubsection{Language summary}

\begin{itemize}
    \item Ahead-of-time (AOT) compiled-to-machine-code languages:
    \begin{itemize}
        \item C, C++, Rust, Go, Zig, Pascal, Fortran, COBOL
        \item Nim (through C)
    \end{itemize}
    \item Purely interpreted languages:
    \begin{itemize}
        \item Lisp, Perl, R, VBScript
    \end{itemize}
    \item Other:
    \begin{itemize}
        \item Python, Lua: internally compiled to bytecode, then interpreted
        \item PyPy (Python), LuaJIT (Lua): internally compiled to bytecode, then JIT compiled
        \item Java, C\#: explicitly compiled to bytecode (bytecode shipped to user), then JIT compiled
        \item Julia: JIT compiled
        \item JavaScript: interpreted and JIT compiled
    \end{itemize}
\end{itemize}

\subsection{Types}

\subsubsection{Static vs. Dynamic Type Checking}

Type systems in programming languages can be broadly classified into static and dynamic type checking mechanisms, each with its approach to ensuring type correctness.

\paragraph{Static Type Checking}
\begin{itemize}
    \item In static type checking, the type correctness of variables is verified at compile-time.
    \item The compiler analyzes the source code and ensures that all type rules are followed before the code is run.
    \item Any type mismatch or type errors are detected and reported as compile-time errors.
    \item Example in C:
    \begin{lstlisting}
    int f() {
        return "this is a string" / 5; // Compile-time error
    }
    \end{lstlisting}
    \item The advantage of static type checking is that many errors can be caught early in the development process.
    \item It enforces type discipline, as the code must explicitly declare and adhere to type contracts.
\end{itemize}

\paragraph{Dynamic Type Checking}
\begin{itemize}
    \item Dynamic type checking, on the other hand, resolves types at runtime.
    \item It allows more flexibility in the code, as types are enforced when operations are actually performed.
    \item Errors due to type mismatches are thrown during the execution of the program, which may lead to runtime exceptions.
    \item Example in Python:
    \begin{lstlisting}
    def f():
        return "this is a string" / 5 # Runtime TypeError
    \end{lstlisting}
    \item This flexibility can be advantageous for rapid prototyping and for code that requires dynamic type behavior.
    \item However, it also means that certain types of errors will only surface when the problematic code path is executed.
\end{itemize}

\textbf{Comparative Analysis}\\
While static type checking adds a layer of safety by catching errors early, it requires more upfront declarations and can be more rigid. Dynamic type checking offers flexibility at the cost of potential runtime errors. The choice between static and dynamic typing depends on the specific needs and context of the software project.


\subsubsection{Strong and Weak Typing in Programming Languages}

The terms "strong" and "weak" typing are used to describe how strictly a programming language adheres to type rules, especially concerning type conversions and type safety.

\paragraph{Weak Typing}

Weak typing indicates a language where types may be implicitly converted and operations may succeed even if they do not make sense from a type-safety perspective.

\begin{itemize}
    \item Implicit type conversion allows assigning a floating-point number to an integer variable without an error, merely truncating the value.
    \item Pointer type conversions can lead to compiled code that may cause runtime errors or undefined behavior.
\end{itemize}
Example from C:
\begin{lstlisting}
int a = -1.8; // Implicitly converted to int, truncated to -1
int *p = (int *)((long int)"abc" + 5); // Compiles, but unsafe
*p = 3; // Dereferencing p could lead to a crash
\end{lstlisting}

\paragraph{Strong Typing}

Strong typing means the language enforces strict adherence to typing, preventing implicit, potentially unsafe conversions.

\begin{itemize}
    \item Operations on incompatible types are not allowed and result in a clear error message.
    \item Certain type-related operations are defined within the language, such as multiplying a string by an integer to repeat the string.
\end{itemize}

Example from Python:
\begin{lstlisting}
>>> "a" + 4
TypeError: Can only concatenate str (not "int") to str

>>> "a" * 4
'aaaa'
\end{lstlisting}


The distinction between strong and weak typing can affect the reliability, safety, and clarity of code. Strongly typed languages may prevent subtle bugs at the cost of less flexibility, while weakly typed languages might allow for a wider range of behaviors at the risk of runtime errors.




\subsection{Memory management}
\subsubsection{Manual Memory Management in C}

C programming requires developers to manage memory manually, which includes allocating and freeing memory as needed.

\paragraph{Common Mistakes in Memory Management}

When using \texttt{malloc} for dynamic memory allocation, common mistakes include:

\begin{itemize}
    \item Not checking if \texttt{malloc} actually succeeded in allocating memory.
    \item Forgetting to release memory with \texttt{free}, leading to memory leaks.
\end{itemize}

\paragraph{Example of Poor Memory Management}
The following C function \texttt{getint} does not check for allocation failure and forgets to free the allocated memory:

\begin{lstlisting}
int getint() {
    char *buffer = malloc(1024);
    size_t n = fread(buffer, 1, 1023, stdin);
    buffer[n] = '\0';
    return strtol(buffer, NULL, 0);
    // Missing free(buffer) leads to a memory leak
}
\end{lstlisting}

A more robust version of the function includes error checking and properly frees the allocated memory.


\subsubsection{Automatic memory management}
Automatic memory management simplifies the developer's job by abstracting the details of allocating and freeing memory. This is commonly seen in higher-level languages like Python.\\
Here, memory management is handled automatically through mechanisms like \textbf{reference counting} and \textbf{garbage collection}, which track and manage memory usage without direct intervention from the programmer.


\textbf{Example}
\begin{lstlisting}
def getint():
    buffer = input()
    return int(buffer)
\end{lstlisting}


% Slide title "How does automatic memory management work?"
\subsubsection{How does automatic memory management work?}

\begin{itemize}
    \item \textbf{Reference Counting:} It is the process of tracking how many references exist to a particular object in memory.
    \begin{itemize}
        \item When an object is created, its reference count is set to one.
        \item The reference count is incremented each time the object is referenced and decremented when the reference goes out of scope.
        \item When the reference count drops to zero, the object's memory is deallocated.
    \end{itemize}
    \item \textbf{Garbage Collection:} Complements reference counting by handling cyclic references where two or more objects refer to each other, preventing their reference counts from reaching zero.
\end{itemize}


% The title is not provided in the image, so it is left out in the LaTeX document

\paragraph{Refcount example} 
Consider the following pyhton function:
\begin{lstlisting}
def f():
    s = ("abc" + "def") + "ghi"
    t = s
    return t
\end{lstlisting}

\begin{enumerate}
  \item "abc" created, refcount 1
  \item "def" created, refcount 1
  \item "abcdef" created, refcount 1
  \item ("abc" + "def") is done, "abc" refcount 0, "def" refcount 0, both freed
  \item "ghi" created, refcount 1
  \item "abcdefghi" created, refcount 1
  \item ("abc" + "def") + "ghi" is done, "abcdef" and "ghi" freed
  \item s = "abcdefghi" done, but it is an assignment, refcount of "abcdefghi" stays 1
  \item s referenced, refcount of "abcdefghi" becomes 2
  \item t = s done, but it is an assignment, refcount stays 2
  \item t referenced, refcount of "abcdefghi" becomes 3
  \item return t is done, but it is a return, refcount of "abcdefghi" stays 3
  \item s and t go out of scope, refcount of "abcdefghi" becomes 1
\end{enumerate}

\subsubsection{Problem with refcounting}
\paragraph{Cycles:}

\begin{lstlisting}
class C:
    pass

def do_nothing():
    a = C()
    t = a
    for i in range(10000000):
        n = C()
        t.prev = t
        t = n
    a.prev = t
    return 1
\end{lstlisting}

% End of the LaTeX document with the lecture content





\subsubsection{Garbage collection}

\begin{itemize}
  \item keep track of all variables in scope
  \item keep track of all allocated blocks of memory
  \item every few seconds, ``garbage collection''
    \begin{itemize}
      \item look through all the variables, if they reference some memory, mark it as in-use
      \item look at every block, if not referenced, free it
    \end{itemize}
\end{itemize}

\begin{itemize}
  \item \textbf{Pro:} does not suffer from cycle issue
  \item \textbf{Con:} memory usage can grow a lot between garbage collections
  \item \textbf{Con:} garbage collections pauses can block the process for a long time (making it feel unresponsive)
\end{itemize}

\subsection{Other Language Features}


\subsubsection{Macros}
Macros in C are preprocessor directives that allow for automatic generation of source code fragments.

\paragraph{Example and Usage}
\begin{itemize}
    \item An example macro in C that repeats arguments:
    \begin{lstlisting}
    #define THIS_5X(a) a, a, a, a, a
    int array[10] = { THIS_5X(1), THIS_5X(2) };
    \end{lstlisting}
    This expands into an array initialization with repeated values.

    \item Macros can calculate the number of elements in an array:
    \begin{lstlisting}
    #define ARRAY_ELEMENTS(a) (sizeof(a) / sizeof((a)[0]))
    \end{lstlisting}
\end{itemize}

\paragraph{Cautions with Macros}
Macros are simple text replacements and can lead to unexpected results without careful use.
\begin{itemize}
    \item Incorrect macro that doesn't account for operator precedence:
    \begin{lstlisting}
    #define PRODUCT_WRONG(a, b) a * b
    int a = PRODUCT_WRONG(1 + 2, 3 + 4); // Yields 11, not 21
    \end{lstlisting}
    
    \item Corrected macro with parentheses ensuring proper precedence:
    \begin{lstlisting}
    #define PRODUCT_CORRECT(a, b) ((a) * (b))
    int a = PRODUCT_CORRECT(1 + 2, 3 + 4); // Correctly yields 21
    \end{lstlisting}
\end{itemize}

\subsubsection{Generics}
Generics provide a way to write functions and data structures that can operate on any data type.

\paragraph{Generics in Different Languages}
\begin{itemize}
    \item In C, without built-in generics, functions must be written for each data type:
    \begin{lstlisting}
    void int_array_sort(int *array, int size);
    void float_array_sort(float *array, int size);
    \end{lstlisting}

    \item Python's dynamic type checking negates the need for explicit generics:
    \begin{lstlisting}
    def array_sort(array):
        # Works for any type that supports comparison and sorting
    \end{lstlisting}

    \item C++ introduces templates, which are its way of implementing generics:
    \begin{lstlisting}
    template <typename T>
    void array_sort(T *array);
    \end{lstlisting}
    Templates allow functions and classes to operate with any type.
\end{itemize}

\subsubsection{Languages with generics}

\begin{itemize}
    \item C++
    \item C\#
    \item Java
    \item Go
    \item Rust
    \item Swift
    \item TypeScript
    \item \ldots
\end{itemize}




\subsubsection{Object-oriented programming}

A compound type is any type that is defined in terms of one or more other types.

\begin{itemize}
    \item In C:
\begin{lstlisting}
struct point {
    float x;
    float y;
};
\end{lstlisting}

    \item In Python:
\begin{lstlisting}
class Point:
    def __init__(self):
        self.x = 0.0
        self.y = 0.0
\end{lstlisting}
\end{itemize}

In object-oriented programming (OOP), compound types (``classes'')
can have functions attached to them (``methods'').

As a consequence, in OOP, \emph{data} and the \emph{methods} that operate on them are usually defined close together.

We can construct complex type hierarchies:
\begin{itemize}
    \item define a class for \emph{vehicle}, has a price method
    \item define a class for \emph{bike}, inherits from \emph{vehicle}
    \begin{itemize}
        \item inherits the price method from \emph{vehicle} (no need to rewrite it)
        \item among other properties, has two wheels
    \end{itemize}
    \item define a class for \emph{car}, inherits from \emph{vehicle}
    \begin{itemize}
        \item inherits the price method from \emph{vehicle} (no need to rewrite it)
        \item among others has four wheels
    \end{itemize}
    \item etc.
\end{itemize}



% The content of the lecture in LaTeX format

\subsubsection{Functional programming}

Functional programming treats functions as "first-class" citizens of the programming language.

\begin{itemize}
    \item Functions can be used within expressions.
    \item Functions can be assigned to variables, passed as arguments, or returned from other functions.
\end{itemize}
\textbf{A python example}
\begin{lstlisting}
def map(array, fn):
    r = array.copy()
    for i in range(len(array)):
        r[i] = fn(r[i])
    return r

def double_it(x):
    return x * 2

map([0, 1, 2, 3, 4], double_it)
# -> [0, 2, 4, 6, 8]
\end{lstlisting}

\subsubsection{Declarative and logic programming}

Declarative programming focuses on describing \textit{what} the program should accomplish rather than specifying \textit{how} to achieve it.

\paragraph{Logic Programming}
\begin{itemize}
    \item In logic programming, the programmer defines the desired results by specifying constraints and relationships, not the steps to solve the problem.
    \item An example is using SAT solvers to determine the satisfiability of logical formulas.
\end{itemize}


\paragraph{Example SAT formulas:}

\begin{lstlisting}
x1 and (not x2 or x3) and (not x3)
\end{lstlisting}
This SAT formula describes a logical constraint. The solver's role is to find a solution that satisfies this constraint, without the programmer detailing the steps to find such a solution.




















\newpage
\section{Portability, Dependencies and Packaging}

\subsection{Appliaction Binary Interface (ABI)}

\begin{itemize}
    \item most Windows laptops, Linux laptops and pre-M1 Macs share the same ISA: \texttt{x86\_64}
    \item iPhones, Android phones, M1 and M2 Macs share the same ISA: \texttt{AArch64}
\end{itemize}

\textbf{Q:} Why, then, do applications need to be recompiled separately for each platform?\\
e.g., iPhone vs. Android phone

\textbf{A:} Because platforms have different OSs and ABIs.
\subsubsection{What is an ABI?}

An application binary interfaces (ABI) defines:
\begin{itemize}
    \item file format for
    \begin{itemize}
        \item object files
        \item dynamically-linked files (shared objects / dll)
        \item and executable files
    \end{itemize}
    \item convention for function calls
    \item convention for system calls
\end{itemize}

It is called binary because it is independent of the language in which applications are written (i.e., it is related to the machine code, not to the source code)

\paragraph{ABI: function calls (x86\_64)}

The Application Binary Interface (ABI) for function calls within the x86\_64 architecture is exemplified by a simple C program that makes a call to the \texttt{puts} function. The assembly code generated by the \texttt{clang} compiler on Linux and the Microsoft Visual C++ (MSVC) compiler on Windows shows the differences in calling conventions and assembly syntax.

For \textbf{clang/Linux/x86\_64}, the assembly instructions prepare the function argument by loading the address of the string into \texttt{rdi}, which is the register for the first argument as per the calling convention. It then calls \texttt{puts} and exits the program with a return value of 0.

For \textbf{MSVC/Windows/x86\_64}, the assembly code shows the Windows calling convention where the string address is placed into the \texttt{rcx} register, and then the \texttt{puts} function is called. After the function call, the stack is cleaned up and the program returns.

\paragraph{ABI: function calls (AArch64)}

In the case of the AArch64 architecture, the C program remains the same, but the assembly output changes significantly to reflect the different register and calling convention.

For \textbf{clang/MacOS/AArch64}, the assembly output uses the \texttt{stp} instruction to store pair of registers and the \texttt{adrp} and \texttt{add} instructions to form the address of the string. The first argument is moved into the \texttt{x0} register before the \texttt{puts} call is made.

For \textbf{MSVC/Windows/AArch64}, similar to the clang/MacOS assembly, we see the use of \texttt{stp} for storing register pairs and \texttt{adrp} plus \texttt{add} to handle the string address. The \texttt{puts} function is then called with the argument in \texttt{x0}, followed by the function's exit sequence.

\texttt{Try it for yourself: godbolt.org}

\subsection{Portable Code}

How do we ship code that work across all platforms?


\subsubsection{Option 1: interpreters}
\begin{itemize}
  \item use interpreted languages, ship source
  \begin{itemize}
    \item Python, Javascript, \ldots
  \end{itemize}
  \item languages that compile to virtual machine code
  \begin{itemize}
    \item ship VM code
    \item optionally, ship VM interpreter
    \item Java, C\#
  \end{itemize}
\end{itemize}

\subsubsection{Option 2: multiple compilations}
\begin{itemize}
  \item compile one executable on each platform
  \item in some cases, cross-compilation is possible
  \begin{itemize}
    \item MacOS $\rightarrow$ iOS
    \item Linux $\rightarrow$ Android
  \end{itemize}
\end{itemize}

\paragraph{What if we cannot (or do not want to) recompile?}


\subsubsection{Option 3: Translation}
\textbf{Use case:} same OS, different ISA
\begin{itemize}
    \item Translation is a form of compilation
    \item From machine code
    \item To machine code (of a different ISA)
\end{itemize}
\textit{Example: Apple Rosetta 2 translates \texttt{x86\_64} into \texttt{AArch64}}

\subsubsection{Option 4: Compatibility layers}
\textbf{Use case:} different OSs, same ISA
\begin{itemize}
    \item add OS support for a foreign ABI
    \begin{itemize}
        \item foreign file formats (for objects, DLLs and executables)
        \item foreign convention for system calls
    \end{itemize}
    \item add libraries for foreign ABI
    \begin{itemize}
        \item foreign convention for function calls
    \end{itemize}
    \item Examples:
    \begin{itemize}
        \item Wine allows running Windows apps on Linux.
        \item WSLv1 allows running Linux apps on Windows.
    \end{itemize}
\end{itemize}

\subsubsection{Option 5: emulation}
\begin{itemize}
    \item An emulator acts as an interpreter for machine code, simulating another hardware environment entirely.
    \item It is generally slower than native code execution due to the overhead of interpretation.
    \item Just-In-Time (JIT) compilation can be used within emulators to improve performance by compiling bytecode to native code at runtime.
    \item Emulators can run entire operating systems, effectively creating a virtual machine.
\end{itemize}

\subsubsection{Option 6: virtualization}

\begin{itemize}
    \item Virtualization is a technique that allows multiple operating systems (guest OS) to run on the same physical hardware simultaneously. It is essentially hardware-assisted emulation.
    \item Virtualized environments must operate under the same Instruction Set Architecture (ISA) as the host hardware.
    \item The core component of virtualization is the \textbf{hypervisor}, which is responsible for managing the guest operating systems. There are two types of hypervisors:
    \begin{itemize}
        \item Type 1: Runs directly on the host's hardware (e.g., Xen, KVM).
        \item Type 2: Runs as a software layer on top of the host OS (e.g., VMware, VirtualBox, Parallels).
    \end{itemize}
    \item Virtualization ensures security by controlling all hardware access through the hypervisor, which presents a virtual interface to each guest OS.
    \item It enables cloud computing platforms such as Amazon Web Services (using Xen) and Google Cloud Platform (using KVM) to provide virtual machines that customers can rent and use remotely.
\end{itemize}


Virtualization is mainly deals with security:

Let \textbf{guest OSs} believe they have direct access to hardware...

... but every hardware access is tightly controlled by the \textbf{hypervisor}.

\subsubsection{Option 7: containers}
\textbf{Use case:} Same ISA, same kernel, different OS.

\begin{itemize}
    \item Containers provide a lightweight alternative to full virtualization and are optimal for isolating applications rather than entire operating systems.
    \item They share the same kernel of the host OS but can run different distributions or versions of Linux, thanks to the separation of filesystems, libraries, and applications.
    \item Examples of container use include:
    \begin{itemize}
        \item Running a Debian Linux container on a Fedora Linux host.
        \item Running a Debian 11 Linux container on a Debian 12 host.
        \item Running a Debian 12 container with specific libraries installed, on a Debian 12 host.
    \end{itemize}
    \item Containers are highly portable and efficient, allowing for quick deployment and scaling.
\end{itemize}


\subsection{Application Programming Interfaces (API)}

\subsubsection{Definition}
An API defines how a library (or any other service) is to be used.


\subsubsection{Library API}

\verb|FILE *fopen(const char *path, const char *mode);|

\verb|open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closedfd=True, opener=None)|

\subsubsection{Web API}

\verb|GET https://www.google.com/search?q=<query>|

\paragraph{Example:}

\verb|google-chrome https://www.google.com/search?q=Software%20Engineering|

\verb|GET https://cloudflare.com/cdn-cgi/trace|

\paragraph{Example:}

\verb|curl -s "https://cloudflare.com/cdn-cgi/trace"|
\texttt{PUT https://api.cloudflare.com/client/v4/zones/\{zone\_identifier\}/dns\_records/\{identifier\}}

\begin{lstlisting}
curl --request PUT \
     --url https://api.cloudflare.com/client/v4/zones/zone_identifier/dns_records/identifier \
     --header 'Content-Type: application/json' \
     --header 'X-Auth-Email: ' \
     --data '{
       "content": "198.51.100.4",
       "name": "example.com",
       "proxied": false,
       "type": "A",
       "comment": "Domain verification record",
       "tags": [
         "owner: dns-team"
       ],
       "ttl": 3600
     }'
\end{lstlisting}
\subsubsection{APIs and portability}
\begin{itemize}
  \item many APIs are cross-platform
  \begin{itemize}
    \item C standard library
    \item Almost all Python modules
    \item Qt, Electron, Flutter, \ldots (frameworks for GUI applications)
    \item WEB APIs only depend on an internet connection
  \end{itemize}
  \item some are specific to a platform
  \begin{itemize}
    \item Windows UI Library, MacOS Cocoa
  \end{itemize}
\end{itemize}
\subsection{Dependencies}

\renewcommand{\labelitemii}{$\star$}
\begin{itemize}
  \item your code requires libA version $ \geq 1.1$, lib B version $ \geq 4.5$
  \begin{itemize}
    \item lib B version 4.5 requires libX version 2.0 and libA version 0.8
    \item lib B version 4.7 requires libX version 2.0 and libA version 1.1
    \item lib B version 4.6 requires libX version 2.0 and libA version 2.0
    \item lib X version 2.0 requires libA version $ \leq 1.9$
  \end{itemize}
  How do we install all this? \\
  Which version do we install?
\end{itemize}

\subsubsection{Package managers}

Package managers solve this problem for you. \\
They can solve it...

\begin{itemize}
  \item at the OS level:
  \begin{itemize}
    \item MacOS: \texttt{brew install <package>}
    \item Debian/Ubuntu Linux: \texttt{apt-get install <package>}
    \item Fedora/Suse Linux: \texttt{dnf install <package>}
  \end{itemize}
  \item at the language level:
  \begin{itemize}
    \item Python: \texttt{pip install <module>}
    \item JavaScript/Node: \texttt{npm install <package>}
  \end{itemize}
\end{itemize}

\subsubsection{Limitations}

\begin{itemize}
  \item package selection may be limited (packaging is labor-intensive)
  \item security and trust
\end{itemize}


\subsubsection{Tutorial: Treating Integers as Strings of Bits in python}
\begin{itemize}
    \item \textbf{OR Operation:} The bitwise OR (\texttt{\textbar}) combines bits such that the bit in the result is 1 if either bit is 1.
    \begin{lstlisting}
    >>> x = 0b110000 | 0b000011
    >>> f"{x:06b}"  # '110011'
    \end{lstlisting}

    \item \textbf{AND Operation:} The bitwise AND (\texttt{\&}) combines bits with a result of 1 only if both bits are 1.
    \begin{lstlisting}
    >>> x = 0b111100 & 0b001111
    >>> f"{x:06b}"  # '001100'
    \end{lstlisting}

    \item \textbf{XOR Operation:} The bitwise XOR (\texttt{\textasciicircum}) combines bits with a result of 1 only if the bits are different.
    \begin{lstlisting}
    >>> x = 0b101010 ^ 0b110011
    >>> f"{x:06b}"  # '011001'
    \end{lstlisting}

    \item \textbf{Left Shift Operation:} Shifts bits to the left, filling in with zeros (\texttt{<<}).
    \begin{lstlisting}
    >>> x = 0b000110 << 2
    >>> f"{x:06b}"  # '011000'
    \end{lstlisting}

    \item \textbf{Right Shift Operation:} Shifts bits to the right, discarding excess bits (\texttt{>>}).
    \begin{lstlisting}
    >>> x = 0b011000 >> 2
    >>> f"{x:06b}"  # '000110'
    \end{lstlisting}
\end{itemize}




















\newpage
\section{Python formatted strings, list comprehensions}

\subsection{Python formatted string literals}

Formatted string literals look like string literals, but are prepended with an \texttt{f}:\\

\texttt{f'Hello'}\\

They allow  to:
\begin{itemize}
    \item build a string from python expressions
    \item specify how to format those expressions
\end{itemize}

Syntax:
\texttt{f'raw\_string \{python\_expression : format\} ...'}

\begin{itemize}
    \item \texttt{raw\_string}: anything not between \texttt{"\{\}"} is a raw string literal
    \item between \texttt{"\{\}"} we specify a formatted expression
    \begin{itemize}
        \item \texttt{python\_expression}: most python expressions are allowed here must not be ambiguous, e.g. no \texttt{". "}
        \item \texttt{format}: specifies how to convert the corresponding expression to a string
    \end{itemize}
    \item the f-string is \textit{immediately} evaluated into a string:
\end{itemize}

\begin{lstlisting}
>>> x = 3
>>> f'Hello {x}'
'Hello 3'
>>> type(f'Hello {x}')
<class 'str'>
\end{lstlisting}

Format:
\begin{itemize}
    \item \texttt{<} for left align, \texttt{>} for right-align
    \item \texttt{+} include sign even for positive numbers
    \item \texttt{(space)} empty space for positive numbers
    \item \texttt{b} binary, \texttt{d} decimal (default), \texttt{x} hexadecimal
    \item \texttt{f} floating point number (fixed-point notation)
    \item \texttt{20} (or other number) specify the minimum width of the conversion result
    \item \texttt{.10} (or other number) specify the number of digits after the decimal dot
\end{itemize}

Examples:
\begin{lstlisting}

>>> f'pi = {math.pi:+6.2f}'
'pi = +3.14'
>>> f'pi = {math.pi:<+6.2f}'
'pi = +3.14 '
\end{lstlisting}



\paragraph{Documentation:}

\begin{itemize}
    \item f-strings
    \item format specification
\end{itemize}

\subsection{String methods}

\begin{itemize}
    \item \texttt{str.find(substr)}: Return the lowest index in the \texttt{str} where substring \texttt{substr} is found. Return -1 if \texttt{substr} is not found.
    \item \texttt{str.replace(old, new)}: Return a copy of \texttt{str} with all occurrences of substring \texttt{old} replaced by \texttt{new}.
    \item \texttt{str.strip(chars)}: Return a copy of the string with the leading and trailing characters removed. The \texttt{chars} argument is a string specifying the set of characters to be removed. If omitted or None, the \texttt{chars} argument defaults to removing whitespace.
\end{itemize}



% Start of the LaTeX transcription of lecture material

\begin{itemize}
    \item \texttt{str.split(sep=None, maxsplit=-1)}: Return a list of the words in the string, using \texttt{sep} as the delimiter string. If \texttt{sep} is None, runs of consecutive whitespace are regarded as a single separator (the result will contain no empty strings). If \texttt{maxsplit} is given, at most \texttt{maxsplit} splits are done.
    \item \texttt{str.join(iterable)}: Return the concatenation of the strings in \texttt{iterable}. The separator between elements is \texttt{str}.
\end{itemize}

% The arrow indicating "String methods" represents a transition or
% continuation to another part of the presentation. In LaTeX,
% this is not something we can represent with text, but the following
% comment indicates that there was such a transition.
% > string methods

% The phrase "CONDITIONAL EXPRESSIONS" in a large font size and centered
% could be represented in LaTeX as a section or a plain centered text
% depending on the context. Here, it is shown as a plain text.
\subsection{Conditional Expressions}

% End of the LaTeX transcription of lecture material


% Content transcribed from image into LaTeX
% When compiling, you may choose to use packages like listings for proper formatting of code

% Define listing style for Python code
\lstdefinestyle{mystyle}{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=tb,
    rulecolor=\color{black},
}

\lstset{style=mystyle}


% Section: Syntax
Syntax:

\begin{lstlisting}
x if C else y
\end{lstlisting}

% Section: Example
Example:

\begin{lstlisting}
>>> x = 5
>>> 'big' if x > 10 else 'small'
'small'
\end{lstlisting}

% Section: Information about evaluation
\paragraph{Only the appropriate value is evaluated}

\begin{lstlisting}
>>> 1 / 0
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ZeroDivisionError: division by zero
\end{lstlisting}

\paragraph{Only the appropriate value is evaluated (example)}

\begin{lstlisting}
>>> import math
>>> x = 4
>>> 1 / x if x != 0 else math.inf
0.25

>>> x = 0
>>> 1 / x if x != 0 else math.inf
inf
\end{lstlisting}

\textgreater{} documentation

\subsection{List comprehensions}

\subsubsection{List comprehension syntax:}
The general syntax for a list comprehension in Python is:
\begin{center}
\texttt{[ expression for variable in iterable ]}
\end{center}
\begin{itemize}
  \item iterates over the values in \texttt{iterable}
  \item at each iteration, assign value to \texttt{variable}
  \item evaluate \texttt{expression} (may refer to \texttt{variable})
  \item result becomes one list entry
\end{itemize}

\paragraph{Examples}

\begin{lstlisting}

>>> [i * 2 for i in range(8)]
[0, 2, 4, 6, 8, 10, 12, 14]

>>> [str(i) for i in range(8)]
['0', '1', '2', '3', '4', '5', '6', '7']

>>> [f'{i:02d}' for i in range(8)]|
['00', '01', '02', '03', '04', '05', '06', '07']|

>>> [f'{i:03b}' for i in range(8)]|
['000', '001', '010', '011', '100', '101', '110', '111']|
\end{lstlisting}


\subsection{Dictionary Comprehensions}

% Explanation and syntax for dictionary comprehensions
Dictionary comprehension syntax:\\
\verb|{key_expr : val_expr for variable in iterable}|

Same as list comprehension, but for dict.

% Example of dictionary comprehension
Example:\\
\verb|>>> { i : f'{i:02b}' for i in range(4) }|
\verb|{0: '00', 1: '01', 2: '10', 3: '11'}|



\subsection{Generator Expressions}

Generator expression syntax: \\
(same as list comprehension, but with parentheses)

% Generator expression format
\begin{equation*}
(\text{expression} \quad \textbf{for} \quad \text{variable} \quad \textbf{in} \quad \text{iterable} )
\end{equation*}

Same as list comprehension, but creates a generator (iterable)

Example:

\begin{lstlisting}
>>> a = (i * 2 for i in range(2 ** 28))
>>> a
<generator object <genexpr> at 0x7fa9195c5d80>
>>> sum(a)
72057593769492480
\end{lstlisting}


Note: there is no tuple comprehension

\paragraph{Documentation}
\begin{itemize}
    \item list comprehensions
    \item dict comprehensions
    \item generator expressions
\end{itemize}



















\newpage
\section{Regular Expressions}
\subsection{Definition}
Regular expressions are a mini-language for text pattern matching.
\paragraph{Example}

\textbf{Q}: Find all occurrences of the word ``memory'' in the files in this directory.

\begin{lstlisting}
grep 'memory' *
\end{lstlisting}

\subsection{Matching}

\subsubsection{The grep command}

\begin{lstlisting}
grep [OPTION...] PATTERNS [FILE...]
\end{lstlisting}

\paragraph{Options:}
\begin{itemize}
    \item \texttt{-E}: ``extended'' regular expressions (we will use this syntax)
    \item \texttt{-R}: recursive (if a directory is given, look all files in it, incl. subdirectories)
    \item \texttt{-i}: case insensitive (a same as A)
\end{itemize}

\paragraph{Patterns:}
Use single-quotes (\texttt{' '}) to avoid shell interference

\paragraph{Files}
If no file provided, grep reads its (piped) input
\subsubsection{Piping to grep}

Q: Find all files in the current directory whose name contains the letter L

\begin{lstlisting}
ls | grep -E -i 'L'
\end{lstlisting}

\subsubsection{Introduction to regular expressions}

\begin{itemize}
  \item by default, patterns are looked for line-by-line
  \item strings of “normal” characters are matched
\end{itemize}

\begin{lstlisting}
grep -E 'memory' *
\end{lstlisting}

\subsubsection{Anchors}

\begin{itemize}
  \item the \^{} character at the beginning of a regex matches the beginning of a line
  \item the \$ character at the end of a regex matches the end of a line
\end{itemize}

Examples:

\begin{lstlisting}
grep -E '^int' *
grep -E 's$' *
\end{lstlisting}

\subsubsection{Repetitions}

\begin{itemize}
    \item \texttt{?} indicates that the previous character may or may not occur (once)
    \item \texttt{*} indicates that the previous character may occur zero or multiple times
    \item \texttt{+} indicates that the previous character may occur one or more times
    \item \texttt{\{4\}} indicates that the previous character must occur 4 times
    \item \texttt{\{4,\}} indicates that the previous character must occur 4 or more times
    \item \texttt{\{4,8\}} indicates that the previous character must occur between 4 and 8 times
\end{itemize}

\noindent Examples:

\begin{lstlisting}
grep -E 's?printf' *
grep -E '^ *printf' *
grep -E '0b+0' *
grep -E 'e{2,}' *
\end{lstlisting}

\subsubsection{Grouping}

Any part of a regex can be grouped using parentheses.
Repetitions then apply to the group instead of a single character.

\noindent Examples:

\begin{lstlisting}
grep -E '(Abc)+'    # matches 'Abc', 'AbcAbc', 'AbcAbcAbc', ...
\end{lstlisting}

\subsubsection{Match any character}

The dot (\texttt{.}) matches any character:

\noindent Examples:

\begin{lstlisting}
grep -E 'X.Y'   # matches 'XaY', 'XbY', 'X+Y', ...
grep -E 'X.*y'  # matches 'XabcY', 'X-*-y', ...
\end{lstlisting}

\subsubsection{Bracket expressions}

\begin{itemize}
    \item One character can be matched to multiple options using square brackets:
    \begin{lstlisting}
    grep -E '[abc]XY'  # matches aXY or bXY or cXY
    grep -E '[01]+'    # matches binary numbers
    \end{lstlisting}

    \item We can express ranges of characters using a dash:
    \begin{lstlisting}
    grep -E '[0123456789]+'     # matches decimal numbers
    grep -E '[0-9]+'            # equivalent
    grep -E '[0-9a-fA-F]+'      # matches hexadecimal numbers
    grep -E '[A-Z][a-z]*'       # matches words starting with capital letter
    \end{lstlisting}

    \item Bracket expressions are negated if the first character is \textasciicircum:
    \begin{lstlisting}
    grep -E '[^A]sprintf'    # matches "printf", "sprintf" but not "sprintf"
    \end{lstlisting}
\end{itemize}

\subsubsection{Disjunctions}

Multiple options can be given using the ``|'' character:

\begin{lstlisting}
grep -E 'system_(startup|shutdown)' 
# matches "system_startup" or "system_shutdown"
\end{lstlisting}

\subsubsection{Special characters}

Special characters can be “escaped” using a backslash (“\textbackslash”):

\begin{lstlisting}
grep -E 'printf\(.*\)'  # matches "printf("Hello %s", name)"
\end{lstlisting}

\subsubsection{Using regular expressions in less}

Searching for patterns in the \texttt{less} pager is performed by typing ``/''. 

Patterns are specified using regular expressions

\subsection{Search and Replace: \texttt{sed}}

\texttt{sed [OPTION...] SCRIPT [FILE...]}

\begin{itemize}
    \item Options:
    \begin{itemize}
        \item \texttt{-E}: ``extended'' regular expressions (we will use this syntax)
        \item \texttt{-i}: edit file in-place (instead of printing)
    \end{itemize}
    \item Script: Use single-quotes (\texttt{` ' `}) to avoid shell interference
    \item Files: if no file provided, \texttt{sed} reads its (piped) input
\end{itemize}

\subsubsection{Basic search and replace}

\begin{lstlisting}
sed -E 's/REGEX/REPLACEMENT/'
\end{lstlisting}

\begin{itemize}
  \item Examples:
  \begin{lstlisting}
  sed -E 's/python/Python/'    # replace "python" with "Python"
  sed -E 's/printf\(/\fprintf\(stderr, /' 
  # replace "printf(" with "fprintf(stderr, "
  \end{lstlisting}
  
  \item Allow multiple replacements per line:
  \begin{lstlisting}
  sed -E 's/REGEX/REPLACEMENT/g'   # g stands for global
  \end{lstlisting}
  
  \item Use delimiter different from ``/'':
  \begin{lstlisting}
  sed -E 's|REGEX|REPLACEMENT|'
  sed -E 's_REGEX_REPLACEMENT_'
  \end{lstlisting}
\end{itemize}

\subsubsection{Advanced search and replace}

\begin{itemize}
  \item In the replacement string, \textbackslash1 indicate the first parenthesized group, \textbackslash2 the second, etc.:
  \begin{lstlisting}
  # replace "Hello, World!" with "Bye, World!"
  sed -E 's/Hello, ([A-Za-z]*\!)/Bye, \1!/'
  \end{lstlisting}
  
  \item Groups are numbered in the order of the opening parentheses from the left:
  \begin{lstlisting}
  sed -E 's/((a(b|z)+)(c+))/\1\3/g'
  #         ^  ^    ^  ^
  #         1  2    3  4
  \end{lstlisting}
\end{itemize}
\subsection{Regular Expressions in Programming Languages}
\subsubsection{Using regular expressions in C}
\begin{lstlisting}[language=C]
#include <stdio.h>
#include <regex.h>

int main()
{
    regex_t re;
    
    // REG_EXTENDED: POSIX extended regular expression
    // REG_NOSUB: do not report position of matches
    if (regcomp(&re, "0[xX][0-9a-fA-F]+", REG_EXTENDED | REG_NOSUB)) {
        fprintf(stderr, "Failed to compile regex\n");
        return 1;
    }

    int r = regexec(&re, "Does this contain a hex number, like 0xff ?", 0, NULL, 0);

    if (r == 0) {
        printf("Found\n");
    } else if (r == REG_NOMATCH) {
        printf("Not found\n");
    }
    
    regfree(&re);

    return 0;
}
\end{lstlisting}
See: \texttt{man regex}

\subsubsection{Using regular expressions in Python}
\begin{lstlisting}
>>> import re
>>> m = re.search(r'0[xX][0-9a-fA-F]+', 'Does this contain a hex number, like 0xff ?')
>>> m.group(0)
'0xff'
\end{lstlisting}

\textgreater{} \href{https://docs.python.org/3/library/re.html}{documentation}






















\newpage
\section{Version Control Systems}

\subsection{Basic Version Control Commands}
\begin{lstlisting}
    Assume you have a project:
    myproject.py
    
    To try a modification, but unsure of its success:
    mkdir versions/
    mkdir versions/v1/
    cp myproject.py versions/v1/
    
    After modifications:
    - If the modification is good:
      mkdir versions/v2/
      cp myproject.py versions/v2/
    
    - To revert to the old version:
      cp versions/v1/myproject.py myproject.py
\end{lstlisting}

\subsubsection{Use Cases for Version Control}
\begin{itemize}
    \item Experiment with changes (try things).
    \item Determine when and how bugs were introduced.
    \item Coordinate work among multiple collaborators on a project.
\end{itemize}

\subsubsection{Version Control Systems (VCS) / Source Code Management (SCM)}
\begin{itemize}
    \item Revision Control System (RCS), 1982: Early system, operated on single files.
    \item Concurrent Versions System (CVS), 1986: Introduced the concept of repositories.
    \item Apache Subversion ("SVN"), 2000: Centralized VCS, allowed versioning of directories.
    \item Mercurial ("Hg"), 2005 and \textbf{Git, 2005}:
    \begin{itemize}
        \item Distributed VCS, enabling workflows with local repositories.
        \item Git has spawned a large hosting industry (e.g., GitHub, GitLab).
        \item Both are used internally at major tech companies.
    \end{itemize}
    \item \textbf{Piper}: Google's internal monorepo VCS, not publicly available.
\end{itemize}


\subsubsection{Git fundamentals}
A \textbf{repository} stores the complete history of a project.
\begin{itemize}
    \item It is typically contained in the \texttt{.git/} directory at the root of the project.
\end{itemize}

\paragraph{Commit}
A \textbf{commit} is a unit of change capturing:
\begin{itemize}
    \item A snapshot of all the project files.
    \item Metadata such as author, date, and the \textbf{parent commit}.
\end{itemize}
Commits are uniquely identified by a \textbf{hash}.

\subsubsection{Hashes}
\begin{itemize}
    \item A \textbf{hash} function maps any sequence of bits to a fixed-length bit string.
    \item The map is \textit{surjective} but Git assumes it to be \textit{bijective} for practical purposes.
    \item Git uses \textbf{SHA-1} for hashing: 160 bits / 20 bytes / 40 hexadecimal digits.
    \begin{itemize}
        \item Example hash: \texttt{1e6cac37c5c8c5ee99ec104954d09b07e96116ba}
    \end{itemize}
    \item Git is migrating to \textbf{SHA-256}: 256 bits / 32 bytes / 64 hexadecimal digits.
\end{itemize}

\subsubsection{Hashes in Git}
\begin{itemize}
    \item Commits are designated by SHA-1 hashes.
    \item A short hash prefix can be used to refer to commits if it is unambiguous.
\end{itemize}

\subsection{Building a Commit}
\subsubsection{Working Tree}
\begin{itemize}
    \item The \textbf{working tree} consists of files that are currently being worked on.
    \item We never interact with the \texttt{.git/} directory contents directly.
    \item The working tree reflects the current state of files, excluding the \texttt{.git/} directory.
    \item We can use the \texttt{git checkout} command to update the working tree to match a specific commit.
\end{itemize}

\subsubsection{Staging Area}
\begin{itemize}
    \item Before committing, we \textbf{stage} changes, specifying which modifications should be included in the next commit.
    \item The staging area is a snapshot of what our next commit will look like.
\end{itemize}

\subsubsection{Commit}
\begin{itemize}
    \item After staging, we \textbf{commit} the changes to the repository's history.
    \item A commit includes a snapshot of the staged changes and a commit message describing the changes.
\end{itemize}

\paragraph{Staging and Committing Example}
\begin{lstlisting}[language=bash]
# Creating or modifying files
new_file_A.py
new_file_B.py
new_file_C.py

# Staging files
git add new_file_A.py new_file_B.py

# Committing to the repository
git commit -m "My first commit."
\end{lstlisting}

\subsubsection{Listing Past Commits}
\begin{lstlisting}[language=bash]
git log
\end{lstlisting}
\begin{lstlisting}
commit 6ea8433f89c78588190435c877b1d3e7c708 (HEAD -> main)
Author: Laurent Perriraz <lperriraz@dev>
Date:   Fri Sep 29 02:44:18 2023 +0200

    My first commit.
\end{lstlisting}

\subsubsection{Automatic Adding}
Add multiple files at once using a pattern:
\begin{lstlisting}
git add *.py
\end{lstlisting}
    
Add all files in the working tree:
\begin{lstlisting}
git add -A
\end{lstlisting}
    
Exclude files from automatic adding using \texttt{.gitignore}.

\subsubsection{Observing the State}
\begin{lstlisting}
git status
\end{lstlisting}

\subsubsection{Showing Differences}
\begin{lstlisting}
git diff
\end{lstlisting}

\subsubsection{Staging Changes}
\begin{lstlisting}
git add -A
git status
git diff
git diff --staged
\end{lstlisting}

\subsubsection{Committing Changes}
\begin{lstlisting}
git commit -m "My second commit."
git log
\end{lstlisting}

\subsubsection{Checking Out Commits}
\begin{lstlisting}
git checkout <commit-hash>
git log
git log --all
\end{lstlisting}

\subsubsection{Working with Commits}
\begin{itemize}
    \item Modify files in the working tree and stage the changes.
    \item Commit the staged changes with a descriptive message.
    \item Use \texttt{git log} to see the history of commits.
    \item Check out a specific commit to view or revert to that snapshot.
\end{itemize}


\subsection{Branches}

\subsubsection{Commit Structure}
Commits in Git form a linked list structure, with each commit pointing back to its parent.

\subsubsection{Checking Out Commits}
You can switch between commits using:
\begin{lstlisting}
git checkout <commit-hash>
\end{lstlisting}

\subsubsection{Problem with Detached HEAD}
If you check out a commit directly, you enter a 'detached HEAD' state where changes are not on any branch.

\subsubsection{Creating Branches}
To avoid a detached HEAD, create a new branch:
\begin{lstlisting}
git branch <branch-name>
\end{lstlisting}
Branches allow to work on features or bug fixes while keeping the main branch stable.

\subsubsection{Switching Branches}
To switch to a different branch:
\begin{lstlisting}
git checkout <branch-name>
\end{lstlisting}

\subsubsection{Visualizing Branches}
You can visualize the commit history and branches using:
\begin{lstlisting}
git log --all --graph
\end{lstlisting}

\paragraph{Branching Example}
\begin{itemize}
    \item Initial state after two commits:
    \begin{lstlisting}
    commit 31a051 (HEAD -> main)
    commit 6ea843
    \end{lstlisting}

    \item Create a new branch and switch to it:
    \begin{lstlisting}
    git branch my_branch
    git checkout my_branch
    \end{lstlisting}
    
    \item The branch points to the current commit.
\end{itemize}

\subsubsection{Merging}
Merging incorporates changes from one branch into another.
\begin{lstlisting}
git merge <branch-name>
\end{lstlisting}

\subsubsection{Merge Conflicts}
Conflicts occur when the same lines are changed in both branches. They need to be resolved manually.

\subsubsection{Rebasing}
Rebasing is an alternative to merging, replaying commits from one branch onto another.
\begin{lstlisting}
git rebase <base-branch-name>
\end{lstlisting}

\subsubsection{Handling Merge Conflicts}
Resolve conflicts by editing files, then stage and commit the resolution.
\begin{lstlisting}
# Edit files to resolve conflicts
git add <file>
git commit
\end{lstlisting}


\paragraph{Important Points}
\begin{itemize}
    \item Always commit changes before switching branches.
    \item Use merging or rebasing as per the team's workflow conventions.
    \item Resolve conflicts carefully to maintain code integrity.
\end{itemize}

\subsection{Remotes}

\subsubsection{Sharing Commits}
Git is distributed and allows multiple remotes. There's no central server.

\paragraph{Fetching Commits}
To download commits from a remote repository:
\begin{lstlisting}
git fetch <URL>
\end{lstlisting}
The URL must be accessible, either public or with appropriate credentials.

\paragraph{Cloning Repositories}
If the repository was initially cloned:
\begin{lstlisting}
git clone <URL>
\end{lstlisting}
Use `git fetch` without a URL to fetch from the original source.

\paragraph{Format Patch}
To save commits as files for email, use:
\begin{lstlisting}
git format-patch
\end{lstlisting}

\subsubsection{Best Practices}
\begin{itemize}
    \item Regularly push to and pull from remotes to keep local and remote repositories synchronized.
    \item Use `git fetch` to update local references with remote repository.
    \item Use `git pull` to fetch and merge changes from the remote server to your working directory.
    \item Use `git push` to update the remote repository with your local changes.
\end{itemize}

















\newpage
\section{Software Licenses}

\subsection{Closed-source software}
Most end-user software is closed-source (proprietary).

\begin{itemize}
    \item The executable is distributed to customers.
    \item The source code is either
    \begin{itemize}
        \item never revealed (most commonly), or
        \item only made available to select customers (rarely).
    \end{itemize}
\end{itemize}
\paragraph{Example of closed-source software}

\begin{itemize}
    \item Operating systems: Microsoft Windows; Android, iOS, MacOS (except kernel)
    \item Office suites: Microsoft 365, iWork
    \item Creative software: Adobe suite, Autodesk suite, Final Cut Pro, Pro Tools, Logic Pro
    \item Development software: Visual Studio, XCode (except compiler)
    \item Collaboration software: Zoom, Teams, Skype, Slack, Discord
    \item Server-side and enterprise software: Microsoft IIS, SAP, OpenAI GPT-4
    \item Almost all videogames
    \item Almost all mobile apps
\end{itemize}
\subsection{Free software}
\begin{itemize}
    \item ``free'' as in freedom (not ``free lunch'')
    \item defined by the Free Software Foundation (FSF, est. 1985)
    \item software attached with a \textit{license} (uses copyright law)
    \item gives \textit{freedoms} (rights) to the \textit{user}, to:
    \begin{itemize}
        \item run the software as they wish
        \item study and modify the software as they wish
        \item redistribute (original and modified versions)
    \end{itemize}
    \item based on the philosophy that all \textit{software should be free} to protect users
    \item the ``GNU'' project is FSF's software collection
\end{itemize}
\subsection{Open-source software}
\begin{itemize}
    \item defined by the Open Source Initiative (OSI, est. 1998)
    \item software attached with a \textit{license} (uses copyright law)
    \item specifies how software can be \textit{distributed}:
    \begin{itemize}
        \item no restrictions on redistribution
        \begin{itemize}
            \item no discrimination against specific users, fields, products, other software, other technologies
        \end{itemize}
        \item source code must be available
        \item derived works must be allowed
        \item but modifications can be required to be clearly delineated
    \end{itemize}
\end{itemize}
\subsubsection{FSF vs. OSI}
\begin{itemize}
  \item \textbf{FSF:} for the user's sake, all software \textit{should be free} on ethical grounds -- free software licenses are a means to that end
  \item \textbf{OSI:} help businesses and developers publish and disseminate their open-source software -- pragmatically, we do not want to add hurdles if they impair practical use
\end{itemize}

\paragraph{In practice?}
The FSF and OSI each maintain a list of ``approved'' license.

Most FSF-approved \textit{free software} licenses are also OSI-approved \textit{open-source} licenses. And vice-versa.

The difference lies in the licenses each organization \textit{promotes}.

\paragraph{The ``baseline'' FSF license}
The GNU General Public License (GPL):
\begin{itemize}
  \item any user who receives the executable must be provided the source code as well upon request
  \item any derivative work is automatically covered by the GPL (the GPL is ``viral'')
  \item dynamic linking with GPL software counts as derivative work
\end{itemize}


\paragraph{Amended FSF licenses}

\begin{itemize}
  \item The ``more permissive'' \textbf{GNU Lesser General Public License (LGPL)}:
  \begin{itemize}
    \item adds exception to allow dynamic linking with non-GPL software
  \end{itemize}
  
  \item The ``more restrictive'' \textbf{GNU Affero General Public License (AGPL)}
  \begin{itemize}
    \item definition of ``user'' includes over-the-network interactions
  \end{itemize}
\end{itemize}

\subsubsection{Typical open-source licenses}

\begin{itemize}
  \item Most popular: Apache License, BSD License, MIT License
  \item ``permissive licenses'': fewer constraints on derivative work
  \begin{itemize}
    \item unmodified parts still covered by the original license
    \item but modified parts are not, can even be closed source
  \end{itemize}
  \item some require acknowledgement of the original work (authors and/or project)
  \item differences among permissive licenses are minor (but important to lawyers)
\end{itemize}

\paragraph{Example projects}

\begin{itemize}
  \item ``free software'' (GPL-type licenses)
  \begin{itemize}
    \item Linux kernel, \textbf{GPL}
    \item GNU project, \textbf{GPL}
    \item gcc, \textbf{GPL}
    \item glibc (gcc’s standard C library), \textbf{LGPL}
    \item git, \textbf{GPL}
    \item gmp, \textbf{LGPL}
  \end{itemize}
  
  \item ``open source'' (permissive licenses)
  \begin{itemize}
    \item Apache web server, \textbf{Apache}
    \item NGINX web server, \textbf{BSD}
    \item LLVM, \textbf{Apache}
    \item Chrome (more precisely: chromium), \textbf{BSD}
    \item Node.js, Angular, React, \textbf{MIT}
  \end{itemize}
\end{itemize}

\subsection{End-user cost}

Whether or not customers pay for software is orthogonal to source availability.

\begin{tabular}{>{\bfseries}l l l}
Cost & Closed-source & Free / Open-source \\
\hline
0 & TikTok, Whatsapp, Discord & Chrome, Gimp, VLC, Blender \\
\texttt{>} 0 & Photoshop, Maya, Ableton & Red Hat Enterprise Linux \\
\end{tabular}
\subsection{Commercial, non-commercial}

Whether or not developers are commercial entities is orthogonal to source availability.

\begin{tabular}{>{\bfseries}l l l}
developers & closed-source & free / open-source \\
\hline
non-commercial & (most amateur code until 2010s, some government software, legacy scientific software) & GNU system (Free software foundation), Blender, Krita, LibreOffice \\
commercial & Microsoft Windows, Microsoft 365, iWork, Adobe suite, Autodesk, \ldots & Chrome, Ubuntu, Red Hat, NGINX, Docker, GitLab, Redis, LLVM \\
\end{tabular}
*the distinction between commercial and non-commercial is often blurry

\paragraph{How can commercial software be free / open-source?}
\begin{itemize}
    \item software has zero price, sell support and services (Ubuntu, Red Hat, NGINX)
    \item software costs money, convince customers not to redistribute it (Red Hat)
    \item open-core: basic functionality is open-source, sell advanced features (NGINX)
    \item open-sourced software accesses proprietary services (Chrome)
    \item open-sourced software is not core business (LLVM)
\end{itemize}

\subsection{Use cases}

\begin{itemize}
  \item Closed-source:
  \begin{itemize}
    \item source code is your ``secret sauce''
    \item customers willing to pay
  \end{itemize}
  
  \item Open-source permissive licenses:
  \begin{itemize}
    \item encourage wider adoption
    \item encourage commercial entities to participate
  \end{itemize}
  
  \item Free software GPL-type licenses:
  \begin{itemize}
    \item protect users (ethical grounds)
    \item force downstream developers to reciprocate
  \end{itemize}
  
  \item Share source code, but do not give any right to modify (limited usefulness)
\end{itemize}

\subsection{Patents}

\noindent In most countries:
\begin{itemize}
  \item Contrary to copyright law (protects creative processes) patents are not a fundamental right
  \item Patents are a pragmatic compromise for promoting innovation.
  
  \noindent The bargain is:
  \begin{itemize}
    \item Share your innovation with the patent office (as opposed to keeping it secret)
    \item Get N-year exclusivity on commercialization
  \end{itemize}
\end{itemize}

\subsubsection{Patents and software innovation}

\begin{itemize}
  \item Software innovation is quicker: N years is like centuries
  \item Ideas are cheap, execution is everything
  \item Software is close to mathematics (discovered, not invented)
  \item Patent disclosures do not include code! They don't actually help anyone.
\end{itemize}

\subsubsection{Stuff that has been patented}

\begin{itemize}
    \item Buy with a single click (Amazon)
    \item Automatically make email addresses clickable (Apple)
    \item Fourier (1768--1830) series for compression (Fraunhofer Institute)
\end{itemize}






















\newpage
\section{Specifications}
\subsection{A note about C}

\subsubsection{Why C?}
\begin{itemize}
    \item The C language has deep flaws
    \item but the C ABI is everywhere:
    \begin{itemize}
        \item CPU and OS vendors define the ABI for C function calls
        \item OS services are typically provided via C functions:
        \begin{itemize}
            \item Win32 and WinRT (even though WinRT is C++)
            \item MacOS's Cocoa uses the Objective-C ABI (a superset of the C ABI)
            \item Linux kernel ABI
        \end{itemize}
        \item almost all other languages support calling into C code
    \end{itemize}
\end{itemize}

\subsubsection{Why the C ABI?}
The C ABI is simple:
\begin{itemize}
    \item just functions and simple types: integer, pointer, \texttt{struct}
    \item no objects or methods
    \item no exceptions
\end{itemize}
% End of the LaTeX transcription of the lecture material


% LaTeX transcription of the lecture's text content

\subsubsection{Other ABIs}

\begin{itemize}
    \item There are multiple C++ ABI specifications
    \begin{itemize}
        \item but they change over time (no ``stable'' ABI)
        \item even across versions of the same compiler
    \end{itemize}
    \item There is no Rust ABI specification
\end{itemize}

\subsection{Specifications}

% And another section break or page break
\subsubsection{What is even the C language?}

\subsubsection{Questions}
\begin{itemize}
    \item What is (and is not) valid C?
    \item Who defines the C language?
    \item What does \texttt{-std=c2x} mean?
\end{itemize}

% Slide break - LaTeX does not support 'slide' concept outside of presentations classes like Beamer,
% but the content is continued as a separate section for clarity

% "What is valid C?" slide
\subsubsection{What is valid C?}
\begin{itemize}
    \item Pragmatically, C code is valid if your compiler produces a valid executable.
    \item However, there are many compilers.
    \item It would be convenient if they agreed on a definition for the C language.
\end{itemize}

\textit{In the beginning, there was K\&R C (1978)}

\begin{itemize}
    \item 1978: Kernighan and Ritchie publish their book
    \item 1983: The American National Standards Institute (ANSI) forms a committee to standardize C
    \item 1989: The committee publishes the standard, ``ANSI C'' / ``C89''
    \item 1990: The International Organization for Standardization (ISO) adopts the standard
    \item 1999: ISO updates the standard (ANSI adopts it): ``C99''
    \item 2011: ISO update: ``C11''
    \item 2017: ISO update: ``C17''
    \item 2023: ISO working on update ``C23'', provisionally ``C2x''
\end{itemize}

Hence \verb|-std=c2x|

\subsubsection{Who defines the C language nowadays?}

\begin{itemize}
    \item A ``working group'' within ISO: ``WG14''
    \begin{itemize}
        \item Compiler writers
        \item Hardware vendor representatives
        \item OS maintainers
        \item Academics
    \end{itemize}
    \item \textgreater{} C23 draft (742 pages)
\end{itemize}

\subsection{Behaviours}

\subsubsection{Locale-Specific Behavior}

Behavior that depends on local conventions (nationality, culture, and language) that each implementation documents.

\paragraph{Example}

Whether \texttt{islower()} returns true for characters other than the 26 lowercase Latin letters.

\begin{verbatim}
int a = islower('æ');
\end{verbatim}

\subsubsection{Unspecified Behavior}

\begin{itemize}
  \item Behavior upon which this document provides two or more possibilities and imposes no further requirements on which is chosen in any instance
  \item Behavior that results from the use of an unspecified value
\end{itemize}

\paragraph{Examples}

\begin{itemize}
  \item The order in which the arguments to a function are evaluated.
  \item Value of padding bytes:
\end{itemize}

\begin{lstlisting}[language=C]
struct s {
    char a; // 1 byte
    // 3 padding bytes
    int b;  // 4 bytes
};
\end{lstlisting}


\subsubsection{Implementation-defined Behavior}

Unspecified behavior where each implementation (compiler / platform / OS) documents how the choice is made

\paragraph{Example}

The propagation of the high-order bit when a signed integer is shifted right.

\begin{lstlisting}
int a = -8;
int b = a >> 1;
\end{lstlisting}


On x86\_64 and AArch64: sign-extend

\subsection{Undefined Behavior}

Behavior, upon use of a nonportable or erroneous program construct or of erroneous data,
for which this document imposes no \textbf{requirements}.

\textbf{Possibly:}
\begin{itemize}
    \item ignoring the situation completely with unpredictable results,
    \item implementation-defined behavior
    \item compilation or execution yields error message
    \item compilation or execution crashes
    \item \textbf{anything else}
\end{itemize}

\textbf{Example}
\begin{lstlisting}
int *a = NULL;
int b = *a;
\end{lstlisting}

\subsubsection{Easy UB: division by zero}

``The result of the \texttt{/} operator is the quotient from the division of the first operand by the second;
the result of the \texttt{\%} operator is the remainder.

In both operations, if the value of the second operand is zero, the behavior is undefined.''

\begin{itemize}
    \item A simple C program that attempts to divide by zero can cause a runtime error:
    \begin{lstlisting}[language=C]
    int main(int argc) {
        return 5 / (argc - 1);
    }
    \end{lstlisting}
    When \texttt{argc} is 1, the program attempts to divide by zero, resulting in a floating-point exception.

    \item Compiling with warnings enabled can alert the developer to the potential issue:
    \begin{lstlisting}
    clang -O3 -std=c2x -o main main.c
    main.c:3:11: warning: division by zero is undefined
    \end{lstlisting}
    
    \item The actual result of running the compiled program may vary, producing different outputs or crashing:
    \begin{lstlisting}
    ./main
    -882586408
    1687000168
    -1071941800
    -60110776
    \end{lstlisting}
\end{itemize}

\subsubsection{Easy UB: division overflow}
Integer division in C follows specific rules, but certain edge cases, like division overflow, can result in undefined behavior.\\

\textbf{Truncation Towards Zero}
\begin{itemize}
    \item The result of the \texttt{/} operator is the algebraic quotient with any fractional part discarded, a process known as truncation toward zero.
    \item For example, both \texttt{5 / -2} and \texttt{-5 / 2} would result in \texttt{-2}.
\end{itemize}

\textbf{Representability and Equality}
\begin{itemize}
    \item If the quotient \texttt{a/b} is representable in the data type of the result, then the equation \texttt{(a/b)*b + a\%b} should equal \texttt{a}.
    \item This property is used to confirm that division and modulus operations are consistent with mathematical definitions.
\end{itemize}

\textbf{Undefined Behavior in Division}
\begin{itemize}
    \item When \texttt{a/b} produces a quotient that is not representable, such as an integer division by zero or division overflow, the behavior is undefined.
    \item Undefined behavior means the output and side effects of the code are unpredictable and not reliable.
\end{itemize}

\paragraph{Example}
Consider the following C code that demonstrates undefined behavior due to division overflow:

\begin{lstlisting}[language=C]
#include <stdio.h>
#include <limits.h>

void print_if_negative(int a) {
    if (a >= 0) return;
    printf("a = %d\n", a);
    printf("a / -1 = %d\n", a / -1);
}

int main() {
    print_if_negative(INT_MIN);
    return 0;
}
\end{lstlisting}

\subsubsection{Explanation}
\begin{itemize}
    \item The largest positive value an \texttt{int} can represent is \texttt{INT\_MAX}, and the smallest negative value is \texttt{INT\_MIN}.
    \item Dividing \texttt{INT\_MIN} by \texttt{-1} is undefined because the positive result cannot be represented as an \texttt{int} (it would be one more than \texttt{INT\_MAX}).
    \item Compilers may produce a warning, and the program may exhibit unexpected behavior when executed.
\end{itemize}


\subsubsection{Integer overflow}
Overflow occurs when an arithmetic operation exceeds the maximum size that can be represented within a given number of bits.

\subsubsection{Unsigned Integer Overflow}

\begin{itemize}
    \item Unlike signed integer overflow, unsigned integer overflow is well-defined in C.
    \item It exhibits wrap-around behavior, meaning that if the result of an operation is too large to be represented in the given unsigned type, it wraps around from the largest representable number back to zero.
\end{itemize}

\paragraph{Example of Unsigned Overflow}

The following C code demonstrates unsigned integer overflow:

\begin{lstlisting}[language=C]
#include <stdio.h>
#include <stdint.h>

int main() {
    uint8_t a;
    for (int i = 0; i < 1000; i++) {
        printf("%012" PRIu8 "\n", a);
        a = a + 1;
    }
    return 0;
}
\end{lstlisting}

\textbf{Explanation}
\begin{itemize}
    \item The \texttt{uint8\_t} type is an 8-bit unsigned integer, capable of representing values from 0 to \(2^8 - 1\) (255).
    \item When the loop increments \texttt{a} beyond 255, it wraps around to 0 due to unsigned overflow.
    \item This behavior is predictable and consistent with the C standard.
\end{itemize}

\textbf{Wrap-around Behavior}

\begin{itemize}
    \item If \texttt{i} and \texttt{j} are \(n\)-bit unsigned integers, the result of \texttt{i + j} is \((i + j) \mod 2^n\).
    \item This property ensures that any operation on unsigned \(n\)-bit integers produces the bottom \(n\) bits of the true arithmetic value.
\end{itemize}

\textbf{Hardware Implementation}

\begin{itemize}
    \item Modern architectures like x86\_64 and ARM64 adhere to this behavior in their instruction sets, handling unsigned integer overflow by wrapping around.
\end{itemize}

\subsubsection{Signed integer overflow}

While hardware architectures like x86$\_$64 and AArch64 have instructions that exhibit wrap-around behavior on overflow, the C language standard specifies that signed integer overflow is undefined behavior.

\paragraph{Example}

Consider this C code snippet:

\begin{lstlisting}[language=C]
#include <stdio.h>
#include <limits.h>

void print_if_positive(int a) {
    if (a < 0)
        return;

    printf("a = %d\n", a);
    printf("a + 1 = %d\n", a + 1);

    if (a + 1 > 0)
        printf("a + 1 = %d is positive\n", a + 1);
}

int main() {
    print_if_positive(INT_MAX);
    return 0;
}
\end{lstlisting}

\textbf{Explanation}
\begin{itemize}
    \item The code attempts to increment an integer \texttt{a} that is already at \texttt{INT\_MAX}, the largest value a signed integer can hold.
    \item Theoretically, adding 1 to \texttt{INT\_MAX} would wrap around to \texttt{INT\_MIN} due to overflow, but in C, this behavior is not guaranteed and should not be relied upon.
    \item The actual output of the program can vary depending on how the compiler chooses to handle the overflow.
\end{itemize}


\subsubsection{Invalid Pointer Dereferencing}

When a pointer in C is assigned an invalid value, dereferencing it results in undefined behavior.\\

\textbf{Standards Specification}
\begin{itemize}
    \item The C standard states: "If an invalid value has been assigned to the pointer, the behavior of the unary \texttt{*} operator is undefined."
    \item This can lead to various issues, including segmentation faults, data corruption, or other erratic behavior.
\end{itemize}

\textbf{Example and Consequences}

Here's a C code snippet that can cause undefined behavior due to invalid pointer dereferencing:

\begin{lstlisting}[language=C]
int int_at(int *pointer) {
    int r = *pointer;
    return r;
}

int main() {
    printf("%d", int_at((int *)1)); // Invalid pointer dereference
    return 0;
}
\end{lstlisting}

\textbf{Analysis}
\begin{itemize}
    \item The function \texttt{int\_at} attempts to dereference a pointer that has been cast from an integer with value 1, which is not a valid memory address for an \texttt{int}.
    \item Running this code typically results in a segmentation fault, as the address does not point to a legitimate memory location.
\end{itemize}

\textbf{Handling Invalid Pointers}

\begin{itemize}
    \item To avoid undefined behavior, pointers should always be validated before dereferencing.
    \item For instance, checking if a pointer is \texttt{NULL} before using it can prevent some forms of invalid pointer dereferencing.
\end{itemize}

\paragraph{Example of Pointer Validation}

A safer version of the function would include a check for \texttt{NULL}:

\begin{lstlisting}[language=C]
int int_at(int *pointer) {
    if (pointer == NULL) {
        return 0; // Or another error-handling strategy
    }
    int r = *pointer;
    return r;
}
\end{lstlisting}






















\newpage
\section{Undefined behavior}

\subsection{Recap}
The C standard use a few key words that have precise definitions.

Examples:
\begin{itemize}
    \item \texttt{isspace()} : ``The \texttt{isspace} function tests for any character that is a standard white-space character or is one of a locale-specific set of characters [\ldots]" (p206)
    \item \texttt{qsort()} : ``[\ldots] If two elements compare as equal, their order in the resulting sorted array is unspecified." (p369)
    \item \textbf{Byte}: ``A byte is composed of a contiguous sequence of bits, the number of which is implementation-defined." (p4)
    \item ``If an object is referred to outside of its lifetime, the behavior is \emph{undefined}." (p36)
  \item Locale-specific behavior: Behavior that depends on local conventions [...] that each implementation documents. (e.g., \texttt{isspace()})
  
  \item Unspecified behavior: Behavior for which there are multiple possibilities. (e.g., \texttt{qsort()})
  
  \begin{itemize}
    \item Implementation-defined behavior: Unspecified behavior where each implementation (compiler / platform / OS) documents which choice is made. (e.g., \texttt{byte})
  \end{itemize}
  
  \item Undefined behavior
\end{itemize}

\paragraph{Undefined behavior}

\textit{``Behavior, upon use of a nonportable or erroneous program construct or of erroneous data, for which this document'' imposes no requirements.''}

\begin{flushright}
\tiny{*C23 standard}
\end{flushright}

Possible consequences:
\begin{itemize}
  \item compilation or execution crashes
  \item situation completely ignored with unpredictable results,
  \item implementation-defined behavior
  \item by chance, nothing happens and everything goes as intended by the programmer (bad!)
  \item anything else
\end{itemize}

\subsubsection{We have already seen}

All of the following trigger undefined behavior:
\begin{itemize}
  \item division by zero
  \item division overflow
  \item signed integer overflow
  \item dereferencing invalid pointers
\end{itemize}

\begin{lstlisting}
int main()
{
    int i = INT_MAX + 1;
    int b = (i == 100);
    printf("b = %d\n", b);
    return 0;
}
\end{lstlisting}

The compiler is allowed to produce code with output:

\begin{lstlisting}
b = 0
b = 1
b = 42
\end{lstlisting}

\textit{Deleting all your files now...}

If an expression is UB, it does not just get a "wrong" value: it invalidates the whole program.

\subsubsection{Not an idle threat}

The provided C code sample demonstrates a situation where undefined behavior can occur due to compiler optimizations. A static function pointer is initialized to NULL and is intended to point to a function that would erase all files.

\begin{itemize}
    \item The function \texttt{this\_function\_is\_never\_called} is defined which sets the function pointer to the \texttt{erase\_all\_files} function, but it is never called.
    \item The \texttt{main} function returns the result of the function pointer, which is dereferenced and executed.
\end{itemize}

When compiled with \texttt{gcc} using the \texttt{-O3} optimization flag, the program results in a segmentation fault, as the function pointer is NULL. However, when compiled with \texttt{clang} using the same optimization level, the program executes the function that the pointer is supposed to point to, resulting in the message "Deleting all your files NOW..." and potentially dangerous behavior.

\paragraph{Integer Overflow}
On x86\_64 and AArch64, ``add'' has wrap-around semantics:

add w0, INT\_MAX, 1 $\Rightarrow$ w0 = INT\_MIN

\begin{itemize}
    \item will yield i = INT\_MIN sometimes
    \item still undefined behavior
    \item will create bugs in the future!
\end{itemize}

In another example, an integer overflow case is presented. The function \texttt{f} adds 1 to the maximum value an integer can hold, which due to wrap-around semantics in x86\_64 and AArch64 architectures, results in the minimum integer value:

\begin{itemize}
    \item In a situation where the variable \texttt{i} is set to \texttt{INT\_MAX} and then passed to \texttt{f(i)}, it could sometimes yield \texttt{i = INT\_MIN}.
    \item This behavior is considered undefined, meaning that the compiler is not required to handle this situation in any particular way.
    \item Such scenarios can lead to bugs that are difficult to trace and rectify in the future.
\end{itemize}


Following the C standard, the compiled code is (only) bound to behave as if it was running on the "C abstract machine".

No additional constraints are placed on the compiler when targeting a particular ISA even if that ISA's specification has no undefined behavior

\subsubsection{(Almost) everything wrong is undefined behavior (1)}
"The behavior is undefined in the following circumstances: [...]
An unmatched ' or " character is encountered on a logical source line during tokenization" (p584)

\begin{lstlisting}
#include <stdio.h>

int main()
{
    printf("Hello
}

% Error message generated by the compiler
test.c:5:16: error: missing terminating " character
    5 |     printf("Hello
      |                ^
\end{lstlisting}

All modern compilers turn this (and all other parsing errors) into implementation-defined behavior specifically: interrupted compilation with error message

\subsubsection{(Almost) everything wrong is undefined behavior (2)}
\begin{lstlisting}
#include <stdio.h>

char *f()
{
    char buffer[16]; 
    snprintf(buffer, sizeof(buffer), "Hello");
    return buffer;
}

int main()
{
    char *s = f();
    printf("Here is the return value of f():\n");
    printf("%s\n", s);
    return 0;
}

% Compiler warnings and errors
gcc -O3 -o bug bug.c
bug.c: In function 'f':
bug.c:9:16: warning: function returns address of local variable [-Wreturn-local-addr]
    9 |     return buffer;
      |                ^

% Runtime output causing a segmentation fault
./bug
Here is the return value of f():
Segmentation fault (core dumped)
\end{lstlisting}

\subsubsection{Undefined behavior can time-travel}

``[...] However, if any such execution contains an undefined operation,
this document places no requirement on the implementation executing
that program with that input (not even with regard to operations
preceding the first undefined operation).''
(C++20, p7)

\begin{lstlisting}
int f(int a, int b)
{
    printf("a = %d, b = %d\n", a, b);
    printf("We could get a crash now:\n");
    return a / b;
}
\end{lstlisting}

The compiler is allowed to produce an executable that does this:

\begin{lstlisting}
a = 10, b = 0
DELETING ALL FILES NOW, HA HA HA HA !!!!!
We could get a crash now:
Floating point exception (core dumped)
\end{lstlisting}

\subsubsection{Undefined behavior can time-travel (really)}

The example C code sample demonstrates two critical concepts: handling division by zero and an illustration of what is colloquially known as "time travel" in code execution due to compiler optimizations.

\begin{itemize}
    \item The function \texttt{f(int a, int b)} performs an integer division of \texttt{a} by \texttt{b}. If \texttt{b} is zero, the division would result in a runtime error: a floating point exception.
    \item In the \texttt{main} function, the result of the division is conditionally assigned based on the \texttt{argc} parameter. If \texttt{argc} is less than 2, the result is set to 5; otherwise, it's set to the result of \texttt{strtol} conversion of the second argument. This result is then passed as the divisor to the function \texttt{f}.
    \item When the code is compiled with \texttt{gcc} using optimization flag \texttt{-O3} and executed with \texttt{argc} less than 2, the "time travel" behavior occurs. The compiler optimizes the code in a way that the division by zero exception happens even before the conditional check in \texttt{main} can prevent it.
    \item The assembly output below the code shows the division instruction \texttt{idiv} which is the source of the exception. The program crashes with a floating point exception, demonstrating the consequences of undefined behavior in C.
\end{itemize}

\subsubsection{But why?!??}

\begin{itemize}
  \item Performance!
  \item It is all about letting the compiler make \textbf{assumptions}
  \begin{itemize}
    \item Specifically, the compiler assumes that undefined behavior never happens
  \end{itemize}
\end{itemize}

\subsection{Pointer aliasing rules}

\textit{``Aliasing''} means accessing a single object (area of memory) through distinct pointers.

The C standard specifies \textit{``strict aliasing''}:

An object can only be accessed (both read or written) through pointers to that type of object.

$\Rightarrow$ If two pointers have different types, they \textbf{must} point to distinct objects.

``An object shall have its stored value accessed only by an lvalue expression
that has one of the following types:

\begin{itemize}
  \item a type compatible with the effective type of the object,
  \item a qualified version of a type compatible with the effective type of the object,
  \item a type that is the signed or unsigned type corresponding to the effective type of the object,
  \item a type that is the signed or unsigned type corresponding to a qualified version of the effective type of the object,
  \item an aggregate or union type that includes one of the aforementioned types among its members (including, recursively, a member of a subaggregate or contained union), or
  \item a character type.'' (p71)
\end{itemize}

\subsubsection{a type compatible with the effective type of the object}

Valid:

\begin{lstlisting}
typedef int my_int;

my_int f(int *pointer)
{
    my_int *my_pointer = pointer;
    return *my_pointer;
}
\end{lstlisting}

Undefined behavior:

\begin{lstlisting}
int f(long *pointer)
{
    int *my_pointer = (int *)pointer;
    return *my_pointer;
}
\end{lstlisting}

\paragraph{Explanation:}
In the context of pointer aliasing, the term \textit{``effective type''} refers to the actual data type that a memory location is being treated as at any given point in the program. Strict aliasing rules in C dictate that you can only safely access a memory location through a pointer that matches the effective type of the data stored at that location.

In the valid example, `my\_int` is a type alias for `int`, and thus, `my\_int *` and `int *` are considered compatible types. This means that the pointer `my\_pointer` is of a type compatible with the effective type of the object `pointer` points to (which is `int`). As a result, accessing the value pointed by `my\_pointer` is well-defined and adheres to the strict aliasing rules.

On the other hand, the undefined behavior example demonstrates a violation of these rules. Here, a `long *` is being forcefully cast to an `int *`. `int` and `long` are distinct types and may have different sizes or representations in memory. According to strict aliasing rules, an object of type `long` must not be accessed through a pointer of type `int *`. This breach can lead the compiler to make optimizations based on the assumption of strict aliasing, potentially resulting in unpredictable behavior at runtime.

\subsubsection{a qualified version of a type compatible with the effective type of the object}

Valid:

\begin{lstlisting}
int f(int *pointer)
{
    const int *my_pointer = (const int *)pointer;
    return *my_pointer;
}
\end{lstlisting}

\paragraph{Explanation:}
In this valid example, the pointer `pointer` is of type `int *`, and it is cast to `const int *` for `my\_pointer`. This is permissible under the strict aliasing rules as it involves adding a qualifier to the type. 

The type `const int` is considered a qualified version of `int`. The act of adding the `const` qualifier doesn't change the effective type of the object; it simply adds a constraint that prevents modification of the object through the `const`-qualified pointer. In C, it's allowed to access an object through a pointer to a qualified version of the object's effective type. This ensures that the access remains well-defined, and the object's memory representation is correctly interpreted.

However, it's important to note that while you can safely read the value pointed to by `my\_pointer`, attempting to modify it (if `my\_pointer` was not originally `const`) would violate the immutability imposed by the `const` qualifier and lead to undefined behavior.

\subsubsection{a type that is the signed or unsigned type corresponding to the effective type of the object}

Valid:
\begin{lstlisting}
unsigned int f(int *pointer)
{
    unsigned int *my_pointer = (unsigned int *)pointer;
    return *my_pointer;
}
\end{lstlisting}
\paragraph{Explanation:}
In this example, a pointer of type `int *` is cast to `unsigned int *` and then dereferenced. While `int` and `unsigned int` are both integer types, they represent numbers in different ways: `int` for signed integers and `unsigned int` for unsigned integers. 

According to the strict aliasing rules, it is generally not safe to access an object through a pointer of a different type. However, `int` and `unsigned int` are an exception to this rule. The C standard allows objects to be accessed by pointers of their corresponding signed or unsigned types because the memory representation is the same for both signed and unsigned variants of the same size. This means that the bit pattern in memory is interpreted differently but not modified when accessing an `int` as an `unsigned int` or vice versa.

Nonetheless, this practice should be approached with caution. The reinterpretation of the bit pattern means that the numerical value accessed via `my\_pointer` may differ significantly from the value pointed to by `pointer`. For instance, a negative `int` value, when accessed as an `unsigned int`, will be interpreted as a large positive number. Such behavior is well-defined but can lead to unexpected results if not carefully managed.

\subsubsection{a type that is the signed or unsigned type corresponding to a qualified version of the effective type of the object}

Valid:
\begin{lstlisting}
unsigned int f(int *pointer)
{
    const unsigned int *my_pointer = (const unsigned int *)pointer;
    return *my_pointer;
}
\end{lstlisting}

\paragraph{Explanation:}
In this example, a pointer of type `int *` is cast to `const unsigned int *` and then dereferenced. As previously discussed, the C standard allows the object to be accessed through a pointer to the corresponding signed or unsigned type, acknowledging that `int` and `unsigned int` have compatible memory representations.

Adding the `const` qualifier to `unsigned int *` to form `const unsigned int *` is also permissible and aligns with the strict aliasing rules. As noted in the earlier example, adding a qualifier like `const` introduces a promise not to modify the object through this pointer, ensuring read-only access. This action does not change the underlying effective type of the object in memory but merely adds a constraint on how the pointer can be used.

Combining these two aspects, this example is valid because it involves accessing an object through a pointer to a qualified version of a type (with `const`) that corresponds to the signed or unsigned type of the effective type of the object (`int` being accessed as `unsigned int`). However, the same caution mentioned previously applies here: the numerical value accessed via `my\_pointer` might be interpreted differently than the value pointed to by `pointer`, due to the difference between signed and unsigned representation.

\subsubsection{an aggregate or union type that includes one of the aforementioned types among its members (including, recursively, a member of a subaggregate or contained union)}

Valid:
\begin{lstlisting}
struct vec3d {
    int x, y, z;
};

void vec3d_copy(struct vec3d *dst, struct vec3d *src)
{
    *dst = *src;
}
\end{lstlisting}

\paragraph{Explanation:}
This example demonstrates the valid use of an aggregate type, specifically a `struct`, in the context of strict aliasing rules. In C, an aggregate type is a data type that groups multiple individual variables, possibly of different types, into a single unit. In this case, `struct vec3d` is an aggregate type composed of three `int` members: `x`, `y`, and `z`.

The strict aliasing rules permit an object to be accessed through a pointer to an aggregate or union type that includes among its members the type compatible with the effective type of the object. Here, both `src` and `dst` are pointers to `struct vec3d`, and the operation `*dst = *src` involves copying the contents of one `struct vec3d` object to another. This operation is well-defined because it respects the memory layout and type of the aggregate `struct vec3d`.

It's worth noting that aggregate types are treated as a single unit. This means that accessing or modifying a `struct vec3d` object as a whole (as done in this example) is different from accessing or modifying its individual members. This example adheres to the rules by treating `src` and `dst` as pointers to the whole aggregate type, ensuring that the strict aliasing rules are followed.


\subsubsection{a character type}

Valid:

\begin{lstlisting}
struct vec3d {
    int x, y, z;
};

void copy(char *dst, char *src, size_t n)
{
    for (size_t i = 0; i < n; i++) {
        dst[i] = src[i];
    }
}

int main()
{
    struct vec3d a = {1, 2, 3};
    struct vec3d b;

    copy((char *)&b, (char *)&a, sizeof(a));

    return 0;
}
\end{lstlisting}


\paragraph{Explanation:}
This example illustrates an exception in the strict aliasing rules that allows any object to be accessed through a pointer to a character type. In this case, `char *` is used to access and copy the bytes of a `struct vec3d` object. This is permissible because character types (`char`, `signed char`, `unsigned char`) are specifically allowed to alias any other type.

The function `copy` takes pointers to `char` and copies `n` bytes from the location pointed to by `src` to the location pointed to by `dst`. In the `main` function, the addresses of `struct vec3d` objects `a` and `b` are cast to `char *`, and the `copy` function is used to replicate the memory contents of `a` into `b`. This is a common technique used for generic memory copying in C, often seen in functions like `memcpy`.

This is valid under the C standard because it is an established exception to the strict aliasing rules, acknowledging that character pointers can be used for low-level memory manipulation, allowing them to access the byte representation of any object.

\subsubsection{Strict aliasing violations}

Whenever we cast a pointer type to another pointer type,
it is very likely that we invoke undefined behavior.

\textcolor{red}{Danger!} Probable undefined behavior ahead:

\begin{lstlisting}
int *a;
short *b = a;
\end{lstlisting}
\subsubsection{Strict aliasing violations (1)}

\begin{lstlisting}
uint32_t build_u32(uint16_t a, uint16_t b)
{
    uint32_t r;

    uint16_t *p = &r;

    p[0] = a;
    p[1] = b;

    return r;
}
\end{lstlisting}

\paragraph{Explanation:}
This code violates strict aliasing rules by accessing a `uint32\_t` object through a pointer of type `uint16\_t *`. The C standard requires that an object's memory be accessed only through its own type or a compatible type. Here, `uint32\_t` and `uint16\_t` are distinct types with different sizes, and accessing the `uint32\_t` object `r` as if it were an array of `uint16\_t` leads to undefined behavior according to the strict aliasing rules.



\subsubsection{Strict aliasing violations (2)}

This code demonstrates a strict aliasing violation involving different structure types. Although `struct my\_data\_0`, `struct my\_data\_1`, and `struct my\_data\_2` all have a common initial member `subtype`, they are distinct types with different memory layouts. The function `get\_first` attempts to access the passed `struct my\_data\_0 *data` pointer as if it could be `struct my\_data\_1 *` or `struct my\_data\_2 *` based on the `subtype`. This violates strict aliasing rules because `struct my\_data\_1` and `struct my\_data\_2` are not compatible with `struct my\_data\_0`, and accessing memory through incompatible types leads to undefined behavior. The C standard does not guarantee that different structure types will have compatible memory layouts, even if they share initial members.

\subsubsection{Strict aliasing violations (3)}

The code snippet involves a `union` named `mux` that allows its memory to be accessed as either an array of `int32\_t` (`i`) or `int16\_t` (`s`). While unions are designed to provide a way to treat the same memory location in multiple ways, this specific usage may lead to a strict aliasing violation. The code assigns values to the `int32\_t` array and then attempts to access these values through the `int16\_t` array. According to strict aliasing rules, accessing a memory location as a different type than it was written with (excluding character types) is undefined. However, it's worth noting that some compilers provide extensions or options to allow this kind of access, potentially yielding the intended operations. Nevertheless, this practice is not portable and relies on compiler-specific behavior, which is generally discouraged

\textbf{Note:} Some compilers promise to yield the intended operations here.

\subsubsection{How do I do type-punning then?}

\textit{``Type punning''} is reading the bits of an object as an object of a different type.

\textbf{Valid:}

\begin{lstlisting}
int main()
{
    int i[2];
    short s[4];

    i[0] = 0x03020100;
    i[1] = 0x07060504;

    memcpy(s, i, 2 * sizeof(int));

    printf("%d %d %d %d\n", m.s[0], m.s[1], m.s[2], m.s[3]);

    return 0;
}
\end{lstlisting}

\paragraph{Explanation:}
The code demonstrates valid type-punning using `memcpy` to safely copy data from an `int` array to a `short` array. This method avoids strict aliasing violations by copying the memory representation without directly accessing the data through pointers of differing types. This approach ensures that the underlying bytes are transferred accurately without invoking undefined behavior, making the operation safe and portable across different compilers and platforms. `memcpy` respects the memory boundaries and types, providing a reliable method for type-punning in compliant C code.


\subsubsection{Why is strict aliasing good for code optimization?}

Strict aliasing rules significantly impact code optimization by providing the compiler with guarantees about memory access patterns. These guarantees enable compilers to perform more aggressive optimizations, resulting in faster and more efficient code. Here's how:

\begin{itemize}
    \item \textbf{Assumption of Independence:} When strict aliasing rules are followed, the compiler assumes that pointers of different types point to non-overlapping memory regions. This assumption allows the compiler to optimize code without having to consider the possibility of one pointer aliasing (or pointing to the same memory location as) another pointer of a different type. As a result, the compiler can generate more streamlined and efficient machine code, as it is free from the burden of inserting additional checks or conservative code paths that account for potential aliasing.

    \item \textbf{Loop Optimizations:} The examples labeled "Fast" demonstrate how adherence to strict aliasing rules enables the compiler to optimize loops more effectively. The compiler can safely assume that the elements being accessed in the loop do not overlap in memory, allowing it to minimize memory loads and stores, and sometimes even vectorize the loop (use SIMD instructions). This results in fewer instructions and a more efficient utilization of the CPU's execution units.

    \item \textbf{Effective Instruction Scheduling:} By understanding the non-overlapping nature of memory accesses, the compiler can schedule instructions in a way that reduces pipeline stalls and improves the overall throughput of the program. This is particularly beneficial in modern processors with deep pipelines and multiple execution units.

\end{itemize}

\subsubsection{And when strict aliasing is not enough?}

When strict aliasing is not enough to guarantee non-overlapping memory regions, particularly in pointer-intensive operations, the \texttt{restrict} keyword can be used as an additional hint to the compiler. This keyword is a promise to the compiler that for the lifetime of the pointer, only the pointer itself or a derivative of it will be used to access the object to which it points. This assurance allows the compiler to make further optimizations by assuming that pointers declared with \texttt{restrict} are not aliased, meaning they do not point to overlapping memory regions.

\begin{itemize}
    \item \textbf{Enhanced Loop Optimizations:} The use of \texttt{restrict} enables the compiler to assume that the arrays \texttt{dst}, \texttt{src}, and \texttt{constant} do not overlap. This allows for more aggressive optimizations in loops, as seen in the "Fast" example. The compiler can optimize memory access patterns and instruction scheduling, resulting in more efficient code execution.
    
    \item \textbf{Improved Parallelism:} Knowing that the data does not overlap allows the compiler to avoid conservative assumptions about data dependencies. This can lead to improved parallel execution, both at the instruction level (such as pipelining and instruction-level parallelism) and at higher levels (such as loop unrolling and vectorization).
    
    \item \textbf{Better Resource Utilization:} With clearer assumptions about memory usage, the compiler can make more informed decisions about register allocation, instruction selection, and other optimizations that contribute to better utilization of the CPU and memory hierarchy.
\end{itemize}

\subsection{More types of undefined behavior}

\subsubsection{Unaligned pointers}

Every type has a required alignment (which we can query with \texttt{alignof(type)}). (see p44)

Every pointer to that type must be a multiple of that alignment. 

Undefined behavior:

\begin{lstlisting}
int *alloc_5_bytes()
{
    char *c = malloc(1 + sizeof(int));
    return c + 1;
}
\end{lstlisting}

\subsubsection{Out-of-bounds pointer arithmetic}

“When two pointers are (added or) subtracted, 
both shall point to elements of the same array object, 
or one past the last element of the array object;” (p84)

Undefined behavior:

\begin{lstlisting}
size_t eight()
{
    char c[4];
    return &(c[8]) - &(c[0]);
}
\end{lstlisting}

\subsubsection{Infinite loops}

An infinite loop with no side effects is undefined behavior.

Undefined behavior:

\begin{lstlisting}
while (1)
{
}
\end{lstlisting}

Valid:

\begin{lstlisting}
while (1)
{
    printf("Hello\n");
}
\end{lstlisting}

\subsubsection{Shift beyond integer size}

Undefined behavior:

\begin{itemize}
    \item left/right shift integer by a negative number
    \begin{lstlisting}
        uint32_t a = 1 >> -5;
    \end{lstlisting}

    \item left/right shift an \textit{n}-bit integer by \textit{n} or more positions
    \begin{lstlisting}
        uint32_t a = 1;
        uint32_t b = a << 32;
    \end{lstlisting}

    \item left shift signed integer \textit{i} by \textit{k} positions and \textit{i} $ \times 2^{k} $ is not representable
    \begin{lstlisting}
        uint32_t a = -1024;
        uint32_t b = a << 30;
    \end{lstlisting}
\end{itemize}
C23 pp584--594: 218 types of undefined behavior






















\newpage
\section{Floating-point arithmetic}

\subsection{Real Numbers}

\subsubsection{How do we represent non-integers?}

Keeping in mind:
\begin{itemize}
    \item If we consider $n$ bits of memory,
    \begin{itemize}
        \item their values can take $2^n$ combinations
        \item so we can represent $2^n$ numbers at best with those $n$ bits
    \end{itemize}
    \item We have a finite amount of memory,
    \begin{itemize}
        \item so we cannot represent all real numbers
    \end{itemize}
    \item We (typically) want fast operations,
    \begin{itemize}
        \item so (ideally) we need hardware to perform them.
        \item Hardware has tight limits on the number of logic gates available
        \item meaning we use very few bits (say 16, 32 or 64, like for integers)
        \item \ldots further restricting how many real numbers we can represent
    \end{itemize}
\end{itemize}

\subsubsection{Practical limitations}

\begin{itemize}
  \item Integer are restricted in one way:
  \begin{itemize}
    \item their range (e.g. [\texttt{INT\_MIN}, \texttt{INT\_MAX}])
  \end{itemize}
  \item Reals are restricted in two ways:
  \begin{itemize}
    \item their range (e.g. [$-10^{308}$, $10^{308}$])
    \item the number of reals we can represent in that range (e.g. \{..., $0, 10^{-200}, 2 \times 10^{-200}, ...$\})\\
    i.e. their precision
  \end{itemize}
\end{itemize}

\subsection{Fixed-point arithmetic}

\subsubsection{Decimal example}

Instead of computing money values in \texteuro, we could use c:

e.g.  29.99\texteuro{} = 2999c\\
then use integer operations.

\begin{itemize}
  \item This is fixed-point arithmetic
  \item specifically, with 2 decimal places reserved for the fractional part.
\end{itemize}

% Note: LaTeX does not have a built-in command for the Euro symbol by default. 
% The \texteuro command used in this transcription can come from a package like "eurosym" 
% which provides the Euro currency symbol. If that package is not used, the Euro symbol 
% will not appear correctly in the resulting document unless an alternative method is implemented.


% Since the LaTeX document setup (such as \documentclass) is not requested, we begin directly with the content of the slide.
% Note that the LaTeX code provided here is not a standalone document, and a proper preamble would be needed for compilation.
% Replace images or non-textual elements with [TBD].

If $+, -, \times, /$ are the elementary integer operations:

\begin{itemize}
	\item $euro\_to\_cent(e) \mathrel{::=} e \times 100$
	\begin{itemize}
		\item $euro\_to\_cent (5 \texteuro) = 500 \textcent$
	\end{itemize}
	\item $cent\_to\_euro(a) \mathrel{::=} a/100$
	\begin{itemize}
		\item $cent\_to\_euro(700 \textcent) = 7 \texteuro$
	\end{itemize}
	\item $cent\_add(a, b) \mathrel{::=} a+b$
	\begin{itemize}
		\item $cent\_add(700 \textcent, 500 \textcent) = 1200 \textcent$
	\end{itemize}
	\item $cent\_sub(a, b) \mathrel{::=} a-b$
	\begin{itemize}
		\item $cent\_sub(700 \textcent, 500 \textcent) = 200 \textcent$
	\end{itemize}
	\item $cent\_mul(a, b) \mathrel{::=} (a \times b)/100$
	\begin{itemize}
		\item $5 \texteuro \times 7: cent\_mul(500 \textcent, 700 \textcent) = 500 \times 700 / 100 = 3500 \textcent$
	\end{itemize}
	\item $cent\_div(a, b) \mathrel{::=} (a \times 100)/b$
	\begin{itemize}
		\item $8 \texteuro / 4: cent\_div(800 \textcent, 400 \textcent) = (800 \times 100) / 400 = 200 \textcent$
	\end{itemize}
\end{itemize}

\subsubsection{Binary fixed-point arithmetic}

\begin{itemize}
	\item There is no universally accepted standard for fixed-point arithmetic
	\item But there is no real need for one:
	\begin{itemize}
		\item Only two parameters:
		\begin{description}
			\item[$n$:] total number of bits
			\item[$p$:] number of bits after the decimal point
		\end{description}
		\item All the operations are just integer operations
		\begin{itemize}
			\item For $mul$ and $div$, two integer operations each
		\end{itemize}
	\end{itemize}
\end{itemize}

\paragraph{Binary example}
\begin{itemize}
    \item A 64-bit integer can be used for fixed-point arithmetic by dividing it into two parts: a 32-bit integer part and a 32-bit fractional part.
    \item The conversion and arithmetic operations are defined as follows:
\end{itemize}


\begin{lstlisting}
i64_to_fix(e) := e x 2^32
fix_to_i64(a) := a / 2^32
fix_add(a, b) := a + b
fix_sub(a, b) := a - b
fix_mul(a, b) := (a x b) / 2^32
fix_div(a, b) := (a x 2^32) / b
\end{lstlisting}

The function $i64_to_fix()$ is taking an integer $e$ of type $i64$ (which is a 64-bit integer) and converting it into a fixed-point representation of type \textit{fix}. The multiplication by $2^32$ effectively shifts the integer value $e$ into the higher 32 bits of the 64-bit representation, leaving the lower 32 bits for the fractional part. This way,  it is possible to perform fixed-point arithmetic with fix types while maintaining precision for the fractional component of the values.\\

These operations are implemented using integer arithmetic, providing precise control over the scaling and range of the values.

\paragraph{Implementation in C}

Fixed-point arithmetic operations can be implemented in C using a typedef for a 64-bit integer to represent fixed-point numbers. The integer part occupies the upper 32 bits and the fractional part occupies the lower 32 bits. 

\begin{itemize}
    \item \textbf{Conversion to Fixed-Point:} An integer is converted to fixed-point format by shifting it left by 32 bits, moving the integer part to the upper 32 bits.
    \item \textbf{Conversion from Fixed-Point:} A fixed-point number is converted back to an integer by shifting it right by 32 bits, discarding the fractional part.
    \item \textbf{Addition and Subtraction:} These operations are performed directly on the fixed-point representations, with the result being in fixed-point format.
    \item \textbf{Multiplication:} Multiplication requires casting to a wider integer (128-bit) to accommodate the intermediate result before shifting right by 32 bits to maintain the correct scale.
    \item \textbf{Division:} Division is handled by first shifting the dividend left by 32 bits (to align the integer part correctly) before performing the division.
\end{itemize}

Each operation maintains the scale of fixed-point representation and ensures that the integer and fractional parts are processed correctly. Care is taken during multiplication and division to avoid overflow and to maintain precision.


\paragraph{Fixed-point arithmetic, Pros}
\begin{itemize}
    \item fast, no need for extra hardware
    \item easy to understand and study (predictible):
    \begin{itemize}
        \item uniform absolute precision (e.g. $2^{-32}$ over whole range)
    \end{itemize}
\end{itemize}

% The following is an enumeration list for "Cons" of fixed-point arithmetic
\paragraph{Fixed-point arithmetic, Cons}
\begin{itemize}
    \item limited range (e.g. $[-2147483648.999, 2147483647.999]$)
    \item limited precision (e.g. $2^{-32} \approx 0.000000002328$)
\end{itemize}

% Potential improvements that could be made to fixed-point arithmetic
\textbf{Possible improvements:}
\begin{itemize}
    \item larger range
    \item better absolute precision around zero
    \item lower absolute precision for big numbers
\end{itemize}

% The next line would suggest the topic of the following section or lecture part.
\subsection{Floating-point Arithmetic}


% Transcription of the image content into LaTeX
% Explanatory comments and LaTeX commands are included as % comments for clarity

\subsubsection{Scientific notation}

Take the number $-2147483648.999$: 
\begin{align*}
-2147483648.999 &= -2.147483648999 \times 10^{9} \\
&= -2.147483648999e+9
\end{align*}

Similarly, take the number $0.0000000002328$:
\begin{align*}
0.0000000002328 &= 2.328 \times 10^{-10} \\
&= 2.328e-10
\end{align*}

\subsubsection{Scientific Notation}
Scientific notation is a way to express numbers that are too large or too small to be conveniently written in decimal form. It is commonly used in calculations and by scientists, mathematicians, and engineers.

\begin{itemize}
    \item In scientific notation, numbers are written as a product of two numbers: a coefficient and \(10\) raised to a power, for example, \(-2.147483648999 \times 10^9\).
    \item The coefficient must be a number between \(1\) and \(9\), followed by a decimal point and the rest of the significant digits.
    \item The exponent is written as a power of \(10\), indicating how many places the decimal point should be moved to convert the number into a standard decimal number.
\end{itemize}

\subsubsection{Binary Floating-Point Numbers}
In computing, binary floating-point numbers are a way to represent real numbers in a binary system, typically using three components: the sign, exponent, and mantissa.

\begin{itemize}
    \item The sign determines if the number is positive or negative.
    \item The mantissa, or significand, is composed of the significant digits of the number.
    \item The exponent indicates where the decimal (or binary) point is placed relative to the beginning of the mantissa.
\end{itemize}

Binary floating-point numbers allow for a wide range of values to be represented in a standardized way, which is why they are commonly used in computer systems.


\subsubsection{Pros and Cons of Binary Floating-Point Numbers}
\textbf{Pros:}
\begin{itemize}
    \item Efficient to process with modern hardware.
    \item Predictable precision and representation across different systems.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item Limited precision, which can lead to rounding errors in calculations.
    \item Limited range, meaning extremely large or small numbers may not be representable.
\end{itemize}


\subsubsection{Floating-point standard}
\begin{itemize}
    \item In 1985, the Institute of Electrical and Electronics Engineers publishes standard \#754 about floating-point arithmetic (IEEE-754)
    \item The standard \#754 defines the number of bits used for the sign, exponent, and mantissa. This standard ensures that floating-point operations behave consistently across different computing systems.
    \item Most hardware makers adopt the standard very quickly thereafter (Intel 80387, launched in 1987, is fully compliant)
    \item x86\_64 natively supports binary32 and binary64 formats
    \item AArch64 natively supports binary16, binary32 and binary64 formats
\end{itemize}

\begin{tabular}{lccc}
    component & binary16 & binary32 & binary64 \\
    \hline
    $\pm$ sign bit & 1 & 1 & 1 \\
    $mmmmm...$ mantissa bits & 10 & 23 & 52 \\
    $xxxx...$ exponent bits & 5 & 8 & 11 \\
    exponent range & -14..15 & -126...127 & -1022...1023 \\
\end{tabular}

\subsubsection{Precision}

\textbf{Absolute and Relative Precision}
\begin{itemize}
    \item \textbf{Absolute precision:} For a given \(x\), the smallest \( \varepsilon \) such that
    \[
        fl(x + \varepsilon) \neq fl(x)
    \]

    \item \textbf{Relative precision:} For a given \(x\),
    \[
        \varepsilon := \frac{\varepsilon}{x}
    \]
\end{itemize}

\subsubsection{Binary64 vs. Fixed-Point 32+32 Comparison}

\textbf{Precision at Different Scales}
\begin{itemize}
    \item The fixed-point 32+32 format provides a consistent absolute precision across different magnitudes, beneficial for values around 1 or below.
    \item Floating-point binary64 format, based on IEEE-754, offers varying precision, with relative precision being consistent but absolute precision varying with the value's magnitude.
\end{itemize}

\textbf{Range}
\begin{itemize}
    \item Fixed-point 32+32 has a limited range up to \(|x| \leq 2.15 \times 10^9\), making it unsuitable for very large values.
    \item Binary64 supports a significantly larger range up to \(|x| \leq 1.80 \times 10^{308}\), accommodating both very large and very small values.
\end{itemize}

\begin{tabular}{l|cc|cc}
    \hline
     & \multicolumn{2}{c|}{fixed-point 32+32} & \multicolumn{2}{c}{floating-point binary64} \\
     \hline
     & absolute & relative & absolute & relative \\
    \hline
    precision at \(10^{-9}\) & \(2.33 \times 10^{-10}\) & \(0.0233\) & \(2.07 \times 10^{-25}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{-6}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-5}\) & \(2.12 \times 10^{-22}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{-3}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-8}\) & \(2.17 \times 10^{-19}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(1\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-11}\) & \(2.22 \times 10^{-16}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{3}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-14}\) & \(1.14 \times 10^{-13}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{6}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-17}\) & \(1.16 \times 10^{-10}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{9}\) & \(2.33 \times 10^{-10}\) & \(2.33 \times 10^{-20}\) & \(1.19 \times 10^{-7}\) & \(2.22 \times 10^{-16}\) \\
    precision at \(10^{16}\) & \textcolor{red}{\textbf{X}} &  & \(2.00\) & \(2.22 \times 10^{-16}\) \\
    \hline
    range & \(|x| \leq 2.15 \times 10^{9}\) &  & \(|x| \leq 1.80 \times 10^{308}\) &  \\
    \hline
\end{tabular}

\subsubsection{The Floating-Point Number Line}

\begin{itemize}
    \item The floating-point number line is non-linear, with denser representation of numbers near zero and sparser as the magnitude increases.
    \item This reflects the constant relative precision of floating-point numbers.
\end{itemize}

\subsubsection{Programming Languages and IEEE-754 Standard}

\begin{itemize}
    \item Several programming languages mandate compliance with the IEEE-754 standard for floating-point arithmetic, ensuring consistency across computing platforms.
    \item Languages like C (since C99) and C++ (since C++03) offer types like \texttt{float} for binary32 and \texttt{double} for binary64.
    \item Modern languages such as Python and JavaScript also adhere to this standard, with Python using binary64 precision by default.
\end{itemize}


\begin{tabular}{lccc}
\textbf{language} & \textbf{since} & \textbf{binary32} & \textbf{binary64} \\
C & C99 & float & double \\
C++ & C++03 & float & double \\
Fortran & Fortran 2003 & real & double \\
Rust & & f32 & f64 \\
Python & & & \checkmark \\
JavaScript & & & \checkmark \\
\end{tabular}




\subsubsection{Inaccuracy in Base 10 and Base 2}
In floating-point arithmetic, inaccuracy can arise due to the way numbers are represented in base 10 and base 2. For example, fractions like \( \frac{1}{3} \) and \( \frac{2}{3} \) do not have exact representations in base 10 and result in repeating decimals. This inaccuracy is also present in binary (base 2), where numbers like 0.1 cannot be precisely represented.

\subsubsection{Numerical Instability}
Numerical instability refers to errors that can grow when applying mathematical operations, especially in iterative calculations or when dealing with very large or small numbers.\\

\textbf{Derivative Approximation}\\
A common operation in numerical analysis is the approximation of derivatives using finite differences:
\[ \frac{df(x)}{dx} \approx \frac{f(x + \delta) - f(x)}{\delta} \]
However, choosing a very small \( \delta \) can lead to significant inaccuracies due to the limited precision of floating-point numbers.

\subsubsection{Example of Numerical Instability}
Consider computing the derivative of \( f(x) = x \) at large values of \( x \) using a small \( \delta \). When \( x \) is much larger than \( \delta \), the subtraction \( (f(x + \delta) - f(x)) \) can result in loss of precision, and the computed derivative will deviate from the expected value of 1.

\subsubsection{What is happening?}
\begin{itemize}
    \item At $x = 10^{+5}$, we first compute $(10^{+5} + 10^{-6})$
    \begin{itemize}
        \item which is a big number, close to $10^{+5}$
        \item floating-point numbers have a good \textit{relative} accuracy everywhere, $\sim 2.22 \times 10^{-16}$
        \item but at $10^{+5}$, the \textit{absolute} accuracy is not great, $\sim 1.91 \times 10^{-6}$
        \item so the result of $(10^{+5} + 10^{-6})$ may be off by roughly $1.91 \times 10^{-6}$
    \end{itemize}
    \item We then subtract $10^{+5}$.
    \begin{itemize}
        \item If we were using exact arithmetic, we would get $10^{-6}$ exactly,
        \item but we are using floating-point arithmetic,
        \item so we get something close to $10^{-6}$...
        \item ... but potentially off by roughly $1.91 \times 10^{-6}$
    \end{itemize}
    \item We divide by $10^{-6}$,
    \begin{itemize}
        \item and get a number in $[1 - 1.91, 1 + 1.91]$
    \end{itemize}
\end{itemize}
The deviations we just discussed arise because floating-point numbers have more relative precision near zero, and less absolute precision as their magnitude increases.

Therefore,
\begin{itemize}
    \item floating-point accuracy is often great
    \item but some algorithms are \textbf{unstable}
    \item we need to be extremely careful before trusting floating-point results
\end{itemize}




\subsubsection{Never do exact comparisons}

\begin{lstlisting}
>>> 0.1 + 0.2 == 0.3
False
\end{lstlisting}

\begin{lstlisting}
>>> 1.0 - 1e-16 == 1.0
True
\end{lstlisting}

\subsubsection{So how do we do comparisons?} % Lecture slide heading

\begin{itemize}
  \item If exact comparisons are important, \textbf{do not use floating-point arithmetic}.
  \item If we care about speed and can tolerate some errors...
\end{itemize}

% Example of a proper way to do comparisons with tolerance in floating point arithmetic
\begin{lstlisting}
>>> 0.1 + 0.2 == 0.3
False
\end{lstlisting}

becomes

\begin{lstlisting}
>>> tolerance = 1e-10
>>> abs( (0.1 + 0.2) - 0.3 ) < tolerance
True
\end{lstlisting}

% Another example of a proper way to do comparisons with tolerance
\begin{lstlisting}
>>> x = 0.0
\end{lstlisting}

becomes

\begin{lstlisting}
>>> x > -tolerance
\end{lstlisting}



\subsection{Rounding Floating-Point Numbers}
When a real number cannot be represented exactly by a floating-point number, we approximate it by "rounding" to the nearest floating-point number. The IEEE-754 standard outlines several rounding modes to handle such cases:

\begin{itemize}
    \item \textbf{Round to Nearest, Ties to Even:} The default rounding mode which rounds to the nearest value; if equidistant, it rounds to the nearest even number in the floating-point representation.
    \item \textbf{Round to Nearest, Ties Away from Zero:} Rounds to the nearest value; if equidistant, it rounds away from zero.
    \item \textbf{Round Toward Zero:} Always rounds towards zero, truncating the extra digits.
    \item \textbf{Round Toward \(+\infty\):} Always rounds up.
    \item \textbf{Round Toward \(-\infty\):} Always rounds down.
\end{itemize}

\subsubsection{Determinism in Floating-Point Arithmetic}
While floating-point arithmetic may be inaccurate due to rounding, it is deterministic:

\begin{itemize}
    \item The result of most operations is precisely defined by the standard.
    \item We can predict the result of operations bit-for-bit, ensuring reproducible calculations.
\end{itemize}

\subsubsection{IEEE-754 Rounding Rules}
Let us denote by \(fl(x)\) the floating-point representation of the real number $x \in \mathbb{R}$.\\
The IEEE-754 standard mandates correct rounding as per the selected rounding mode for various operations:

\begin{itemize}
    \item \textbf{Addition, Subtraction:} \( x + y \) results in \( fl(x + y) \).
    \item \textbf{Multiplication, Division:} \( x / y \) results in \( fl(x / y) \).
    \item \textbf{Square Root:} \( \sqrt{x} \) results in \( fl(\sqrt{x}) \).
    \item \textbf{Fused Multiply-Add:} \( fma(x, y, z) \) results in \( fl(x \times y + z) \).
\end{itemize}






\subsubsection{Division example}

When executing

\[ z = x / y \]

\begin{itemize}
  \item we first take the floating-point numbers $x$ and $y$, and consider them as if the were (exact, infinite-precision) real numbers
  \item we compute the (exact, infinite-precision) real quotient $x / y$.
  \item we round the result according to the current \textbf{rounding mode}: fl($x / y$)
  \item we store the \textbf{rounded} floating-point value into $z$
\end{itemize}

% End of LaTeX code


% Note that this LaTeX code represents the textual content of the slide.
% Depending on the document class and preamble you choose to wrap this with,
% you may need to include packages such as amsmath for math typesetting.

\subsubsection{Expression example}
Consider the expression:
\[
\frac{y \times (x + 4.0)}{z - 3.0}
\]
In a floating-point computation, due to rounding errors, this expression gives:
\[
fl\left( fl(y \times fl(x + 4)) \div fl(z - 3) \right)
\]
where \( fl \) denotes the floating-point representation of a number, highlighting the sequential rounding that occurs at each step.

\subsubsection{About fused multiply-add}
The fused multiply-add operation is a common computation in many numerical algorithms. It combines multiplication and addition but must be treated with caution:

\textbf{Beware:} The fused multiply-add function \( fma(x, y, z) \) does not simply equate to \( x \times y + z \).

\textbf{Indeed:}
\begin{itemize}
    \item \( fma(x, y, z) \) computes \( fl(x \times y + z) \), providing a more accurate result by combining the operations into a single step to minimize rounding errors.
    \item However, separately computing \( x \times y + z \) results in \( fl(fl(x \times y) + z) \), where each operation introduces potential rounding errors.
\end{itemize}

\subsubsection{More floating-point non-identities}
Certain algebraic identities do not hold for floating-point numbers due to rounding errors and finite precision:
\begin{itemize}
    \item Associativity does not hold: \( x + (y + z) \neq (x + y) + z \).
    \item Distributivity does not hold: \( x \times (y + z) \neq x \times y + x \times z \).
\end{itemize}


The IEEE-754 standard defines the representation and behavior of floating-point numbers:

\begin{itemize}
    \item It mandates correct rounding for basic operations such as \( + \), \( - \), \( \times \), \( \div \), \( \sqrt{} \), and \( fma() \).
    \item However, it does not mandate correct rounding for more complex functions like trigonometric, hyperbolic, exponential, and logarithmic functions. This can lead to discrepancies in different implementations and requires careful handling.
\end{itemize}

\subsubsection{Floating-point and compilers}

\begin{itemize}
  \item C99 and C++03 mandate IEEE-754
  \item which in turn mandates correct rounding for $+, -, \times, \div, \sqrt{()}, \text{fma}()$.
  \item However, if we do not specify a C or C++ standard (e.g., \texttt{-std=c17} or \texttt{-std=c++20}), \\
  gcc and clang do not follow IEEE-754
  \begin{itemize}
    \item they will happily exploit associativity and distributivity
    \item they will replace $x \times y + z$ by fma$(x, y, z)$
  \end{itemize}
\end{itemize}

\subsubsection{Why does correct rounding matter?}

\begin{itemize}
  \item (generally) not because of accuracy
  \item but because for any real number $x$, there is exactly one correct rounding
  \item as a result, there is no ambiguity:
  \begin{itemize}
    \item given a set of floating-point numbers
    \item given any expression involving those numbers and $+, -, \times, \div, \sqrt{()}, \text{fma}()$
    \item there is exactly one correct answer
    \item which is precisely specified by IEEE-754, down to its bit representation
  \end{itemize}
\end{itemize}



% This LaTeX code is designed to replicate the text content of a presentation slide. 
% Direct LaTeX commands are used to format the structure of the text.
% Sections, items, and emphasis are arranged to reflect the visual layout of the slide.

\subsubsection{What happens without correct rounding?} % Section title
\textbf{Results can change when:} % Bold statement

\begin{itemize} % Unordered list of items
  \item we change architecture
  \item we change compiler
  \item we change the standard C library
  \item we change the version of the compiler
  \item we change the version of the standard C library
  \item we change our code (even a completely unrelated part)
\end{itemize}

\textbf{Note:} If we use $\sin$, $\cos$, $\log$, $\exp$, ..., which are not correctly rounded, 
then we are exposed to result \textit{changes} 
whenever we change the version of the standard C library 
(which could be dynamically linked!)

\subsection{Beyond floating-point Arithmetic} % Section title with greater emphasis

\subsubsection{Interval arithmetic} % Subsection title
Interval arithmetic is an approach to encompass the uncertainty and rounding errors in numerical computations.

\begin{itemize}
    \item Every real number \( x \in \mathbb{R} \) is represented by an interval \([l, u]\) using a pair of floating-point numbers, ensuring that \( x \) lies within this interval.
    \item This method employs rounding modes, specifically \textit{Round toward \( +\infty \)} and \textit{Round toward \( -\infty \)}, to compute the accurate interval for each operation.
\end{itemize}

\textbf{Advantages:}
\begin{itemize}
    \item It is a fast method.
    \item It provides knowledge of how accurate a result is.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item The interval \([l, u]\) can become excessively large quickly, often yielding overly pessimistic bounds.
\end{itemize}

\subsubsection{Unum}
Unum arithmetic is a relatively new system, introduced to improve the precision and reliability of computations:
\begin{itemize}
    \item It was introduced in 2015, with the latest revision in 2017.
    \item Unum claims to allocate precision better within a given fixed bit width and offers optional interval arithmetic.
    \item However, it has seen very limited adoption, mainly because there is no hardware support on mainstream platforms as of the last update.
\end{itemize}

\subsubsection{The GNU Multi-Precision Library (GMP)}
The GNU Multi-Precision Library is an important tool for computations requiring high precision:

\begin{itemize}
    \item GMP is a C library designed to provide support for variable-width integers and arbitrary-size rational numbers.
    \item A rational number is expressed as a fraction \( \frac{\text{numerator}}{\text{denominator}} \), where the greatest common divisor (gcd) of the numerator and denominator is 1, ensuring the fraction is in its simplest form.
    \item More information about GMP can be found at \url{http://gmplib.org}.
\end{itemize}


\subsubsection{The GNU MPFR library}
The GNU MPFR library enhances the GNU Multi-Precision Library (GMP) by providing a facility for arbitrary-precision floating-point computation.

\begin{lstlisting}
double x = 22.0 / 7.0;
printf("%.20f\n", x);

mpfr_t x;
mpfr_init2(x, 512);             // initialize x with 512-bit mantissa
mpfr_set_ui(x, 22, MPFR_RNDN);  // set x to value 22, round-to-nearest
mpfr_div_ui(x, x, 7, MPFR_RNDN); // divide x to 7, round-to-nearest
mpfr_printf("%.20Rf\n", x);     // print x
mpfr_clear(x);                  // free memory
\end{lstlisting}

For more details, visit \url{http://mpfr.org}.

\subsubsection{Python fractions}
Python supports arbitrary-precision integers and provides a module for exact rational number arithmetic.

\begin{lstlisting}
>>> -2 ** 65
-36893488147419103232   # correct result, no overflow

import fractions
a = fractions.Fraction(numerator, denominator)
\end{lstlisting}


\subsubsection{Why don't we always use exact rational numbers?}
While exact rational numbers provide precision, their use is not always practical in computing due to several factors:

\textbf{Convenience:}
\begin{itemize}
    \item Using GMP in C or importing fractions in Python may not always be straightforward.
\end{itemize}

\textbf{Memory:}
\begin{itemize}
    \item The size of the numerator and denominator can become very large, especially in iterative algorithms, even after gcd reductions.
\end{itemize}

\textbf{Speed:}
\begin{itemize}
    \item Operations on arbitrary-sized integers or exact rationals are slower (often by an order of magnitude) compared to fixed-size native types, due to lack of native hardware support.
\end{itemize}

\subsubsection{The Case for Exact Rational Numbers}
Should we use exact rational numbers more often in computations? Yes, particularly when:
\begin{itemize}
    \item Exactness is crucial to the application.
    \item Computational speed is not a limiting factor.
\end{itemize}

Exact rational numbers provide unparalleled precision and are essential in applications where accuracy cannot be compromised, such as in cryptography or symbolic mathematics.

\subsubsection{Symbolic computations}
Symbolic computation systems are powerful tools that allow for manipulation and solving of algebraic expressions symbolically rather than numerically. This means that the expressions are kept in their algebraic form, allowing for exact solutions and manipulations.\\

In a symbolic algebra system:
\begin{itemize}
    \item The square root of 2, denoted as \(\sqrt{2}\), is never approximated to a decimal such as 1.4142. It is kept in the root form for exact calculations. For instance, in SageMath:
    \begin{lstlisting}
    sage: sqrt(8)
    2*sqrt(2)
    \end{lstlisting}

    \item Variables can be defined without assigning them specific values, which is useful for carrying out algebraic manipulations. For example:
    \begin{lstlisting}
    sage: x, y, z = var('x y z')
    sage: sqrt(8) * x
    2*sqrt(2)*x
    \end{lstlisting}

    \item Symbolic computations allow solving algebraic problems, such as equations, symbolically. For instance:
    \begin{lstlisting}
    sage: x, b, c = var('x b c')
    sage: solve([x^2 + b*x + c == 0], x)
    [x == -1/2*b - 1/2*sqrt(b^2 - 4*c), x == -1/2*b + 1/2*sqrt(b^2 - 4*c)]
    \end{lstlisting}
\end{itemize}

\subsubsection{Symbolic Algebra Systems}
Several systems are available for performing symbolic computations:

\begin{itemize}
    \item SageMath is an open-source mathematical software system with a syntax similar to Python.
    \item Maple is a commercial computer algebra system.
    \item Wolfram Mathematica is a widely used system that offers powerful symbolic computation capabilities.
\end{itemize}

WolframAlpha, a web application based on Mathematica, provides step-by-step solutions and is accessible at \href{http://www.wolframalpha.com}{WolframAlpha}.























\newpage
\section{Software engineering practices}
\subsection{Tools for Program Correctness}
Today:
\begin{enumerate}
  \item Documentation
  \item Testing
  \item Static analysis
  \item Dynamic analysis
\end{enumerate}

\begin{itemize}
  \item Each uncovers bugs
  \item For each, there are useful tools (compilers can help!)
\end{itemize}
\subsection{Documentation}

Documentation is \textbf{Good}.

\begin{itemize}
    \item Allows others to understand your code
    \item Allows yourself (in a few weeks) to understand your own code
    \item Helps make your thought process and assumptions explicit
\end{itemize}

\subsubsection{Types of documentation}

\begin{itemize}
    \item Reference manuals
    \item Tutorials
    \item Questions and answers (Q\&A)
\end{itemize}

\paragraph{Reference manuals}
\begin{itemize}
    \item Authoritative source of information \\
    If the code does not do what the manual says, then the code is wrong.
    
    \item Must be complete
    
    \item Must use precise language, even at the cost of legibility
    
    \item Examples: ``man'' pages, C standard, IEEE-754 specifications
\end{itemize}

\paragraph{Tutorials}
\begin{itemize}
    \item Beginner-friendly
    
    \item Usually emphasize getting things to work quickly, even at the cost of completeness
    
    \item Good tutorials do not sacrifice accuracy (but many bad ones do)
    
    \item Examples: various books (K\&R C, Think Python) and intro material
\end{itemize}

\paragraph{Questions and answers (Q\&A)}
\begin{itemize}
    \item Prioritize quick answers to frequently asked questions
    
    \item Not exhaustive
    
    \item Examples: Stack Overflow, various FAQs
\end{itemize}
\paragraph{When reading documentation:}

\begin{itemize}
  \item as a beginner, aim for \textbf{tutorials} and Q\&As
  \item as you become an expert, you need a \textbf{reference manual}.
\end{itemize}

\paragraph{When writing documentation:}

\begin{itemize}
  \item ideally, you \textbf{write all three}!
\end{itemize}

\subsubsection{Automated documentation}

Automated documentation systems

\begin{itemize}
  \item read and parse source code
  \item find functions (methods, classes, \ldots)
  \item create a (PDF or webpage) document containing function signatures
  \item specially-formatted comments in the source code are copied into the documentation along with the corresponding function signatures
\end{itemize}

\subsubsection{Automated documentation systems}

\begin{itemize}
\item General:
    \begin{itemize}
    \item doxygen
    \item sphinx
    \end{itemize}
\item Python-specific:
    \begin{itemize}
    \item pdoc
    \item PyDoc
    \item pydoctor
    \end{itemize}
\end{itemize}

Note: Some projects choose to not use automated documentation.

\subsection{Testing}
\begin{lstlisting}
/*
 * This functions returns:
 * 5 if one or both of its arguments are 5
 * 0 otherwise
 */
int five_if_some_five(int a, int b)
{
    if (a != 5)
        a = 0;
    if (b != 5)
        b = 0;
    return a | b;
}

int tests()
{
    int errors = 0;
    errors += (five_if_some_five(100, 100) != 0);
    errors += (five_if_some_five(100,   5) != 5);
    return errors;
}
\end{lstlisting}

\subsubsection{Test coverage}
\begin{itemize}
    \item line coverage: \\
    is every line of code covered by some test case?
    \item branch coverage: \\
    for every conditional branch, is there a test covering each of the two possibilities \\
    (taking the branch or not taking it?)
\end{itemize}

After running the provided tests using coverage analysis tools, the results are as follows:

\begin{itemize}
    \item The \texttt{clang} command with flags \texttt{-Wall -O3 --coverage} was used to compile the source file \texttt{five.c} into the object file \texttt{five.o}, and to enable coverage analysis.
    \item Executing the compiled test binary \texttt{./test} resulted in zero errors, indicating that all the tests passed successfully.
    \item Coverage analysis was performed using \texttt{gcov}. The report generated by \texttt{gcov five.c} showed that 100\% of the lines were executed, suggesting full line coverage.
    \item When running \texttt{gcov -b five.c}, which includes branch coverage information, the report indicated that all branches were executed 100\% of the time, demonstrating full branch coverage.
    \item However, further detailed analysis using \texttt{gcov} revealed that the branches within the \texttt{five\_if\_some\_five} function were not equally covered. Specifically, the condition \texttt{if (a == 5)} was never true (0\% taken), indicating that the test cases did not cover the scenario where \texttt{a} is equal to 5.
    \item On the other hand, the branch \texttt{if (b == 5)} was taken 50\% of the time, corresponding to the test case where \texttt{b} is equal to 5.
\end{itemize}

\subsubsection{Line coverage vs. branch coverage}

\begin{lstlisting}
int test()
{
    int errors = 0;

    errors += (five_if_some_five(100, 100) != 0);

    return errors;
}
\end{lstlisting}

\begin{tabular}{ l l }
Line coverage: & 100\% \\
Branch coverage: & 50\%
\end{tabular}
\subsubsection{How does it work?}
Line coverage measures the percentage of code lines that have been executed during a test run. Line coverage does not ensure that all edge cases or logical branches are tested; it only indicates which lines of code were run.

\subsubsection{Limitations of test coverage measures (1)}
\begin{lstlisting}
/*
 This functions returns:
 5 if one or both of its arguments are 5
 0 otherwise
 */
int WRONG_five_if_some_five(int a, int b)
{
    return a | b;
}

int test()
{
    return (WRONG_five_if_some_five(0, 5) == 5);
}
\end{lstlisting}

Line coverage: 100\%
Branch coverage: 100\%
\paragraph{Explanation:}
Despite achieving 100\% line and branch coverage, the test case does not effectively validate the function's correctness. The function \texttt{WRONG\_five\_if\_some\_five} uses the bitwise OR operation, which does not implement the intended logic correctly. While the test case covers the scenario where one argument is 5 (yielding the expected result due to the nature of bitwise OR with 0 and 5), it fails to catch incorrect results for other inputs, like both arguments being 5 or neither being 5.

\subsubsection{Limitations of test coverage measures (2)}

The example illustrates that even with 100\% line and branch coverage, test coverage metrics can fail to ensure the correctness of the function. The function is intended to return 5 if any of the arguments is 5, and 0 otherwise. The test cases do not cover all possible paths, specifically the case where neither argument is 5. Consequently, it does not detect the flaw in the function logic, where the function incorrectly modifies the arguments and potentially returns the wrong result.

\subsubsection{Assertions}

\begin{itemize}
    \item Assertions are used to document (and check) assumptions made in the code.
    \item An assertion failure
    \begin{itemize}
        \item should correspond to a bug in your code,
        \item triggers an immediate crash (\texttt{abort()}) of your program.
    \end{itemize}
\end{itemize}
\begin{lstlisting}
#include <assert.h>

int gcd(int a, int b)
{
    if (a < b) {
        int t = a;
        a = b;
        b = t;
    }

    while (b != 0) {
        assert(a >= b); // <--- this should always be true
        int t = a % b;
        a = b;
        b = t;
    }

    return a;
}
\end{lstlisting}

\subsubsection{Disabling assertions}

\begin{lstlisting}
clang -DNDEBUG -Wall -O3 -o main main.c
\end{lstlisting}
(equivalent to \texttt{\#define NDEBUG} at the beginning of every file)

\subsubsection{Error vs assertion failure}

\begin{itemize}
  \item an error happens when, for external reasons, your program cannot run
  \begin{itemize}
    \item examples: out of memory, file cannot be read, network unreachable
  \end{itemize}
  \item an assertion fails if a fundamental assumption in your code is violated
  \begin{itemize}
    \item indicates a bug in your code
  \end{itemize}
\end{itemize}
\subsection{Static Analysis}

\begin{itemize}
  \item \textbf{Static analysis} operates on the source code
  \begin{itemize}
    \item (before any assembly or executable code is produced)
  \end{itemize}
  \item Compilers do advanced case analysis on the code
  \begin{itemize}
    \item (in order to produce faster code)
  \end{itemize}
  \item The same analysis can be used to find (potential) bugs

  \item Not an exact science
  \begin{itemize}
    \item Relies on heuristics to detect hazardous code
    \item Suffers from false negatives and false positives
  \end{itemize}
\end{itemize}
\subsubsection{Clang's static analyzer}
If you use a Makefile, run

\begin{lstlisting}
scan-build make
\end{lstlisting}
\subsubsection{Python linters}
\begin{itemize}
\item A ``linter'' is a static analyzer
\item Typically, linters enforce a specific coding style
\end{itemize}

Examples:
\begin{itemize}
\item Pylint
\item flake8
\item mypy (adds static type checking)
\end{itemize}
\paragraph{Example}
\begin{lstlisting}
def fib(n):
    a, b = 0, 1
    while a < n:
        yield a
        a, b = b, a+b
\end{lstlisting}

\begin{lstlisting}
def fib(n: int) -> Iterator[int]:
    a, b = 0, 1
    while a < n:
        yield a
        a, b = b, a+b
\end{lstlisting}
\subsection{Dynamic Analysis}
\begin{itemize}
    \item Dynamic analysis operates on the running executable (during testing)
        \begin{itemize}
            \item by adding runtime checks
            \item can find more bugs than static analysis...
            \item ... but only for those bugs are triggered by some test!
        \end{itemize}
\end{itemize}
\subsubsection{Sanitizers}

With \emph{sanitizers}, runtime checks are added by the \emph{compiler}.
\subsubsection{UBSan}

\begin{itemize}
    \item The ``undefined behavior sanitizer'' detects many types of undefined behavior (at runtime)
    \item triggers an immediate crash (with an explanation message)
    \item Pass \texttt{"-fsanitize=undefined"} to \texttt{gcc} or \texttt{clang}
\end{itemize}

\paragraph{Demonstration of UBSan in Action}

The code sample illustrates the usage of the Undefined Behavior Sanitizer (UBSan) by comparing the output of a program compiled without and with UBSan.

\begin{itemize}
    \item The function \texttt{f(int a, int b)} attempts to divide \texttt{a} by \texttt{b}. When compiled without UBSan and executed with \texttt{b} as zero, it results in a floating point exception. This is a runtime error that occurs due to division by zero, an undefined behavior in C.
    
    \item When compiled with UBSan using \texttt{clang -O3 -fsanitize=undefined -o timetravel timetravel.c}, the program terminates with a runtime error message indicating the cause of the error: division by zero. UBSan provides a stack trace pointing to the exact location in the code where the undefined behavior occurred.
    
    \item The output with UBSan includes details about the file name, line number, and even the assembly instruction pointer location, which are invaluable for debugging.
    
    \item UBSan causes the program to abort execution instead of continuing past the undefined behavior, which helps prevent further errors or unpredictable behavior in the program's execution.
\end{itemize}
\paragraph{Pros}

\begin{itemize}
    \item Fixes the anything-can-happen problem with undefined behavior (we get a crash with an explanation instead)
    \item No false positives
\end{itemize}

\paragraph{Cons}

\begin{itemize}
    \item Not all types of undefined behavior detected (most are)
    \item Does not always stop the compiler from exploiting undefined behavior
    \item Overhead (ca. 3x slowdown)
    \item Needs good tests
\end{itemize}

\subsubsection{ASan}

\begin{itemize}
    \item The ``address sanitizer'' detects many types memory access errors (at runtime)
    \item Separate from UBSan because it uses different mechanisms
    \item triggers an immediate crash (with an explanation message)
    \item Pass ``-fsanitize=address'' to gcc or clang
\end{itemize}

\paragraph{Example}
In the function \texttt{f}, a local buffer is declared, and a string is written into it. The function then returns a pointer to this local buffer, which is a mistake because the buffer's scope ends when the function returns, rendering the pointer invalid.

When the program is compiled without sanitization, this bug may go unnoticed, potentially leading to undefined behavior at runtime. However, when compiled with ASan using \texttt{clang -O3 -fsanitize=address -o bug bug.c}, the error is caught:

\begin{itemize}
    \item ASan outputs an error message indicating that there was a read of size 1 at a memory address that is part of the stack (i.e., the local \texttt{buffer}) after the scope of the \texttt{buffer} has ended.
    \item The error message includes a stack trace that shows where the invalid read occurred, in this case, the \texttt{puts} function called from \texttt{main}, which attempts to print the contents of the now non-existent buffer.
    \item The output also pinpoints the exact location in the source code where the invalid access occurs, aiding in debugging the issue.
\end{itemize}

\paragraph{ASan detects (1)}

\begin{itemize}
  \item Out-of-bounds accesses to heap, stack and globals
    \begin{lstlisting}
       int a[10];
       printf("%d\n", a[20]);
    \end{lstlisting}

  \item Use-after-free
    \begin{lstlisting}
       free(pointer);
       printf("%d\n", *pointer);
    \end{lstlisting}
\end{itemize}
\paragraph{ASan detects (2)}

\begin{itemize}
  \item Use-after-return
    \begin{lstlisting}
       int *f()
       {
         int a[10];
         return a;
       }
       
       void g()
       {
         int *pointer = f();
         printf("%d\n", pointer[0]);
       }
    \end{lstlisting}

  \item Use-after-scope
    \begin{lstlisting}
       void g()
       {
         int *pointer;
         
         if (1)
         {
           int a[10];
           pointer = a;
         }
          
         printf("%d\n", pointer[0]);
       }
    \end{lstlisting}
\end{itemize}
\paragraph{ASan detects (3)}

\begin{itemize}
    \item Double-free, invalid free
    
    \begin{lstlisting}
void *other_pointer = pointer;
free(pointer);
free(other_pointer);

int a[10];
free(a);
    \end{lstlisting}
    
    \item Memory leaks
    
    \begin{lstlisting}
void f()
{
    void *ptr = malloc(10);
}
    \end{lstlisting}
    
\end{itemize}

\textbf{Pros}
\begin{itemize}
    \item Detects most memory issues
    \item No false positives
\end{itemize}

\textbf{Cons}
\begin{itemize}
    \item Not every memory issue detected (many are)
    \item Overhead (ca. 2x slowdown)
    \item Needs good tests
\end{itemize}

\subsubsection{Valgrind}

\begin{itemize}
    \item Valgrind adds runtime checks on already-compiled executable.
    
    \item It is a hybrid interpreter / JIT compiler for machine code.
    
    \item It adds checks around all memory accesses.
    \begin{itemize}
        \item Detects uses of invalid pointers (incl. uninitialized memory)
        \item Detects memory leaks (at exit)
    \item Valgrind requires compiling with the ``-ggdb'' option (gcc / clang)
    \end{itemize}
\end{itemize}

\subsubsection{Pros}
\begin{itemize}
\item Detects almost all memory issues (that happen at runtime)
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
\item Large overhead ($\sim$10x slowdown)
\item Needs good tests
\end{itemize}

\subsection{Fuzzing}

\subsubsection{We need good tests}

\begin{itemize}
    \item Dynamic analysis tools are useful 
    \item but only if we have good test cases and enough of themm
    \item $\Rightarrow$ How do we generate good tests?
\end{itemize}

On a basic level, a fuzzer proceeds as follows:

\begin{enumerate}
    \item take a (mostly valid) example input file
    \item run the tested program with that input file
    \item check for crashes
    \item modify the input file:
    \begin{itemize}
        \item random modifications
        \item truncations, duplications
        \item mergers with other example input files
    \end{itemize}
    \item go back to 2
\end{enumerate}

\paragraph{Advanced fuzzers}

\begin{itemize}
    \item use test coverage techniques 
    to determine which input files are ``interesting'',
    in that they cover previously-uncovered source code
    \item use static analysis techniques 
    to determine input file modifications that could trigger specific code branches
\end{itemize}

\paragraph{AFL++}
\begin{itemize}
    \item open source project (\url{https://aflplus.plus/})
    \item takes as an input a directory with many (mostly valid) example input files
    \item generates modified input files that (try to) yield crashes
\end{itemize}






















\newpage
\section{Debugging}
\subsection{Debugging Techniques}

% Slide content for "Instrumentation"
\subsubsection{Instrumentation}
Instrumentation is a fundamental technique in debugging. It involves inserting additional code to monitor the execution of a program to ensure that the assumptions about the program's behavior hold true.\\

\textbf{Key concepts}
\begin{itemize}
    \item The primary goal is to confirm that what we \textit{think} is true is \textit{actually} true.
    \item It helps in pinpointing the exact location where execution diverges from our expectations.
    \item Common instrumentation techniques include:
    \begin{itemize}
        \item Assertions: \lstinline|assert| or \lstinline|assert()|
        \item Debugging messages: \lstinline|print()| or \lstinline|printf()|
        \item Machine-readable output for automated debugging tools.
    \end{itemize}
\end{itemize}

% Slide content for "Crash instrumentation example"
\subsubsection{Crash instrumentation example}

Instrumentation can be used to identify the point of failure in a sequence of actions:

\begin{lstlisting}[language=C]
void perform_actions(struct state *s)
{
    action_a(s);
    action_b(s);
    action_c(s);
    action_d(s);
    action_e(s);
}
\end{lstlisting}

By adding debugging messages, we can observe the execution flow:

\begin{lstlisting}[language=C]
void perform_actions(struct state *s)
{
    printf("Action A...\n");
    action_a(s);
    printf("Action B...\n");
    action_b(s);
    printf("Action C...\n");
    action_c(s);
    // If the program crashes after "Action C...", we know the issue is in action_c()
    printf("Action D...\n");
    action_d(s);
    printf("Action E...\n");
    action_e(s);
    printf("Actions done.\n");
}
\end{lstlisting}

\subsubsection{Machine-readable output example}
Machine-readable output is particularly useful for automated testing and debugging frameworks:

\begin{lstlisting}[language=Python]
def matrix_inverse(mtx):
    # ...
    return result
\end{lstlisting}

To verify the correctness of the output, we can add checks that output errors in a machine-readable format:

\begin{lstlisting}[language=Python]
def matrix_inverse(mtx):
    # ...
    error_matrix = mtx * result - matrix_identity()
    matrix_write(mtx, "mtx.m")
    matrix_write(result, "result.m")
    assert matrix_norm(error_matrix) < 1e-5
    return result
\end{lstlisting}

\textit{Note:} The example assumes that there are no undefined behaviors, such as accessing invalid memory, that could cause the program to crash before reaching the problematic code.


\subsubsection{How to handle large test cases?}
\begin{itemize}
    \item Suppose the \texttt{matrix\_inverse()} function has a bug.
    \item The issue is observed with a specific \(2000 \times 2000\) matrix.
    \item Directly visualizing such a large matrix to debug is impractical.
\end{itemize}

\paragraph{Strategies for Instrumentation}
\begin{itemize}
    \item Instrumenting \texttt{matrix\_inverse()} by printing the matrix at each step can be overwhelming due to the matrix's size.
    \item An effective approach is needed to handle the large size of the test case.
\end{itemize}

\subsubsection{Test Case Reduction}
Test case reduction is a technique to reduce the complexity of the test case while preserving the bug.\\

\textbf{Steps for Reduction}\\
Given a matrix \( A \in \mathbb{R}^{n \times n} \):
\begin{enumerate}
    \item Construct a smaller matrix \( B \in \mathbb{R}^{m \times m} \) by selecting a square submatrix of \( A \).
    \item Test \texttt{matrix\_inverse()} on \( B \).
    \item If \texttt{matrix\_inverse(B)} fails, replace \( A \) with \( B \) and repeat the process.
    \item Continue iteratively until a sufficiently small matrix that reproduces the bug is found.
\end{enumerate}

\textbf{Example Approach}
\begin{itemize}
    \item Initially, try removing a random half of the rows and columns of \( A \).
    \item If the problem persists, reduce the number of rows and columns incrementally.
    \item If necessary, narrow down to removing a single row and column.
\end{itemize}

\textit{Note:} This process can be automated, allowing for systematic and efficient debugging of large test cases.



\subsubsection{Code Bisection}

Code bisection is a systematic method for identifying the location of a bug in the code by repeatedly dividing the range of code under suspicion and verifying at each division point.\\

\textbf{Applying Code Bisection}
\begin{enumerate}
    \item Begin with a range of code where the bug is suspected.
    \item Insert debugging statements at the midpoint to determine if the first half is executed without error.
    \item Based on the outcome, narrow down the range to the half where the bug occurs.
    \item Repeat the process with the narrowed range until the exact location of the bug is identified.
\end{enumerate}

\textbf{Example of Code Bisection}\\
Consider a function \texttt{perform\_actions} which executes a series of actions:

\begin{lstlisting}[language=C]
void perform_actions(struct state *s)
{
    action_000(s);
    ...
    action_999(s);
}
\end{lstlisting}

To locate the bug, we instrument the code with print statements:

\begin{lstlisting}[language=C]
void perform_actions(struct state *s)
{
    printf("First action...\n");
    action_000(s);
    ...
    printf("Action 500...\n");
    action_500(s);
    // If a crash occurs after this print statement, the bug is between action 500 and 999.
    ...
    action_999(s);
    printf("Actions done.\n");
}
\end{lstlisting}

\textbf{Narrowing Down the Range}\\
Through iterative bisection, we can narrow the range further:

\begin{lstlisting}[language=C]
void perform_actions(struct state *s)
{
    // Inserting additional print statements between the narrowed range
    printf("Action 750...\n");
    action_750(s);
    // The crash between 750 and 999 indicates the half where the bug resides.
    ...
}
\end{lstlisting}

Continuing this process will eventually isolate the action causing the crash, allowing for a targeted investigation and resolution of the bug.



\subsubsection{Version bisection}
Version bisection is used to identify a change in the codebase that introduced a bug by checking out different commits and testing them.\\

\textbf{Using Git for Bisection}\\
The `git log` command is used to list commits in a concise format, and `git bisect` can automate the bisection process:

\begin{lstlisting}[language=bash]
git log --oneline
\end{lstlisting}

A list of commit messages may look like this:

\begin{lstlisting}
9e9e6fc (HEAD -> main, origin/main) Added perf version check.
ff3c21b Changed branch mispredict ratio displayed.
fd49f78 Silently ignore branch events.
85afe03 Support new perf-script brstack format with added spaces.
77f8759 Made perf script output parsing more lenient.
637f374 Version bump.
47b578b Fixed erroneous use of atime, should have been mtime.
1dadd0f Moved objdump cache to /tmp.
60a534a Added caching of objdump output.
6f3c377 Some debugging code.
b2daa9b Updated version.
\end{lstlisting}

To find the commit that introduced a bug, we can use `git bisect` to mark the current version as bad and an earlier version that is known to be good. Git will then checkout a commit halfway between the two, and we can compile and test that version:

\begin{lstlisting}[language=bash]
git bisect start
git bisect bad # mark the current version as bad
git bisect good b2daa9b # mark commit b2daa9b as good
\end{lstlisting}

\textit{Note:} You would continue this process, marking each tested commit as `good` or `bad`, until `git bisect` identifies the commit that introduced the bug.


\subsection{Debuggers}

\begin{itemize}
    \item A \textbf{debugger} is a tool that allows us to run our code step-by-step (e.g., line by line)
    \item Between each step, we can examine
    \begin{itemize}
        \item program \textbf{output}
        \item program \textbf{state} (i.e., variables)
    \end{itemize}
    \item Debuggers for interpreted languages are language-specific
    \item Debuggers for compiled languages work at the assembly level
\end{itemize}






















\newpage
\section{Pipelined CPUs, memory caches}

\subsection{Performance}

\subsubsection{What is performance?}

We want computers to perform required actions while minimizing their use of \textbf{resources}:
\begin{itemize}
    \item Time
    \item Power use (mobile, servers)
    \item Network use (mobile)
    \item Memory use (peak RAM usage)
    \item Storage (solid state drive / hard disk drive)
\end{itemize}

We care about those resources in proportion to their cost (financial, emotional, etc.)

\subsubsection{How do we achieve good performance?}
From high level to low level:
\begin{enumerate}
    \item Pick a good algorithm
    \begin{itemize}
        \item specifically, an algorithm with low computational complexity:
        \item $O(\log(n))$ better than $O(n^2)$ better than $O(n^6)$ better than $O(2^n)$
        \item we can hope for great improvements, $100\times$ faster, $1000\times$, etc.
        \item $\rightarrow$ first thing to try!
    \end{itemize}
    \item Pick an algorithm that is fast on the computers you use and implement it well
    \begin{itemize}
        \item this course!
        \item smaller improvements: from a few percent to $50\times$ faster
    \end{itemize}
    \item Translate the implementation into efficient instructions (compilers do that well)
\end{enumerate}
\subsubsection{What hides inside the big-O?}

Let us compare two algorithms:
\begin{itemize}
    \item Algorithm A has complexity $O(n^2)$
    \item Algorithm B has complexity $O(n \log(n))$
\end{itemize}

Specifically,
\begin{itemize}
    \item Algorithm A performs $2n^2 + 16$ operations
    \item Algorithm B performs $16n \log(n) + 64$ operations
\end{itemize}

For smaller n, Algorithm A is better even though it has a worse big-O scale.

\subsubsection{Which algorithm do we choose?}

Assume that
\begin{itemize}
    \item Algorithm A is insertion sort
    \item Algorithm B is merge sort
\end{itemize}

\subsubsection{Merge sort example}
The given charts illustrate the performance characteristics of two sorting algorithms, Algorithm A (Insertion Sort) and Algorithm B (Merge Sort), across different input sizes.

\begin{itemize}
    \item In the first chart, we see that for smaller values of \( n \), Algorithm A performs competitively, but as \( n \) increases, its performance degrades significantly compared to Algorithm B.
    \item The takeaway is that we should choose the right algorithm based on the size of the input: for smaller datasets, Algorithm A may be more efficient, whereas for larger datasets, Algorithm B is preferable.
    \item The second set of charts emphasizes that we can do even better by combining algorithms. For small datasets, we can use Algorithm A, and as the dataset grows, we can switch to Algorithm B. This combination approach leverages the strengths of both algorithms.
    \item The right chart in the second image shows that combining algorithms results in a performance that is close to the best of both worlds: the efficiency of Algorithm A for small datasets and the scalability of Algorithm B for larger ones.
\end{itemize}

This strategy is often employed in practice, such as in the implementation of sorting functions in standard libraries, which use a hybrid approach (like Timsort in Python and Java) that adapts to the nature of the dataset to provide optimal sorting performance.

\subsection{CPU Pipelines}
\subsection{Back to instructions}

Instruction decoding:

\begin{lstlisting}
48 2b 06        sub    rax, QWORD PTR [rsi]
48 0f af 06          imul   rax, QWORD PTR [rsi]
48 af af 02     imul   rax, QWORD PTR [rdx]
\end{lstlisting}

From \texttt{48 0f af 06}, the CPU needs to understand:

\begin{itemize}
  \item that it must perform a multiplication (as opposed to, say, a subtraction)
  \item that one term is the value of a 64-bit register, \texttt{rax}
  \item that the other term comes from memory: the 64-bit value pointer to by \texttt{rsi}
\end{itemize}


\paragraph{What happens after instruction decoding?}
\texttt{48 af af 06     imul   rax, QWORD PTR [rsi]]}
\begin{itemize}
    \item the CPU has Boolean circuitry to compute multiplications
    \item it must ensure that one of the two inputs of the multiplier is \texttt{rax}
    \item the CPU has Boolean circuitry to access memory
    \item it must ensure that the input of the memory circuitry is \texttt{rsi}
    \item it sets the second input of the multiplier to the output of the memory circuitry
    \item it stores the output of the multiplier back to \texttt{rax}
\end{itemize}
\textbf{It is no longer possible to do all this in a single cycle} \\
(e.g., at 4 GHz, i.e. 4 billion cycles per second, so in 0.25 ns)
\paragraph{Imagine that it takes:}
\begin{itemize}
    \item one cycle to decode an instruction
    \item one cycle to fetch data from memory
    \item one cycle to perform arithmetic
\end{itemize}

\paragraph{Sequential Execution of CPU Instructions}

The provided diagrams illustrate the sequential execution of two assembly instructions in a non-pipelined CPU architecture. Each instruction goes through the decode, memory, and arithmetic stages one at a time without overlap. Considering one cycle for each stage, the execution timeline is as follows:

\begin{enumerate}
    \item The first instruction, \texttt{sub rax, [rdi]}, involves subtracting the value at the memory location pointed to by \texttt{rdi} from the \texttt{rax} register. The execution stages are:
    \begin{itemize}
        \item Cycle 1: Decoding the \texttt{sub} instruction.
        \item Cycle 2: Fetching the operand from memory \texttt{[rdi]}.
        \item Cycle 3: Performing the subtraction and updating the \texttt{rax} register.
    \end{itemize}
    \item After the first instruction completes all its stages, the second instruction, \texttt{imul rbx, [rsi]}, which multiplies the \texttt{rbx} register by the value at the memory location pointed to by \texttt{rsi}, begins its execution:
    \begin{itemize}
        \item Cycle 4: Decoding the \texttt{imul} instruction.
        \item Cycle 5: Fetching the operand from memory \texttt{[rsi]}.
        \item Cycle 6: Performing the multiplication and updating the \texttt{rbx} register.
    \end{itemize}
\end{enumerate}

\textbf{Each instruction takes 3 cycles}

However, in this model,
\begin{itemize}
  \item while the memory circuitry is busy fetching QWORD PTR {[rsi]}, the multiplier is idle
  \item while the multiplier computes the result, the memory is idle
  \item during instruction decoding, everything else is idle
\end{itemize}
We can exploit this!
\subsubsection{Pipelined execution}

The images demonstrate the pipelined execution of instructions in a modern CPU. This execution model allows multiple instructions to be processed concurrently at different stages of the instruction cycle. Given the scenario where each stage (decode, memory access, arithmetic operation) takes one cycle to complete, the pipeline allows for a new instruction to enter the pipeline before the previous instruction has finished, thus improving the overall throughput of the system.

\begin{itemize}
    \item Initially, the pipeline is filled with the first instruction \texttt{sub rax, [rdi]} going through the decode, memory, and arithmetic stages.
    \item As soon as the decode stage is complete for the first instruction, the second instruction \texttt{imul rbx, [rsi]} enters the decode stage while the first instruction is in the memory stage.
    \item This process continues with the third instruction \texttt{add rcx, [rbp]} entering the pipeline.
    \item Once the pipeline is full, each cycle will see an instruction at each stage of execution, effectively executing multiple instructions simultaneously.
\end{itemize}

The pipelined execution is visualized in the images, with each horizontal level representing a cycle and each stage of instruction processing moving one step down the pipeline in each subsequent cycle. The advantage of this model is that the CPU does not have to wait for one instruction to complete all its stages before starting the next one, which significantly increases the instruction throughput.

\subsubsection{Throughput vs. latency}

\begin{itemize}
    \item Latency:
    \begin{itemize}
        \item Executing each instruction still takes 3 cycles!
    \end{itemize}
    \item Throughput:
    \begin{itemize}
        \item But on average, we execute up to 1 instruction per cycle.
    \end{itemize}
\end{itemize}

\subsubsection{Data dependencies}

The images depict the execution of two assembly instructions that have a data dependency. Data dependencies occur when an instruction requires the result of a previous instruction.

\begin{enumerate}
    \item The first instruction, \texttt{mul rcx, [rdx]}, performs a multiplication operation with the value at the memory location pointed to by \texttt{rdx} and stores the result in \texttt{rcx}.
    \item The second instruction, \texttt{add rdx, [rsi]}, adds the value at the memory location pointed to by \texttt{rsi} to the register \texttt{rdx}.
\end{enumerate}

Given the following conditions:
\begin{itemize}
    \item Each stage (decode, memory, arithmetic) takes one cycle to complete.
    \item An instruction must wait for the previous instruction to complete its memory and arithmetic stages if there is a data dependency.
\end{itemize}

The execution proceeds as follows:
\begin{itemize}
    \item The \texttt{mul} instruction is decoded first, then the corresponding value is fetched from memory, and finally, the multiplication is performed.
    \item The \texttt{add} instruction must wait until the \texttt{mul} instruction has completed its arithmetic stage before it can proceed with the memory fetch, as it depends on the updated value of \texttt{rdx}.
    \item This results in a stall in the pipeline, where the \texttt{add} instruction's decoding is delayed until the \texttt{mul} instruction's result is written back, ensuring the correct value is used for the addition.
\end{itemize}

The pipelined execution is disrupted by the data dependency, causing delays or stalls, which are illustrated in the sequence of images.

\subsubsection{Conditional branching}

Conditional branching poses a unique challenge in CPU instruction pipelines. When an \texttt{if} statement is encountered, the CPU must determine which path to follow – the true branch or the false branch. The provided diagrams illustrate the step-by-step decision-making process in a simplified CPU pipeline.

\begin{enumerate}
    \item Initially, the CPU decodes the comparison instruction \texttt{cmp rdi, rsi}. This instruction sets up the condition flags based on the comparison of the values in the \texttt{rdi} and \texttt{rsi} registers.
    
    \item Once the \texttt{cmp} instruction is decoded, the CPU moves on to decode the subsequent \texttt{jge} (jump if greater or equal) instruction. However, the jump cannot be performed until the result of the \texttt{cmp} instruction is known, which requires waiting for the arithmetic stage to complete.
    
    \item If the condition is true, the CPU follows the branch to the instruction labeled \texttt{.L1} (in this case, representing the code block \texttt{YYY}). Otherwise, it continues to the next instruction in sequence (\texttt{zzz}).
    
    \item During this decision phase, the pipeline may stall, waiting for the comparison result to decide the next instruction to fetch and decode. This waiting period is where branch prediction can play a significant role in maintaining pipeline efficiency.
\end{enumerate}
\subsubsection{Branch prediction}

\begin{itemize}
\item in practice, the CPU will try to predict which branch will be taken (based on past choices at that specific instruction)
\item and speculatively choose that branch
\end{itemize}


\paragraph{Example}
\begin{itemize}
    \item Initially, the CPU encounters a conditional branch instruction (\texttt{jge .L1}) and speculatively decodes the subsequent instructions (YYY) assuming the branch will not be taken.
    \item Meanwhile, the comparison instruction (\texttt{cmp rdi, rsi}) is executed to evaluate the actual condition. Until this condition is evaluated, the CPU continues to decode and execute the speculatively chosen path.
    \item If the branch prediction is correct, the CPU continues execution without any interruption, leading to higher performance due to reduced waiting time.
    \item However, if the prediction is incorrect, as depicted in the final images, the CPU must discard or "flush" the speculatively executed instructions (YYY) and redirect execution to the correct path (ZZZ). This action is known as a pipeline flush, which can cause a temporary decrease in performance due to the time taken to correct the path and refill the pipeline with the correct instructions.
\end{itemize}

The diagrams illustrate a scenario where the CPU's branch predictor incorrectly anticipates the direction of a branch, resulting in several cycles of mispredicted instruction execution. Once the correct path is determined (ZZZ), the pipeline is adjusted, but the mispredicted instructions (YYY) must be invalidated, showcasing the cost of a misprediction.

\subsubsection{In practice}
\begin{itemize}
    \item When all goes perfect, processors can actually execute more than one instruction per cycle
    \item Modern processor pipelines have between 5 and 40 stages
    \item At each stage, there are multiple circuitry blocks (decoders, arithmetic and logic unit (ALU) "ports", etc.)
    \item Branch mispredict penalty is typically $\geq 10$ cycles
    \item Main memory latency is 50--200 cycles
\end{itemize}
\subsubsection{How do we write good code?}
\begin{itemize}
    \item These parameters vary widely from CPU to CPU
    \item Specific characteristics are often not public
    \item It is almost impossible to predict the number of cycles a given set of instructions will take (in the presence of branches and memory accesses)
    \item $\Rightarrow$ Qualitatively: we try to \textit{understand} the phenomena at play
    \item $\Rightarrow$ Quantitatively: We \textit{measure} at runtime
\end{itemize}

\subsubsection{Out-of-order execution}

Out-of-order execution is an advanced CPU technique that improves the utilization of execution units by allowing instructions behind a slow-running instruction to be executed in advance if the operands they require are available.

\begin{itemize}
    \item In the given example, there are three assembly instructions: two addition instructions (\texttt{add rdx, rdi} and \texttt{add rcx, rbx}) and one memory read instruction (\texttt{mov rax, [rsi]}).
    
    \item The CPU starts by decoding the first instruction. If the operands are immediately available, it may dispatch this instruction to an Arithmetic Logic Unit (ALU) for execution.
    
    \item If the subsequent instruction does not depend on the first one's result and its operands are ready, the CPU may decode and dispatch this instruction to another ALU, even if the first instruction has not completed yet.
    
    \item The memory read instruction may be dispatched to the memory unit independently if the memory address is ready, potentially allowing all three instructions to be in various stages of execution simultaneously.
    
    \item This results in improved overall throughput as the CPU does not idle waiting for instructions to complete in program order. However, the CPU must ensure that the results are written back in the correct order to preserve the program's logical flow.
\end{itemize}

\subsection{Memory}

Access to memory (``random access memory'' or RAM) is slow

On desktop computers, RAM is typically on distinct integrated circuit (IC) packages,
physically centimeters away from the CPU.

Solution: \textbf{caching}

\subsubsection{Caching}

Level 1 (``L1'') cache:
\begin{itemize}
    \item The CPU contains a \textbf{small amount of extremely fast memory}
    \item This memory \textbf{requires many of logic gates} on-package
    \item But it is \textbf{always available} (no latency)
    \item The CPU contains logic to decide which part of the main memory gets stored in its L1 cache
    \item This continuously changes over time
\end{itemize}

Level 2 (``L2'') cache:
\begin{itemize}
    \item slower than L1
    \item but requires fewer logic gates, so we can have more
\end{itemize}

Level 3 (``L3'') cache:
\begin{itemize}
    \item slower than L2
    \item but requires fewer logic gates, so we can have more
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Zen 4 Cache} & \textbf{L1I cache} & \textbf{L1D cache} & \textbf{L2 cache} & \textbf{L3 cache} \\ \hline
Cache size           & 32kB               & 32kB                & 1MB               & xxx               \\ \hline
Associativity        & 8 way              & 8 way               & 8 way             & xxx               \\ \hline
Cache line size      & 64 b               & 64 b                & 64 b              & 64 b              \\ \hline
\end{tabular}
\caption{Cache configuration for Zen 4 architecture.}
\label{table:zen4cache}
\end{table}
\subsubsection{Typical configuration}

\begin{itemize}
    \item memory transits through in units of one cache line
    \begin{itemize}
        \item 64 bytes on x86\_64
        \item 128 bytes on M1 Macs
    \end{itemize}
    \item there is no concept of locality beyond cache lines
    \item every memory access is performed through L1 cache
    \item when all cache entries are full, we need to overwrite one
    \begin{itemize}
        \item $\rightarrow$ cache eviction policies e.g. least-recently used (LRU)
    \end{itemize}
    \item pipelined CPUs feature a memory prefetcher (speculatively fills caches in advance)
\end{itemize}

\subsubsection{How do we write good code?}

\begin{itemize}
    \item Again, cache operation varies widely from CPU to CPU
    \item It is almost impossible to predict how it will behave with complex instruction streams
    \item $\Rightarrow$ Qualitatively: we try to \textbf{understand} how caches work
    \item $\Rightarrow$ Quantitatively: We \textbf{measure} at runtime
\end{itemize}






















\newpage
\section{Benchmarking and static instrumentation}

\subsection{More caches and pipelines}
Virtualization of memory involves translating virtual addresses to hardware addresses, which is essential for every memory access operation. This translation relies on a page table, which is itself stored in memory. This process might suggest the need for two memory accesses per memory fetch instruction, which is inefficient.

\textbf{Solution:} To optimize this, a portion of the page table is cached in the CPU in the Translation Lookaside Buffer (TLB), reducing the frequency of memory accesses.

\subsubsection{Cache latency}
Caches are implemented at various levels to mitigate access latency. Below are some typical latencies for different technologies and operations:


\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{Typical Latency} & \\
\midrule
Instruction & $\sim0.25$ ns & $0.25$ ns \\
RAM & $\sim100$ ns & $100$ ns \\
Solid State Drive (SSD) & $\sim0.2$ ms & $200,000$ ns \\
Hard Disk Drive (HDD) & $\sim2$ ms & $2,000,000$ ns \\
Wired Ethernet (round-trip) & $\sim1$ ms & $1,000,000$ ns \\
WiFi Latency (round-trip) & $\sim10$ ms & $10,000,000$ ns \\
Same-city Internet (round-trip) & $\sim5$ ms & $5,000,000$ ns \\
Same-continent Internet (round-trip) & $\sim25$ ms & $25,000,000$ ns \\
Transatlantic Internet (round-trip) & $\sim100$ ms & $100,000,000$ ns \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Examples of caches}
Caches are ubiquitous in computing and networking:

\begin{itemize}
    \item SSDs have internal RAM caches, typically ranging from 0-4 GB.
    \item Operating systems cache files in memory to speed up access.
    \item Large content providers, such as Google, Amazon, Netflix, and Cloudflare, implement extensive caching mechanisms globally.
\end{itemize}

\textbf{Real-world Caching Example}\\
Here is an example showing the output of pinging servers located in different parts of the world:

\begin{lstlisting}
ping canada.ca
PING canada.ca (205.193.117.159): 56 data bytes
64 bytes from 205.193.117.159: icmp_seq=0 ttl=228 time=20 ms
... (similar lines omitted for brevity) ...
64 bytes from 205.193.117.159: icmp_seq=4 ttl=228 time=18 ms

ping google.com.au
PING google.com.au (142.251.209.3): 56 data bytes
64 bytes from mi104s04-in-f3.1e100.net (142.251.209.3): icmp_seq=0 ttl=115 time=12.2 ms
... (similar lines omitted for brevity) ...
64 bytes from mi104s04-in-f3.1e100.net (142.251.209.3): icmp_seq=4 ttl=115 time=11.8 ms
\end{lstlisting}

\textbf{Key points}
\begin{itemize}
    \item Understand the role of the Translation Lookaside Buffer (TLB) in reducing memory access latency.
    \item Be able to compare the latencies of different memory and storage technologies, from CPU instructions to transatlantic internet round-trips.
    \item Recognize the impact of caching at various system levels, including hardware (like SSDs) and software (like operating system file caches).
    \item Consider the real-world implications of caching, such as the performance differences observed when accessing local vs. remote resources.
\end{itemize}


\subsubsection{Examples of pipelines}
\begin{itemize}
\item Storage devices:
\begin{itemize}
    \item SSDs typically access data in “pages” of 4096 bytes
    \item 0.2ms SSD latency would imply a max speed of 20 MB/s
    \item instead SSDs routinely read and write 500 MB/s
\end{itemize}
\item Networks:
\begin{itemize}
    \item Network packets are typically 1500 bytes
    \item 10ms WiFi latency would imply 150 KB/s
    \item instead most WiFi networks do at least 10 MB/s
\end{itemize}
\item Browsers:
\begin{itemize}
    \item Google Chrome maintains up to 6 connections per domain
\end{itemize}
\end{itemize}


% We begin with a basic structure for a LaTeX document, such as article class.
% Since the instructions specify not to add document creation commands, they are omitted here.
% We will instead focus solely on the body content.

% The slide title "BENCHMARKING" is formatted as a section.
\subsection{Benchmarking}

When profiling applications in Unix-like systems, the `time` command is used to measure the duration of program execution.\\

\textbf{Time Command Output}\\
Running the `time` command with an application gives us three key pieces of information:
\begin{itemize}
    \item \textbf{real}: This is the elapsed "real" time or wall-clock time from start to finish of the call.
    \item \textbf{user}: This time is spent in user mode, that is, the time the CPU takes to run the application code.
    \item \textbf{sys}: This is the time spent in system mode, which means the time the CPU takes to run OS kernel operations on behalf of the application.
\end{itemize}

It's important to note that the sum of user and system times (\(user + sys\)) will typically be less than the real time, as real time also includes time spent waiting for I/O or other applications.

\subsubsection{Variance in Execution Time}
Execution time can vary between runs due to a variety of factors, such as system load, I/O overhead, or CPU scheduling. Variance is observed when running the same command multiple times:

\begin{lstlisting}
time ./application
\end{lstlisting}

For example, running a command to read from `/dev/random` may yield different `real`, `user`, and `sys` times upon repeated executions.\\

\textbf{Examining Variance}\\
To understand the stability and performance of a system or application, examine the variance in execution time over multiple runs. For instance, running the following command multiple times:

\begin{lstlisting}
time head -n 1000000 /dev/random > /dev/null
\end{lstlisting}

will provide different execution times, which is an example of the inherent variance in system performance.


% subsubsection for reasons of variance
\subsubsection{Reasons for variance}
Variability in execution time can often be attributed to several factors, which include but are not limited to:

\begin{itemize}
    \item \textbf{Power and Temperature Throttling:}
    Modern CPUs can adapt their operating speed to avoid overheating, leading to performance changes.
    
    \item \textbf{Interactions with Devices:}
    The operating system has in-memory caches for files, and storage devices have internal memory caches, which can affect the timing of operations.
    
    \item \textbf{Other Processes:}
    The necessity to share system resources among multiple processes can introduce variability in the execution time of a program.
\end{itemize}


% Effect of file caches section
\subsubsection{Effect of file caches}
File caching by the operating system can have a significant effect on the performance of file operations. Below is an example showing the execution of the `md5sum` command on a large file, first without file caches and then with the file cached in memory:

\begin{lstlisting}
time md5sum 2GB_file
\end{lstlisting}

In this example, the second run of `md5sum` on the same file is faster due to the effect of file caching.

% Inaccuracies section
\subsubsection{Inaccuracies}
\begin{itemize}
    \item executable startup is slow
    \item initialization adds overhead
    \item input and output are slow
\end{itemize}

% Executable startup is slow section
\paragraph{Executable startup is slow}
The process of starting an executable can introduce overhead that affects the accuracy of performance benchmarking, especially for applications that only run for a few milliseconds.

\begin{itemize}
    \item Compiling and running an empty program in C still incurs a measurable startup time.
    \item Interpreted languages like Python also have a significant startup time even for executing a simple exit command.
\end{itemize}


\paragraph{Initialization adds overhead}
Initial setup or parsing steps in applications add overhead, which might dominate the execution time for short-running processes.

\begin{itemize}
    \item For example, timing the execution of a simplex algorithm might inadvertently measure the time taken by the file parser rather than the algorithm itself.
\end{itemize}

\paragraph{Input and output are slow}
I/O operations can be a major source of slowdown in applications.

\begin{itemize}
    \item A Python function calculating the Riemann zeta function demonstrates this when its output is printed iteratively.
    \item The execution time drastically increases when printing results inside the loop compared to a single print at the end.
\end{itemize}

\textbf{Takeaway}\\
It is essential to understand that benchmarking should focus on the algorithm or the specific task of interest, avoiding extraneous operations that do not contribute to the performance measurement of the targeted task.

\subsubsection{Recommendations for Accurate Benchmarking}
\begin{itemize}
    \item Isolate the core functionality from initialization and termination procedures when timing an application.
    \item Minimize I/O operations during timing, especially when dealing with high-performance code.
    \item Be aware of the environment and system state, as concurrent processes and system load can impact measured performance.
\end{itemize}
\subsubsection{Aggregate measures}

\begin{itemize}
    \item if we benchmark our code on different inputs, we may want to use
    \begin{itemize}
        \item total time / average time
        \item geometric mean
        \item or other aggregate measures
        \item or some visualization (bar graphs, performance profiles, etc.)
    \end{itemize}
    \item but beware: all aggregate measures are biased
\end{itemize}

\medskip
\begin{tabular}{lcccc}
    \toprule
    & Input 1 & Input 2 & Input 3 & Average \\
    \midrule
    Version A & 2530s & 2300s & 12s & 1614s \\
    Version B & \begin{tabular}[c]{@{}c@{}}2535s\\1.002x\end{tabular} & \begin{tabular}[c]{@{}c@{}}2304s\\1.002x\end{tabular} & \begin{tabular}[c]{@{}c@{}}6s\\0.5x\end{tabular} & 1615s \\
    \bottomrule
\end{tabular}





\subsection{Static instrumentation}
When it comes to performance optimization, it is crucial to focus on specific parts of the code. Static instrumentation is a method for inserting code to measure the execution time of particular segments directly.

\subsubsection{{Why Benchmark Specific Parts of Code?}}
Benchmarking specific parts of code is important for several reasons:
\begin{itemize}
    \item To bypass the time consumed by executable startup, initialization, and I/O operations, which can obscure the true performance of the code segments.
    \item To accurately measure the performance of code sections that execute quickly and might otherwise be overshadowed by the startup overhead.
    \item To identify bottlenecks, which are the parts of the code that significantly impact the overall performance.
\end{itemize}

\paragraph{{Adding Timing Instrumentation}}
To obtain accurate measurements, developers should add timing code around the functions or blocks of interest. This allows for granular analysis of where most of the execution time is being spent.


\subsubsection{About bottlenecks}

Donald Knuth famously said, "Premature optimization is the root of all evil." This highlights the importance of identifying true bottlenecks before optimization:

\begin{itemize}
    \item A small section of code could be responsible for a disproportionate amount of the execution time, which could be a target for optimization.
    \item Focusing on optimizing non-critical parts of the code without understanding the actual performance impact can lead to wasted effort and complexity.
\end{itemize}

\paragraph{Identifying Bottlenecks with Static Instrumentation}
For instance, consider a program with three functions where static instrumentation has revealed the following time consumption:

\begin{itemize}
    \item \texttt{function\_A()} takes up 12\% of the time, with 500 lines of code.
    \item \texttt{function\_B()} consumes 60\% of the time, but it only has 20 lines of code.
    \item \texttt{function\_C()} uses 18\% of the time for its 80 lines of code.
    \item All other code combined takes up the remaining 10\% of the time.
\end{itemize}

With this information, it is clear that \texttt{function\_B()} is the primary bottleneck and should be the focus of optimization efforts. Despite its small size, it has the greatest impact on performance.


\subsubsection{Using time.time()}
In Python, `time.time()` is used to get the wall-clock time at various points during execution. Here's an example of how to use it:

\begin{lstlisting}
import time
t0 = time.time()
initialize()
# ... run various functions ...
cleanup()
t5 = time.time()
print(f"Total time: {t5 - t0}")
\end{lstlisting}

This method is simple but has limitations due to the granularity and precision of the time measured.


\subsubsection{Using clock\_gettime()}
In C, `clock$\_$gettime()` provides a more precise measurement of execution time. It can be used like this:

\begin{lstlisting}
struct timespec t0, t1, t2, t3, t4, t5;
clock_gettime(CLOCK_MONOTONIC, &t0);
initialize();
# ... run various functions ...
cleanup();
clock_gettime(CLOCK_MONOTONIC, &t5);
print_all_clocks(&t0, &t1, &t2, &t3, &t4, &t5);
\end{lstlisting}

"CLOCK\_MONOTONIC" ensures that the time measured is not affected by discontinuous jumps in the system time.



\subsubsection{Cumulative time}
To measure cumulative time across multiple iterations or function calls, you can sum the time differences for each specific part:

\begin{lstlisting}
for i in range(1000000):
    t0 = time.time()
    function_A()
    tA += time.time() - t0
    # ... similarly for functions B and C ...
\end{lstlisting}

\paragraph{Caveats in Timing}
\begin{itemize}
    \item Measuring time itself takes time, which can introduce an overhead (~40 ns for `time.time()`).
    \item The actual time may fluctuate due to various system factors.
\end{itemize}

\subsubsection{Microbenchmarks}

For functions that execute very quickly, microbenchmarking can be used to get a more accurate measure by running the function many times and measuring the total time:

\begin{lstlisting}
tA, tB, tC = 0
t0 = time.time()
for i in range(5000000):
    function_A()
t1 = time.time()

cleanup()
\end{lstlisting}

\paragraph{Key Takeaways}
\begin{itemize}
    \item When benchmarking, it's important to isolate the part of the code you're interested in.
    \item Take into account the overhead introduced by the timing mechanism itself.
    \item Use cumulative timing and microbenchmarking for a more precise performance analysis, especially for quick-executing code.
\end{itemize}

\subsubsection{Microbenchmarks limitations}

\begin{itemize}
    \item It may not make sense to call \texttt{function\_A()} in isolation
    \begin{itemize}
        \item Take \texttt{sin(x)} for example: which value of x do we choose?
        \item Always the same?
        \begin{itemize}
            \item Are we sure \texttt{sin(0)} takes as much time as \texttt{sin(0.1)}?
        \end{itemize}
        \item A random value for x?
        \begin{itemize}
            \item What if generating pseudo-random values takes more time than \texttt{sin()}?
        \end{itemize}
    \end{itemize}
    \item What about caches?
    \begin{itemize}
        \item Caches will be "hot" (already filled with relevant data)
        \item Microbenchmarking presents an over-optimistic picture of memory access times
    \end{itemize}
\end{itemize}


\subsection{Automated instrumentation: profilers}

\subsubsection{gprof}
`gprof` is a profiling tool that helps identify sections of code that take up significant execution time. It automatically instruments the code, collects data on the program's performance, and generates a report.

\paragraph{Using gprof}
To use `gprof`, follow these steps:
\begin{enumerate}
    \item Compile the program with profiling enabled using `-pg` option:\\
    \texttt{gcc -O3 -o app app.c -pg}
    \item Run the application normally:\\
    \texttt{./app}
    \item Generate the performance report:\\
    \texttt{gprof app}
\end{enumerate}

The generated report includes a flat profile and a call graph that show the time spent in each function.

\paragraph{Understanding gprof Output}
The `gprof` output report contains:
\begin{itemize}
    \item A flat profile with the time spent in each function.
    \item A call graph showing the relationships between functions and the time spent in each call.
\end{itemize}

\subsubsection{Pros and Cons of gprof}
\textbf{Pros:}
\begin{itemize}
    \item It is easy to use and integrates well with existing code bases.
    \item Provides exhaustive profile information, including time spent in each function and the call hierarchy.
    \item Generally introduces low overhead to the application's runtime.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item The overhead can increase when bottlenecks are in small, short functions due to the granularity of the profiling.
    \item The accuracy of time measurement may be limited, and the actual time spent can be affected by the profiling itself.
\end{itemize}

\paragraph{Key Takeaways}
\begin{itemize}
    \item Understand how to compile and run a program with `gprof` to collect profiling data.
    \item Be familiar with interpreting the `gprof` report to identify performance bottlenecks.
    \item Recognize the advantages of using automated tools like `gprof` for profiling, as well as their limitations.
\end{itemize}






















\newpage
\section{Stochastic Instrumentation}

\subsection{Hardware performance counters}

The simplest hardware-aided performance-measuring tool is:
the \textbf{time stamp counter (TSC)}
\begin{itemize}
    \item Introduced by \textbf{Intel} with the Pentium architecture (1993)
    \item Similar feature available on \textbf{ARM} since ARMv7 (1996)
    \item Special integer register
    \item Incremented by one at a constant rate (e.g. every clock cycle)
    \item Reading this register has high latency ($>$10 cycles)
    \item Useful for microbenchmarks and instrumentation
    \item \texttt{time.time()} / \texttt{clock\_gettime()} use this internally
\end{itemize}
\subsubsection{More complex performance counters}

Since then, Intel and ARM have added many more performance counters:

\begin{itemize}
  \item executed (“retired”) instructions
  \item branches
  \begin{itemize}
    \item successfully predicted
    \item mispredicted branches
  \end{itemize}
  \item memory accesses
  \begin{itemize}
    \item found in L1 cache
    \item L1 misses, found in L2 cache
    \item L2 misses, found in (last-level) L3 cache
    \item L3 misses, found in main memory
  \end{itemize}
  \item TLB (page table cache) hits
  \item TLB misses
\end{itemize}

\begin{itemize}
  \item Pros
  \begin{itemize}
    \item always measured
    \item no performance penalty
    \item no interference with normal execution
  \end{itemize}
  \item Cons
  \begin{itemize}
    \item only an aggregate measure (totals)
  \end{itemize}
\end{itemize}

\subsubsection{Linux perf}

\texttt{perf stat ./application} gives various performance metrics for ./application, like cycles, instructions, etc. 

\subsection{Stochastic Instrumentation}

\subsubsection{Limitations of performance counters}

\begin{itemize}
    \item How could we find \textbf{hot spots}\\
    (small groups of instructions that the application spends a lot of time running)
    
    \item What about performance counts (cache misses, mispredicted branches,\ldots)
    at those \textbf{hot spots}?
    
    \item Instrumentation is expensive (and affects accuracy)
\end{itemize}

\paragraph{Solution: stochastic instrumentation}

\begin{itemize}
    \item every N cycles (e.g., every 1,000,000th cycle / every 0.1ms), a \textbf{sample} is taken
    \item the \textbf{sample} records:
    \begin{itemize}
        \item which instruction is currently being executed
        \item optionally, what it is waiting for (instr. decoding, pipeline bubble, memory access,\ldots)
        \item optionally, instruction addresses of the last few branches
        \item optionally, whether those branches were successfully predicted
    \end{itemize}
\end{itemize}

\subsubsection{Stochastic instrumentation}
\begin{itemize}
    \item Pros
    \begin{itemize}
        \item no performance penalty
        \item no interference with normal execution
        \item accuracy naturally increases on hotspots
    \end{itemize}
    \item Cons
    \begin{itemize}
        \item none
    \end{itemize}
\end{itemize}

\subsubsection{Analysis applications}

\begin{itemize}
  \item Linux
  \begin{itemize}
    \item \texttt{perf record / perf report}
    \item KDAB hotspot
  \end{itemize}
  \item MacOS: Apple XCode Instruments
  \item Windows: Visual Studio (``dynamic instrumentation'' / ``collection via sampling'')
  \item Intel-specific: vTune
  \item AMD-specific: uProf
\end{itemize}

\subsubsection{Bottom-up analysis}
[TBD] - Placeholder for the bottom-up analysis image from the Intel VTune Profiler

\subsubsection{Flame graphs}
[TBD] - Placeholder for the flame graphs image






















\newpage
\section{Data structures: Arrays, Linkned Lists}

\subsection{Abstract Data Types and Data Strucutres}

An abstract data type (ADT) is a model for data containers that defines the type's behavior in terms of possible values, operations, and behavior of those operations. Examples include:

\begin{itemize}
    \item In Python: \texttt{list}, \texttt{dict}, \texttt{set}, etc.
    \item In C++: \texttt{std::vector}, \texttt{std::unordered\_map}, etc.
\end{itemize}

ADTs specify the operations supported but not the implementation details, such as how data is stored or how the operations are executed.

\subsubsection{Data Structure Implementation}
A data structure is a concrete implementation of an ADT that details how data is arranged in memory and the specific algorithms used for operations, which allows us to compute the computational complexity.


\subsection{Lists}
Lists represent one of the simplest forms of ADTs, essentially a collection of ordered elements that support operations like:
\begin{itemize}
    \item Storing multiple elements together.
    \item Optionally appending, inserting, or deleting elements.
    \item Optinally accessing or modifying all elements in sequence or at a specific index (with varying efficiency).
\end{itemize}

\subsection{Arrays}
\subsubsection{Static arrays}
Static arrays are fixed-size, contiguous memory data structures that provide:

\begin{itemize}
    \item Constant-time (\(O(1)\)) access or modification of elements via an arbitrary index.
    \item Linear-time (\(O(n)\)) traversal to access or modify all elements due to the direct addressing capability.
\end{itemize}

\subsubsection{Dynamic arrays}
Dynamic arrays extend static arrays to allow variable size \( n \), which introduces additional complexity:

\begin{itemize}
    \item Theoretical complexity \( O(n) \) for operations that change the size of the list.
    \item This affects the complexity of appending, inserting, or deleting elements.
\end{itemize}

\paragraph{Key Takeaways}
\begin{itemize}
    \item ADTs define the what, not the how, of data manipulation.
    \item Data structures implement the specifics of ADTs and determine the efficiency of operations.
    \item Understanding the difference between ADTs and data structures is crucial for choosing the right implementation for the needed operations, especially considering the performance implications.
\end{itemize}

\subsubsection{Size increase}

\begin{itemize}
  \item An array occupies the bytes in memory:
  \begin{itemize}
    \item from array\_address
    \item to \( \text{array\_address} + n \times \text{element\_size} - 1 \)
  \end{itemize}
  \item Increasing \( n \) has \( O(n) \) complexity, because the memory at
  \[ \text{array\_address} + n \times \text{element\_size} \]
  may be occupied by other data
  \item In that case, the dynamic array must be relocated elsewhere in memory (changing \( \text{array\_address} \))
  \item All \( n \times \text{element\_size} \) bytes must be copied to the new location, hence \( O(n) \) complexity
\end{itemize}




\subsubsection{Size decrease}

\begin{itemize}
    \item Conversely, if the memory before and/or after an array is free,
    \begin{itemize}
        \item we may want to move the array
        \item in order to create a larger block of free memory
    \end{itemize}
    \item Not doing this may cause ``memory fragmentation''
\end{itemize}

\noindent In theory:

\begin{center}
\begin{tabular}{l c}
\hline
operation & complexity \\
\hline
access/modify element at arbitrary index & $O(1)$ \\
increase $n$ & $O(n)$ \\
decrease $n$ & $O(n)$ \\
append an element & $O(n)$ \\
discard last element & $O(n)$ \\
insert an element & $O(n)$ \\
delete an element & $O(n)$ \\
\hline
\end{tabular}
\end{center}

\noindent In practice:

\begin{itemize}
    \item Almost all implementations ignore fragmentation due to shrinking \\
    (no move when decreasing $n > 0$)
\end{itemize}

\begin{center}
\begin{tabular}{l c}
\hline
operation & complexity \\
\hline
access/modify element at arbitrary index & $O(1)$ \\
increase $n$ & $O(n)$ \\
decrease $n$ & $O(1)$ \\
append an element & $O(n)$ \\
discard last element & $O(1)$ \\
insert an element & $O(n)$ \\
delete an element & $O(n)$ \\
\hline
\end{tabular}
\end{center}

\subsubsection{Over-allocation}
Over-allocation is a strategy used in managing dynamic arrays where more memory is allocated than is immediately necessary.

\begin{itemize}
    \item We distinguish between two quantities:
    \begin{itemize}
        \item The user-visible size \( n \): the number of elements the user thinks the array can hold.
        \item The allocated size \( a \): the actual amount of memory reserved for the array.
    \end{itemize}
    \item When the user requests an increase in the array size to \( n' \), if \( n' \leq a \), the dynamic array can accommodate the new size without needing additional memory allocation.
    \item The allocated size \( a \) is not incremented by small amounts (no \( a' = a + 1 \)).
    \item Instead, to optimize performance and minimize frequent allocations, \( a \) is typically increased exponentially, often doubling (\( a' = 2a \)) when the array needs to grow. This approach reduces the number of times the array must be resized and copied over the lifetime of the array.
\end{itemize}

\paragraph{Benefits of Over-Allocation}
\begin{itemize}
\item \textbf{Efficiency:} By allocating more space than immediately necessary, we reduce the frequency of memory allocation operations, which are costly in terms of performance.
\item \textbf{Growth Management:} Over-allocation manages the growth of an array more smoothly, preventing the system from reallocating memory for every single element added once the initial capacity is exceeded.
\end{itemize}

\paragraph{Implications of Over-Allocation}
\begin{itemize}
\item While over-allocation can improve efficiency, it does mean that the data structure may use more memory than what is strictly required for its current elements.
\item This trade-off between memory usage and performance is often acceptable, especially since modern computing devices typically have memory resources to spare.
\end{itemize}



\subsubsection{Illustration of Exponential Allocation}
\begin{itemize}
    \item Initially, an array may have an allocated size (\( a \)) larger than the user-visible size (\( n \)).
    \item As elements are added and \( n \) grows, if \( n \) exceeds \( a \), the array is reallocated with twice the previous allocated size (\( a' = 2a \)).
    \item This approach is visualized in the sequence of diagrams, where the allocated size (\( a \)) increases to accommodate the growing number of elements (\( n \)).
\end{itemize}

\paragraph{Exponential Allocation in Practice}
\begin{itemize}
    \item When an array with \( n = 3 \) grows to \( n = 4 \), the previously allocated size (\( a = 4 \)) is sufficient.
    \item However, when \( n \) reaches \( 5 \), the array is reallocated with \( a = 8 \), doubling the previous allocation to maintain the exponential growth strategy.
    \item This pattern continues as \( n \) grows, with the next reallocation occurring when \( n \) exceeds \( 8 \), increasing \( a \) to \( 16 \).
\end{itemize}

\subsubsection{Pros and cons of exponential allocation}

\textbf{Pros:}
\begin{itemize}
    \item Reduces the frequency of memory reallocations.
    \item Ensures that the allocated size \( a \) is always within a factor of the actual size \( n \), specifically \( a \leq 2n \).
    \item Provides an \( O(1) \) amortized time complexity for increasing the size of the array.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item Can lead to memory waste as the allocated space might not always be fully utilized.
    \item The wasted space is bound by the equation \( a = 2^{\lceil \log_2(n) \rceil} \), which can be significant for large arrays.
\end{itemize}



\subsubsection{Complexity of exponential allocation}
\begin{itemize}
    \item Starting with an empty array and incrementing its size \( n \) times leads to at most \( k = \lceil \log_2(n) \rceil \) memory allocation moves.
    \item The total cost of these moves is a power series sum: \( 1 + 2 + 4 + \ldots + 2^{k-1} = 2^k - 1 \).
    \item This sum is less than or equal to \( 2n \), ensuring that the total time complexity for \( n \) size increments is \( O(n) \).
    \item The amortized time complexity for each size increment is \( O(1) \).
\end{itemize}

\paragraph{Operational Complexities}
\begin{itemize}
    \item Access or modification at an arbitrary index: \( O(1) \).
    \item Increase or decrease \( n \): \( O(1) \) amortized.
    \item Append an element: \( O(1) \) amortized.
    \item Discard the last element: \( O(1) \).
    \item Insert or delete an element: \( O(n) \) due to the need to shift elements.
\end{itemize}

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{operation} & \textbf{complexity} \\
\hline
access/modify element at arbitrary index & $O(1)$ \\
increase $n$ & $O(1)$ amortized \\
decrease $n$ & $O(1)$ \\
append an element & $O(1)$ amortized \\
discard last element & $O(1)$ \\
insert an element & $O(n)$ \\
delete an element & $O(n)$ \\
\hline
\end{tabular}
\end{center}

\subsubsection{Virtual memory}
Virtual memory changes the complexity landscape for operations on dynamic arrays by allowing us to remap memory instead of physically moving it.

\paragraph{Virtual Memory in Asymptotic Complexity}
\begin{itemize}
    \item The asymptotic complexity of changing the size \( n \) of an array involves an \( O(n) \) operation due to memory moves.
    \item However, with virtualized memory, physical byte movement is unnecessary.
    \item Instead, the page table remaps the physical memory associated with a virtual address to a new virtual address, which in practice, can make memory moves essentially \( O(1) \).
\end{itemize}

\subsubsection{Remapping Memory Using the Page Table}
\begin{itemize}
    \item \textbf{Pros:} Memory move becomes \( O(1) \) in practice due to virtual memory's ability to remap addresses instead of moving bytes.
    \item \textbf{Cons:} It requires a system call to the OS kernel to change the page table, which introduces:
    \begin{itemize}
        \item A context switch, potentially polluting the CPU caches.
        \item A significant fixed cost due to the system call overhead.
    \end{itemize}
    \item This operation is therefore only performed when \( n \) grows very large, avoiding frequent system calls.
    \item To manage growth, \( a' \) is set to \( a + K \) for some large \( K \), to avoid waste of exponential decrease.
\end{itemize}

For very large \( n \) (in the order of multiple megabytes), the operational complexities can be optimized:
\begin{itemize}
    \item Access or modification at an arbitrary index remains \( O(1) \).
    \item Increasing or decreasing \( n \) can be \( O(1) \) with virtual memory's remapping capabilities.
    \item Appending or discarding the last element is \( O(1) \).
    \item Inserting or deleting an element within the array still has \( O(n) \) complexity due to the need to shift elements in the array.
\end{itemize}

\begin{center}
    \begin{tabular}{ l c c }
    \hline
    \textbf{operation} & \textbf{complexity} \\
    \hline
    access/modify element at arbitrary index & \( O(1) \) \\
    increase \( n \) & \( O(1) \) \\
    decrease \( n \) & \( O(1) \) \\
    append an element & \( O(1) \) \\
    discard last element & \( O(1) \) \\
    insert an element & \( O(n) \) \\
    delete an element & \( O(n) \) \\
    \hline
    \end{tabular}
\end{center}

\subsection{Linked Lists}
Linked lists are fundamental data structures that implement a collection of elements, allowing for variable size \( n \).\\

\textbf{Linked lists support:}
\begin{itemize}
    \item Inserting, deleting, and modifying elements in \( O(1) \) time complexity, regardless of the position in the list.
    \item Accessing or modifying all elements in order with \( O(n) \) time complexity.
\end{itemize}

\textbf{Linked lists do not natively support:}
\begin{itemize}
    \item Accessing or modifying an element at an arbitrary index, known as "random access," which would have \( O(n) \) complexity if implemented sequentially.
\end{itemize}


\subsubsection{Doubly-linked lists}

Doubly-linked lists are an extension of linked lists where each element has a reference to both the previous and the next element.

\begin{lstlisting}
struct element {
    struct payload data;
    struct element *prev;
    struct element *next;
};
\end{lstlisting}

Functionality such as insertion is demonstrated in the provided code snippet, showing how new elements are linked to their predecessors and successors.

\paragraph{Comparative Operations}
\begin{tabular}{lcc}
\hline
\textbf{operation} & \textbf{dynamic array} & \textbf{doubly-linked list} \\
\hline
access/modify element at arbitrary index & \(O(1)\) & \(O(n)\) \\
increase \(n\) & \(O(1)\) & \(O(1)\) \\
decrease \(n\) & \(O(1)\) & \(O(1)\) \\
append an element & \(O(1)\) & \(O(1)\) \\
discard last element & \(O(1)\) & \(O(1)\) \\
insert an element & \(O(n)\) & \(O(1)\) \\
delete an element & \(O(n)\) & \(O(1)\) \\
\hline
\end{tabular}


\subsubsection{Memory management considerations}

Memory allocation is slow

\begin{lstlisting}
struct element *x = malloc(sizeof(struct element));
\end{lstlisting}

compared to dynamic arrays' fast case

\begin{lstlisting}
if (new_n <= d->sz) {
    d->n = new_n;
    return SUCCESS;
}
\end{lstlisting}

This means that the $O(1)$ complexity of doubly-linked list is slower than the $O(1)$ complexity of dynamic arrays.
\subsubsection{Memory caches considerations}
Cache usage is an important aspect of performance optimization. For linked lists, memory caches do not provide the same benefits as for dynamic arrays due to the non-contiguous memory allocation of elements.

\begin{itemize}
    \item With deep pipelines and good branch prediction, a processor can pre-fetch elements in a dynamic array, but it cannot do so as effectively with linked lists due to data dependencies.
\end{itemize}
While linked lists have fewer applications than one might expect, they can be extremely useful in situations where their specific advantages can be leveraged.


\subsubsection{More options}
Beyond simple linked lists, other data structures offer different performance trade-offs:

\begin{itemize}
    \item Indirection, such as a dynamic array of pointers, allows for a level of indirection which can simplify memory management.
    \item In-memory tree data structures, like binary trees or n-ary trees, provide hierarchical organization and can be more efficient for certain types of operations.
\end{itemize}

\begin{lstlisting}
struct nary_node {
    struct payload data;
    struct nary_node *children[MAX_CHILDREN];
};

struct dll_node {
    struct payload data;
    struct dll_node *prev_sibling;
    struct dll_node *next_sibling;
    struct dll_node *first_child;
};
\end{lstlisting}






















\newpage
\section{Stacks, queues, priority queues}

\subsection{Abstract Data Types and Data Structures}

\begin{itemize}
    \item An abstract data type
    \begin{itemize}
        \item Specifies supported operations
    \end{itemize}
    \item A data structure is an implementation of an abstract data type
    \begin{itemize}
        \item Specifies data layout in memory
        \item Specifies algorithms for operations
    \end{itemize}
\end{itemize}

\subsubsection{Lists}

\begin{itemize}
    \item support storing multiple elements together
    \item and optionally append, insert, delete, random access, \ldots
    \item implementations:
    \begin{itemize}
        \item dynamic arrays
        \begin{itemize}
            \item everything \(O(1)\) in practice except insert/delete \(O(n)\)
        \end{itemize}
        \item linked lists
        \begin{itemize}
            \item everything \(O(1)\) (but slower than arrays) except random access \(O(n)\)
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Stacks / LIFO}

\begin{itemize}
    \item A stack is an ordered collection of elements
    \item supports two operations:
    \begin{itemize}
        \item ``push'': add an element
        \item ``pop'': retrieve-and-remove the last-added element
        \begin{itemize}
            \item $\Rightarrow$ last in, first out (LIFO)
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Static array implementation of a stack}

\begin{itemize}
    \item Useful only when there is a hard limit on the number of elements
    \item We maintain a static array
    \item and a stack pointer (or top index)
    \item this is how ``the'' stack is implemented \\
    (for storing function arguments, local variables and return addresses)
\end{itemize}

\paragraph{Example}

The images illustrate this process:
\begin{enumerate}
    \item Initially, the stack is empty, and the stack pointer points to the base of the stack.

    \item As elements A, B, and C are pushed onto the stack, the stack pointer is incremented after each push to point to the next empty slot.

    \item When elements are popped from the stack, the stack pointer is decremented, and the elements are removed in the reverse order they were added (Last In, First Out - LIFO).

    \item If elements are pushed after popping, the stack pointer moves upwards again to fill the spaces that were previously occupied.
\end{enumerate}

\subsubsection{Linked list implementation of a stack}
\begin{itemize}
    \item \textbf{Pro:} No hard limit on number of elements
    \item \textbf{Con:}
    \begin{itemize}
        \item Memory allocation for every \texttt{push}
        \item Memory freed for every \texttt{pop}
    \end{itemize}
\end{itemize}
\subsubsection{Dynamic array implementation of a stack}

\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item No hard limit on number of elements
        \item Memory management overhead is small
    \end{itemize}
    \item \textbf{Con:}
    \begin{itemize}
        \item No pointer stability
    \end{itemize}
\end{itemize}

\paragraph{Example}
In a dynamic array stack, managing element pointers becomes a non-trivial task due to possible reallocations. Here's an insight into pointer stability during stack operations:

\begin{itemize}
    \item The stack begins empty. We then push elements A, B, C, D, and E onto the stack, which increases in size dynamically. The stack pointer marks the current top of the stack.
    
    \item Taking a pointer `p` to an element, say C, allows direct manipulation of its value. But this raises a concern: what happens to `p` when the stack grows?
    
    \item If new pushes cause the stack to exceed its capacity, it must expand, often necessitating a copy of elements to a new memory block. This reallocation invalidates previous element pointers, such as `p` pointing to C, as they now reference the old memory location.
    
    \item The visual sequence illustrates changing C to C' via pointer `p`. After stack expansion due to additional pushes, if the elements are reallocated, `p` may still point to where C used to be, not to C' in the new stack memory layout.
    
    \item The final state, after expansion and modification through `p`, should reflect the changed value of C' at the correct position in the stack, ensuring that `p` is updated to point to the new location of C'.
\end{itemize}

\subsubsection{Arena}

\begin{itemize}
  \item Known as arena allocator, region-based allocator, zone-based allocator, obstack
  \item Implemented as a list of static array stacks
\end{itemize}

\paragraph{Further explanation}
An arena allocator, also known as a region-based allocator, zone-based allocator, or obstack, is a memory management model particularly suitable for situations where objects are allocated and deallocated in a predictable pattern.

\begin{itemize}
    \item The arena allocator manages memory in large blocks or "arenas." 
    \item Each arena can contain a stack or a pool of objects.
    \item Memory within an arena is allocated sequentially, and objects are typically deallocated all at once.
    \item This model is highly efficient when objects have similar lifetimes, as it minimizes fragmentation and overhead associated with individual deallocation.
\end{itemize}

The images illustrate the process of allocating and deallocating objects within an arena. The operations shown include pushing elements onto the stack and then popping them off, which corresponds to allocating and deallocating memory in the arena.

\begin{itemize}
    \item Initially, the stack pointer points to the base of the arena, indicating that no objects are allocated.
    \item As objects are pushed onto the stack, the stack pointer moves up, marking the end of the used space.
    \item When objects are popped off the stack, the stack pointer moves down, indicating that space is now available for future allocations.
    \item The allocation is fast and efficient since it involves only incrementing or decrementing the stack pointer.
\end{itemize}

Memory allocation in an arena is thus done in a LIFO order, and all objects within an arena can be deallocated at once by resetting the stack pointer to the base of the arena.

\subsubsection{Stack implementations}

\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Implementation} & \textbf{Size} & \textbf{Requires allocations} & \textbf{Pointer stability} \\
\hline
static array & constant & no & yes \\
\hline
dynamic array & can grow & when growing & no \\
\hline
linked list & can grow & every push & yes \\
\hline
arena & can grow & when growing & yes \\
\hline
\end{tabular}

\subsubsection{More options}


Combination of other data structures and indirection can be used, depending on desired properties.

\subsection{queues / FIFO}

A queue is an ordered collection of elements
\begin{itemize}
    \item supports two operations:
    \begin{itemize}
        \item \texttt{enqueue}: add an element
        \item \texttt{dequeue}: retrieve-and-remove the earliest-added element
    \end{itemize}
    \item $\Rightarrow$ first in, first out (FIFO)
\end{itemize}

\subsubsection{Ring buffer implementation of a queue}
\begin{itemize}
    \item Useful only when there is a hard limit on the number of elements
    \item We maintain a static array
    \item and two pointers/indices: head and tail
\end{itemize}

\paragraph{Example}
These operations are illustrated through the following example:

\begin{enumerate}
    \item Start with an empty queue.
    \item \textbf{Enqueue A:} The queue is now: A.
    \item \textbf{Enqueue B:} The queue is now: A, B.
    \item \textbf{Dequeue:} Remove A. The queue is now: B.
    \item \textbf{Enqueue C:} The queue is now: B, C.
    \item Continue this process, adding to the tail and removing from the head.
\end{enumerate}

As we enqueue elements, they take their place at the tail end of the queue. When we dequeue, we remove the element from the head, adhering to the FIFO principle. 

\subsubsection{Implementation detail}

\noindent When head == tail:

\noindent\begin{tabular}{ll}
    \text{the queue is empty?} & \text{or the queue is full?} \\
\end{tabular}

\noindent\begin{itemize}
    \item maintain a variable with the number of elements currently in the queue
    \item or keep incrementing head and tail, and index the static array as
        \begin{align*}
            \text{array[head \% size]} && \text{and} && 
            \text{array[tail \% size]}
        \end{align*}
\end{itemize}

\subsubsection{Applications of ring buffers}

\begin{itemize}
	\item Audio playback/recording devices
	\item Video capture devices
	\item Special case (double-buffering, i.e. size = 2) for computer graphics
	\item Network devices (routers, switches)
\end{itemize}

\subsubsection{More options}

\begin{itemize}
	\item use dynamic arrays
	\item use linked lists
	\item use indirection
	\item \ldots
	\item depending on specific needs
\end{itemize}

\subsection{Priority queues}

\begin{itemize}
    \item A priority queue is a collection of elements, each with an associated priority
    \item supports two operations:
    \begin{itemize}
        \item ``push'': add an element-priority tuple
        \item ``pop'': retrieve-and-remove the highest-priority element
    \end{itemize}
\end{itemize}

\subsubsection{Implementation of a priority queue}

\begin{itemize}
    \item Store element-priority tuples in an array or in a linked list
    \item ``push'': \(O(1)\) of the underlying data structure
    \item ``pop'': scan all elements, find max priority, \(O(n)\)
\end{itemize}

\subsubsection{Binary heap implementation of a priority queue}

% [TBD] replaces the tree diagram from the image
[TBD]

\begin{itemize}
    \item Binary heaps represent a priority queue as
    \begin{itemize}
        \item a binary tree (every node has at most two children)
        \item that is complete (every level full, except possibly the deepest)
    \end{itemize}
    \item Every node is labeled by the corresponding element’s priority
    \item Tree has the heap property:
    \begin{itemize}
        \item Priority of any node \( \geq \) priority of its children
        \item \( \Leftrightarrow \) Priority of any node \( \geq \) priority of all its descendants
    \end{itemize}
\end{itemize}

\subsubsection{Binary heap push}

\begin{itemize}
  \item Step 0: Add new element at the first free slot on the deepest level
  \item Step 1:
  \begin{itemize}
    \item If its priority is \textit{not higher} than its parent's,
    \begin{itemize}
      \item the heap property is \textit{satisfied}, we are done
    \end{itemize}
    \item If its priority is \textit{higher} than its parent's,
    \begin{itemize}
      \item swap them,
      \item go back to Step 1, looking at the pushed element's new position
    \end{itemize}
  \end{itemize}
\end{itemize}


\subsubsection{Binary heap pop}

% The tree is not transcribed as it requires specific drawing packages.
% Tree representation: 
%       30
%     /    \
%    25    15
%   /  \
%  5   18 
%         \
%         9
[TBD]

\begin{itemize}
    \item Step 0: Replace root with last element (on deepest level)
    \item Step 1:
    \begin{itemize}
        \item If its priority is \textbf{not lower} than its children’s,
        \begin{itemize}
            \item the heap property is \textbf{satisfied}, we are done
        \end{itemize}
        \item If its priority is \textbf{lower} than one of its children’s,
        \begin{itemize}
            \item swap with the highest-priority child,
            \item go back to Step 1, looking at the pushed element’s new position
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Binary heap operations}
\begin{itemize}
    \item Push: \(O(\log_2(n))\)
    \item Find max: \(O(1)\)
    \item Pop: \(O(\log_2(n))\)
\end{itemize}

\subsubsection{Complete binary data structure}
\begin{itemize}
    \item Binary heaps are complete binary trees
    \item We can avoid allocation for every ``push'' by storing nodes in an array
    \item Depth \(\ell\) of the tree has at most \(2^\ell\) nodes, \(\forall \ell\)
    \item Depth \(\ell\) of the tree has exactly \(2^\ell\) nodes, except for the deepest level
    \begin{center}
        \begin{tabular}{|c|ccccccccccccccc|}
        \hline
        depth & 0 & 1 & 1 & 2 & 2 & 2 & 2 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3\\
        index & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 \\
        \hline
        \end{tabular}
    \end{center}
    \item There are exactly \( (2^\ell - 1) \) nodes of with depth \( < \ell \)
\end{itemize}



\subsubsection{Storage scheme}

    \begin{center}
        \begin{tabular}{|c|ccccccccccccccc|}
        \hline
        depth & 0 & 1 & 1 & 2 & 2 & 2 & 2 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3\\
        index & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 \\
        \hline
        \end{tabular}
    \end{center}

\begin{itemize}
  \item If a node has index \( j \)
  \item its children are stored at indices \( 2j + 1 \) and \( 2j + 2 \)
  \item its parent is stored at index \( \left\lfloor (j - 1) / 2 \right\rfloor \)
\end{itemize}

\subsubsection{Binary heap with array storage}

\begin{itemize}
    \item Superior to in-memory tree (with pointers)
    \begin{itemize}
        \item We avoid allocation for every ``push''
        \item We avoid data dependencies (load node data to get pointer to parent/children)
    \end{itemize}
    \item Still,
    \begin{itemize}
        \item Push and pop operations are tough for branch predictor
        \item Jumps to indices $(2j + 1)$, $(2j + 2)$ or $\lfloor (j - 1) / 2 \rfloor$ are not cache-friendly for large $j$
    \end{itemize}
\end{itemize}


\subsubsection{Priority queue: special case}

\begin{itemize}
    \item Assume that
    \begin{itemize}
        \item priorities are distinct integers $p \in \{0, \ldots, P - 1\}$
        \item we always push at a priority $\leq$ current max priority
    \end{itemize}
    \item Then,
    \begin{itemize}
        \item we allocate a static array of size $P$
        \item Push: store in array at index $p$ \quad $O(1)$
        \item Pop: sweep array backwards \quad $O(P / n)$ amortized
    \end{itemize}
\end{itemize}
\paragraph{Further Explanation}
\begin{itemize}
    \item A priority queue is implemented as a static array, where the index represents the priority and the array element at that index represents the queue element at that priority level.
    \item The queue supports two main operations:
    \begin{itemize}
        \item \textbf{Push operation:} This inserts an element into the queue at the position that corresponds to its priority. The operation is constant time, \( O(1) \), because it places the element directly at the index of its priority.
        \item \textbf{Pop operation:} This removes the element with the highest priority that is less than or equal to the current max priority. To find this element, the array is swept backwards from the current max priority index. The amortized complexity of this operation is \( O(P / n) \), where \( P \) is the size of the priority range and \( n \) is the number of operations, due to the fact that each element can be popped only once and the cost is spread over multiple pop operations.
    \end{itemize}
    \item The maximum priority element is always kept updated to enable efficient pop operations. After a pop, the max priority is updated to the next highest priority that has an element in the queue.
    \item This implementation allows for efficient insertion of elements in a priority-wise fashion and also ensures that the removal of the highest priority element is done with a reasonable amortized cost.
\end{itemize}

\subsubsection{Implementation details}

\begin{itemize}
  \item good for branch predictor
  \item great for caches
  \item we can store, additionally, an array of $P$ bits (``bitmap'')
  \begin{itemize}
    \item bit $p$ set to one if there is an element with priority $p$
    \item makes ``pop'' operations essentially 64x faster
  \end{itemize}
\end{itemize}

\subsubsection{Applications of bitmap priority queues}

\begin{itemize}
  \item Linux kernel: scheduling parallel tasks
  \item Linear algebra: sparse matrices
\end{itemize}

\subsection{Sort Operations}

\subsubsection{Heap sort}
\begin{itemize}
    \item Push $n$ elements to heap: $O(n \log n)$
    \item Pop $n$ elements one by one: $O(n \log n)$
\end{itemize}

\[
\Rightarrow O(n \log n) \text{ worst case}
\]

\subsubsection{Comparison sort methods}
\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Method} & \textbf{Average} & \textbf{Worst case} & \textbf{Additional storage} & \textbf{Combines with insert. sort} \\
\hline
Quicksort & $O(n \log(n))$ & $O(n^2)$ & none & yes\\
Merge sort & $O(n \log(n))$ & $O(n \log(n))$ & $n$ & yes\\
Heap sort & $O(n \log(n))$ & $O(n \log(n))$ & none & no\\
\hline
\end{tabular}
\end{table}

\subsubsection{Special case 1}
\begin{itemize}
    \item Assume that
    \begin{itemize}
        \item we sort $n$ elements with priorities $S \subseteq \{0, \ldots, P - 1\}$
        \item no two elements have the same priority (hence $P \geq n$)
    \end{itemize}
    \item Then,
    \begin{itemize}
        \item we represent the elements as a bitmap priority queue
        \item Push: $O(n)$
        \item Pop: $O(P)$
    \end{itemize}
    \item[] $\Rightarrow O(n + P)$
\end{itemize}

\subsubsection{Special case 2: counting sort}
\begin{itemize}
    \item Assume that
    \begin{itemize}
        \item we sort $n$ elements with priorities $S \subseteq \{0, \ldots, P - 1\}$
        \item $P \leq n$ (we may have duplicates)
    \end{itemize}
    \item Then,
    \begin{itemize}
        \item we allocate a static array \texttt{count} of size $P$
        \item we allocate a static array \texttt{result} of size $n$
        \item we count the number of occurences of each priority: $O(n)$
        \item we sweep \texttt{count} backwards to determine offsets: $O(P) = O(n)$
        \item we construct the sorted \texttt{result} list: $O(n)$
    \end{itemize}
    \item[] $\Rightarrow O(n)$
\end{itemize}






















\newpage
\section{Tries, hash tables, spatial data structures}

\subsection{Associative Arrays}
Associative arrays, also known as maps or dictionaries, are fundamental data structures in computer science and programming.

\subsubsection{Definition and Operations}
\begin{itemize}
    \item Associative arrays are collections of key-value pairs or tuples.
    \item Keys can be any string of bits, such as integers or strings.
    \item Values are data associated with the keys.
    \item Operations supported by associative arrays include:
    \begin{itemize}
        \item Insertion: Adding a new key-value pair.
        \item Deletion: Removing an existing key-value pair.
        \item Lookup: Finding the value associated with a given key.
    \end{itemize}
\end{itemize}



\subsubsection{Naive implementation}
A naive implementation of an associative array can be achieved using a simple list of key-value tuples.

\subsubsection{Complexity}
The complexity of operations in a naive implementation varies based on the underlying data structure:\\

\begin{tabular}{lccc}
\hline
& Insertion & Deletion (after lookup) & Lookup \\
\hline
Linked list & $O(1)$ & $O(1)$ & $O(n)$ \\
Dynamic array & $O(1)$ & $O(n)$ & $O(n)$ \\
\hline
\end{tabular}



\subsection{Implementations using a total order on keys}
\subsubsection{Total order on keys}
\textbf{Key comparison}
\begin{itemize}
    \item Associative arrays require a mechanism to compare keys (e.g., \( key_i \leq key_j \)).
    \item In practice, key comparison is feasible by interpreting keys as large integers.
    \item Specialized comparison operators may be more efficient for constant-sized keys.
    \item Key space could potentially be infinite, accommodating arbitrary-sized keys.
\end{itemize}


\subsubsection{Sorted dynamic array of (key, value) tuples}

\begin{itemize}
    \item Sorted dynamic arrays maintain keys in a total order (e.g., \( key_0 \leq key_1 \leq \dots \leq key_n \)).
    \item Lookup operations can use binary search, leading to \( O(\log n) \) complexity.
\end{itemize}

\begin{tabular}{lccc}
\hline
\textbf{Data Structure} & \textbf{Insertion} & \textbf{Deletion (after lookup)} & \textbf{Lookup} \\
\hline
Linked list             & O(1)               & O(1)                            & O(n)             \\
Dynamic array           & O(1)               & O(n)                            & O(n)             \\
Sorted dynamic array    & O(n)               & O(n)                            & $O(\log(n))$       \\
\hline
\end{tabular}

\subsubsection{Binary search tree}

\begin{itemize}
    \item A binary search tree (BST) maintains a total order invariant within its structure.
    \item Each node \( i \) guarantees \( key_j \leq key_i \) for all \( j \) in its left subtree and \( key_j > key_i \) for all \( j \) in its right subtree.
    \item The shape of the BST can vary greatly depending on the insertion order, which can impact performance.
\end{itemize}

\subsubsection{Self-balancing binary search tree}

\begin{itemize}
    \item AVL trees
    \item Red-black trees
    \item B-trees, splay trees, treaps, \ldots
\end{itemize}

\begin{tabular}{lccc}
\hline
\textbf{Data Structure}     & \textbf{Insertion}         & \textbf{Deletion (after lookup)} & \textbf{Lookup}      \\
\hline
Linked list                 & O(1)                       & O(1)                            & O(n)                 \\
Dynamic array               & O(1)                       & O(n)                            & O(n)                 \\
Sorted dynamic array        & O(n)                       & O(n)                            & $O(\log(n))$           \\
Binary search tree          & O(n)                       & O(n)                            & O(n)                 \\
AVL tree                    & $O(\log(n))$                 & $O(\log(n))$                      & $O(\log(n))$           \\
Red-black tree              & $O(\log(n))$                 & $O(\log(n))$                      & $O(\log(n))$           \\
\hline
\end{tabular}


\begin{itemize}
    \item Cache behavior: ok, not great (similar to other binary tree structures, e.g., heap)
\end{itemize}


\subsection{Implementations using keys bits - tries}

\subsubsection{Trie}

\begin{itemize}
    \item A trie, also known as a prefix tree, is a tree structure that uses static arrays of size \(2^T\) for routing.
    \item Keys are divided into chunks or "letters" of \(T\) bits.
    \item Each chunk provides an index into a node's static arrays.
    \item The path from the root to a leaf represents the complete key.
\end{itemize}

\paragraph{Trie Operations}
A Trie is a specialized tree used to store associative arrays, where the keys are usually strings. Inserting a key into a trie involves several steps:
\begin{enumerate}
    \item A key is represented by a series of characters. In the context of a trie, each character can be thought of as a node in the tree.
    \item To insert a key, we start at the root of the trie and traverse the tree by following the path defined by the key's characters.
    \item At each step, we look at the current character in the key and move to the corresponding child node.
    \item If a child node corresponding to the current character does not exist, we create a new node.
    \item We continue this process until all characters in the key have been processed.
    \item Once we reach the end of the key, we mark the final node as an end node, which signifies the completion of a key insertion.
\end{enumerate}


\paragraph{Trie Insertion Example}
Consider the insertion of the hexadecimal keys \(0x9f2\), \(0x8cd\), and \(0x532\) with corresponding values \(V1\), \(V2\), and \(V3\) into a trie where each node represents 4 bits of the key.

\begin{enumerate}
    \item For the key \(0x9f2\) with value \(V1\), we would insert the 4-bit segments (2, f, 9) one by one, creating a path in the trie.
    \item Next, for the key \(0x8cd\) with value \(V2\), we would insert the segments (d, 8, c) into the trie, potentially sharing nodes with the previous key if any 4-bit segments are the same.
    \item Finally, for the key \(0x532\) with value \(V3\), we would insert the segments (2, 3, 5) into the trie, again sharing nodes with any previously inserted keys that have matching segments.
\end{enumerate}


\paragraph{Complexity Analysis}
\begin{itemize}
    \item Tries are highly efficient for lookups, insertions, and deletions.
    \item They provide a means to search for keys in a dataset of strings in \(O(m)\) time complexity, where \(m\) is the length of the key to be searched.
    \item They are particularly useful for implementing dictionaries with prefix-based lookups.
\end{itemize}


\subsubsection{Key space}
The key space of an associative array refers to the range of all possible keys that can be used within the array. The concept of \emph{dense} and \emph{sparse} key spaces is crucial in the context of data structure efficiency.

\begin{itemize}
    \item Let \( K \) be the set of all possible keys, and \( n \) the number of tuples in the associative array.
    \item A key space is considered \emph{sparse} if \( n \ll K \), meaning that the number of actual keys used is much less than the possible number of keys.
    \item If the number of used keys is comparable to the number of possible keys, the key space is considered \emph{dense}.
\end{itemize}

\subsubsection{``Dense'' key space}

\begin{itemize}
    \item In a \emph{dense} key space, most of the potential keys have values associated with them.
    \item Operations such as insertion, deletion, and lookup can be done in \( O(\log_{2^T} n) \) time, where \( T \) is a factor related to the trie branching (typically the size of the alphabet or character set).
    \item For a dense key space, a static array can be more efficient as the operations become \( O(1) \).
\end{itemize}

\subsubsection{``Sparse'' key space}

\begin{itemize}
    \item Tries are particularly useful for \emph{sparse} key spaces where a static array of the entire key space would be impractically large.
    \item The complexity of trie operations in a sparse key space is not dependent on the number of entries but on the key size and the trie branching factor \( T \).
    \item A sparse key space can lead to high memory overhead in the worst case, where every leaf node might contain only a single tuple ( $O(n \times 2^T)$).
\end{itemize}
A trie provides a balance between the two extremes, offering more efficient memory usage in sparse key spaces without sacrificing the performance benefits of a static array in dense key spaces.

\subsection{Implementations using key bits - hash tables}
Hash tables are a way to implement associative arrays by mapping a large, potentially sparse key space into a smaller, dense index space using a hash function.

\subsubsection{Hash function}
\begin{itemize}
    \item A hash function \( h \) is a mapping from the key space \( K \) to an index space \( U \), where \( U \subseteq \mathbb{N} \) and the size of \( U \) is much smaller than \( K \). Since $|U| < |K|$ hash functions are necessarily surjective $\exists k_1 \neq k_2 $ such that $h(k_1)=h(k_2)$.
    \item A good hash function distributes keys evenly across the index space, minimizing the chance of collisions.
    \item Examples of (usually bad) hash functions include taking a subset of the key bits or using modular arithmetic, such as \( h(k) = k \mod m \) for some integer \( m \).
\end{itemize}

\subsubsection{Hash table}
\begin{itemize}
    \item A hash table uses a static array of size \( |U| \) to store values.
    \item Each key-value tuple \((k, v)\) is stored in the array at the index given by \( h(k) \).
    \item Due to the surjective nature of hash functions, collisions can occur where multiple keys are hashed to the same index (i.e. tuples with distinct keys are stored at a same array index).
\end{itemize}

\subsubsection{Dealing with collisions}
When implementing associative arrays using hash tables, collisions occur when two different keys hash to the same index. Here are some strategies to handle collisions:

\paragraph{Collision Resolution Strategies}
\begin{itemize}
    \item The hash table can be a static array of linked lists. When a collision occurs, the colliding elements are added to the list at the hashed index.
    \item Alternatively, a static array of dynamic arrays (or vectors) can be used, where colliding elements are appended to the dynamic array at the hashed index.
\end{itemize}

\paragraph{Search Complexity in Case of Collisions}
\begin{itemize}
    \item When collisions occur, a linear search within the linked list or dynamic array at the hash index is performed.
    \item If \( c \) is the maximum number of collisions at a hash index, the worst-case search complexity is \( O(c) \).
    \item In the worst case where all elements collide at the same index, \( c \) can be equal to \( n \), making the complexity \( O(n) \).
\end{itemize}

\paragraph{Insertion and Lookup Operations}
\begin{itemize}
    \item To insert a key-value pair \((k, v)\), hash the key to find the index and add the pair to the appropriate data structure at that index.
    \item To lookup a value for a key, hash the key to find the index, and then search through the data structure at that index to find the value.
\end{itemize}




\subsubsection{Open addressing as a collision resolution method}
Open addressing is a collision resolution method in hash tables. When a collision occurs, it finds another place within the hash table.

\paragraph{Insertion Process of (key, value)}
\begin{enumerate}
    \item Compute the hash index \( i = h(\text{key}) \).
    \item If \texttt{array[i]} is empty, place the \((\text{key}, \text{value})\) tuple there. Process done.
    \item Otherwise, if \texttt{array[i]} is occupied (collision), then let \( i = (i + 1) \mod |U| \), and repeat from step 2.
\end{enumerate}

\textbf{Example of Open Addressing}
\begin{lstlisting}
h(k) = k mod 16
Insert (0x9f2, V1) -> h(0x9f2) = 0x2
Insert (0x8cd, V2) -> h(0x8cd) = 0xd
Insert (0x532, V3) -> h(0x532) = 0x2

// Array after insertion
// Indices:   0 1 2 3 4 5 6 7 8 9 a b c d e f
// Values:    . . V1V3. . . . . . . . . V2. .
\end{lstlisting}

\paragraph{Lookup After Collision}
\begin{itemize}
    \item To lookup a value for a key after collisions, start at \( h(\text{key}) \) and search sequentially until:
    \begin{itemize}
        \item The key is found, or
        \item An empty slot is encountered, indicating the key is not in the table.
    \end{itemize}
\end{itemize}


\paragraph{Lookup in Hash Tables with Open Addressing}
The lookup process for retrieving the value associated with a key in a hash table that uses open addressing is as follows:

\begin{enumerate}
    \item Compute the initial index \( i \) using the hash function: \( i = h(\text{key}) \).
    \item Check if the key at the computed index matches the key we are looking for:
    \begin{itemize}
        \item If \texttt{array[i]} matches the key, return \texttt{array[i]}.
        \item If \texttt{array[i]} is empty, return \textcolor{red}{\textbf{not found}}.
        \item If another key is found at \texttt{array[i]}, compute the next index \( i = (i + 1) \mod |U| \), and go back to the previous step.
    \end{itemize}
\end{enumerate}

\subsubsection{Probing}

Probing is a technique used in hash tables to resolve collisions. It involves finding the next available slot or bucket when a collision occurs. Here's how it works during insertion:

\begin{enumerate}
    \item \textbf{Compute initial index}: Let \( h_0 = h(k) \) be the hash of key \( k \), and initialize \( j = 0 \).
    \item \textbf{Check the slot}: If the slot \( array[i(h_0, j)] \) is empty, insert \( (k, v) \) there and terminate the insertion.
    \item \textbf{Collision resolution}: If the slot is occupied, increment \( j \) by 1 and find the next slot using a probing function \( i(h_0, j) \), then repeat step 2.
\end{enumerate}

The probing function \( i(h_0, j) \) can vary:
\begin{itemize}
    \item \( i(h_0, j) = (h_0 + j) \mod |U| \) as before
    \item \textbf{Linear probing}: \( i(h_0, j) = (h_0 + Kj) \mod |U| \) for some constant $K$.
    \item \textbf{Quadratic probing}: \( i(h_0, j) = (h_0 + Kj + Lj^2) \mod |U| \) for some constants \( K \) and \( L \).
\end{itemize}


\subsubsection{Good hash functions}
\begin{itemize}
    \item Naive hash functions can lead to high collision rates, even with random keys.
    \item Effective hash functions take a non-uniform distribution of keys over the key space \( K \) and map it to a uniform-looking distribution over a set \( U \), minimizing collisions.
    \item Examples of good hash functions are Fowler–Noll–Vo (FNV), djb2, SipHash.
    \item These functions usually output 32-, 64-, or 128-bit numbers, which are then mapped to the table size using modulo operation: \( h(k) = h_0(k) \mod |U| \).
\end{itemize}

\subsubsection{Complexity of hash table operations}

Performance is influenced by many factors, including:
\begin{itemize}
    \item The density \( \frac{n}{|U|} \), key distribution, hash function choice, and probing method all affect performance.
    \item As table density approaches 1, it's often necessary to increase \( |U| \) and rebuild the hash table, known as "rehashing".
\end{itemize}

\subsubsection{In practice}
\begin{itemize}
    \item Keeping the collision rate low ensures that operations like insert, delete, and lookup can remain \( O(1) \) in average case.
    \item The initial access to a hash table often results in a cache miss (at least L1), which impacts performance.
    \item With open addressing, in case of collisions, probing may not be.
\end{itemize}

\subsection{Associative Arrays: performance}
\textbf{Choice of Data Structure:}
\begin{itemize}
    \item There is no universally superior data structure for all scenarios; the choice between self-balancing trees, tries, and hash tables is highly dependent on the specific data and application requirements.
    \item It is often beneficial to perform benchmarking to determine the most appropriate data structure.
\end{itemize}

\paragraph{Hash Tables}
\begin{itemize}
    \item Hash tables may perform better when:
    \begin{itemize}
        \item Hashing operations are low-cost.
        \item Collisions are infrequent.
        \item The scale of the dataset (\( n \)) is predictable.
    \end{itemize}
\end{itemize}

\paragraph{Self-balancing Trees}
\begin{itemize}
    \item Self-balancing trees provide:
    \begin{itemize}
        \item Robustness with better worst-case non-amortized complexity, particularly during operations like rehashing.
    \end{itemize}
\end{itemize}

\paragraph{Tries}
\begin{itemize}
    \item Tries can be more efficient when working with keys that have a structured pattern, such as:
    \begin{itemize}
        \item Virtual address translation in page tables.
        \item IP address routing for networks.
        \item Tokenizers in natural language processing (e.g., GPT-type models).
    \end{itemize}
\end{itemize}

\subsubsection{Combinations of Data Structures}
\begin{itemize}
    \item Combining data structures can yield the benefits of multiple approaches and is common practice. Examples include:
    \begin{itemize}
        \item Hash tables implemented as a static array of self-balancing trees.
        \item Depth-k tries with self-balancing trees at the leaf nodes.
    \end{itemize}
\end{itemize}



\subsection{Spatial data structures}
Spatial data structures are designed to manage multidimensional data, such as vectors in $\mathbb{R}^m$. They support various operations:

\begin{itemize}
    \item Insertion: Adding a vector $\mathbf{x} \in \mathbb{R}^m$.
    \item Deletion: Removing a vector.
    \item Finding the closest vector to a given vector $\mathbf{y} \in \mathbb{R}^m$.
    \item Locating nearest neighbors for each inserted vector.
    \item Identifying $k$ nearest neighbors for each inserted vector.
    \item Discovering all vectors within a certain distance $d$ from each inserted vector.
\end{itemize}

\subsubsection{The problem}
Given a set of vectors, the goal is to find all pairs of vectors that are within a distance $d$ from each other. A naive approach to this problem has a time complexity of $O(n^2)$, described by the following pseudocode:

\begin{lstlisting}
R := {}
for i = 0 to n - 1:
    for j = i + 1 to n - 1:
        if ||x^i - x^j|| <= d:
            R := R union {(i, j)}
\end{lstlisting}


\subsubsection{Grids}
A grid-based approach divides the space into a finite number of cells, which can significantly reduce the number of comparisons needed.

\subsubsection{Pros and Cons of Using Grids}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item Quadratic complexity is limited to within individual grid cells.
    \end{itemize}
    \item \textbf{Cons:}
    \begin{itemize}
        \item Requires finite bounds $L \leq \mathbf{x}_i \leq U$ for all vectors.
        \item Fixed cell size, which can lead to:
        \begin{itemize}
            \item A variable number of vectors in each cell.
            \item Many cells being empty, especially in sparse regions.
        \end{itemize}
    \end{itemize}
\end{itemize}



\subsubsection{Quadtrees and octrees}
Quadtrees and octrees are tree data structures which partition a two-dimensional and three-dimensional space respectively into successively smaller regions.

\begin{itemize}
    \item They adaptively divide the space based on the distribution of the objects within.
    \item Each node in a quadtree corresponds to a square region of space and has four children, which correspond to the four quadrants of the square.
    \item Similarly, each node in an octree corresponds to a cubic region and has eight children.
\end{itemize}

\subsubsection{k-d trees}
k-d trees are a binary tree used to organize points in k-dimensional space.

\begin{itemize}
    \item Nodes which are not leaves represent hyperplanes that divide the space into two parts, known as half-spaces.
    \item Points to the left of this hyperplane are represented by the left subtree of that node and points to the right by the right subtree.
    \item The tree alternates between axes used to partition the space.
\end{itemize}


\subsubsection{Quadtrees, octrees, k-d trees: Pros and Cons}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item No need for finite bounds $L \leq \mathbf{x}_i \leq U$ for all points.
        \item Cell size adapts to the distribution of the data.
    \end{itemize}
    \item \textbf{Limitations:}
    \begin{itemize}
        \item Cells have a fixed shape, which can be inefficient for high-dimensional data.
        \item As the number of dimensions grows, the number of cells can grow exponentially, which is known as the curse of dimensionality.
    \end{itemize}
\end{itemize}

\subsubsection{Binary space partitioning}
Binary Space Partitioning (BSP) is a method for recursively subdividing a space into convex sets by hyperplanes. This method is commonly used in computer graphics, computational geometry, and for developing spatial data structures.

\begin{itemize}
    \item BSP is a generic process of partitioning a space into two parts at each step.
    \item It is typically implemented as a binary tree where each node represents a subregion of space partitioned by a hyperplane.
\end{itemize}

\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item Variable cell shape, which allows for efficient space division according to the distribution of objects.
    \end{itemize}
    \item \textbf{Cons:}
    \begin{itemize}
        \item Computation of separating hyperplanes can be costly.
    \end{itemize}
    \item \textbf{Limitations:}
    \begin{itemize}
        \item Not suitable for high-dimensional data due to the curse of dimensionality.
    \end{itemize}
\end{itemize}


\subsubsection{Locality-sensitive hashing}
Locality-Sensitive Hashing (LSH) is an algorithmic technique that hashes similar input items into the same “buckets” with high probability. The buckets are used to partition the space in a way that preserves locality.

\begin{itemize}
    \item The goal is to maximize the probability that similar items map to the same bucket.
    \item LSH is used for approximate nearest neighbor search in high-dimensional spaces.
\end{itemize}

\subsubsection{Designing a Locality-Sensitive Hash Function}

\begin{itemize}
    \item The hash function \( h \colon \mathbb{R}^m \to \mathbb{R} \) should ensure that if two points \( \mathbf{x}, \mathbf{y} \) are close in the input space, their hash values \( h(\mathbf{x}), h(\mathbf{y}) \) should be close as well.
    \item It is often used in conjunction with other data structures to handle high-dimensional data efficiently.
\end{itemize}






















\newpage
\section{SIMD, threads, distributed computing, hardware acceleration}
\subsection{Parallel Computation}

\subsection{Paralleism that does not require programmer intervention}

\subsubsection{Pipelines}
\begin{itemize}
    \item CPU pipelines can be viewed as implementing some form of parallelism in the sense that multiple executions are being executed simultaneously.
    \item For example, one instruction’s arithmetic is performed (in an ALU) while the next is being decoded.
    \item However, from the programmer’s perspective, everything must happen as if there was no parallelism at all.
\end{itemize}

\subsubsection{Multitasking}

\begin{itemize}
    \item Multitasking allows multiple executables to run “simultaneously” (even on a single processor)
    \item Regularly, the \textbf{scheduler} (part of the OS kernel) decides which task gets to run on a processor.
\end{itemize}

\subsection{Multitasking on a single-core processor}

\begin{itemize}
    \item Multitasking on a single-core processor allows multiple tasks to be handled seemingly in parallel by quickly switching between them. This is known as context switching.
    \item A single-core CPU can only execute one task at a time. However, it gives the illusion of parallelism by rapidly alternating between tasks, which is managed by the operating system's scheduler.
    \item Tasks can be in different states, such as \textit{running}, where a task is actively using the CPU, or \textit{sleeping}, where a task is inactive and waiting for some event or the passage of a certain amount of time.
    \item When multitasking, the scheduler decides which task should be run next. This decision is based on various factors such as task priority, fairness, I/O requirements, and more.
    \item The scheduler uses a context switch to save the state of the current running task and load the state of the next task to run. The state of a task includes information like the program counter, registers, and memory allocation.
    \item This rapid switching is fast enough that users perceive multiple applications are running simultaneously, even though only one is being processed at any instant.
\end{itemize}

\begin{itemize}
  \item The scheduler is called:
  \begin{itemize}
    \item at regular intervals $f$ times per second, by default:
    \begin{itemize}
      \item Linux: $f = 1000\ \text{Hz}\ (\text{see CONFIG\_HZ})$
      \item macOS: $f = 100\ \text{Hz}\ (\text{see sysctl kern.clockrate})$
      \item Windows 10: $f = 64\ \text{Hz}\ (\text{see timeBeginPeriod()})$
    \end{itemize}
    \item when an task performs a system call (\texttt{open()}, \texttt{write()}, \texttt{exit()}, ...)
    \item when a ``hardware interrupt'' happens:
    \begin{itemize}
      \item keyboard received a keypress
      \item network device received data
      \item storage device finished writing
      \item sound/video device ready to receive next buffer
      \item \ldots
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsubsection{Preemptive multitasking}
\begin{itemize}
  \item When the scheduler decides to interrupt a running process (e.g. to run another)
  \begin{itemize}
    \item the process is said to ``preempted''
    \item it becomes ``runnable''
  \end{itemize}
  \item When a process executes a system call,
  \begin{itemize}
    \item it starts ``sleeping''
    \item after the requested operation is performed,
    \begin{itemize}
      \item in some cases, it will run again
      \item in other cases, it becomes runnable and will only run when a CPU is available
    \end{itemize}
    \item many system calls can take a long time to perform (``blocking'' system calls):
    \begin{itemize}
      \item \texttt{read()}, \texttt{write()}, \texttt{recv()}, \texttt{send()}
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item At any given time, most tasks are sleeping
  \begin{itemize}
    \item waiting for data (e.g. from network)
    \item waiting for user interaction (e.g. keyboard or touch input)
    \item waiting on a timer (tasks that run at regular interval)
  \end{itemize}
  \item The only tasks that are normally running/runnable are those performing CPU-intensive operations
  \begin{itemize}
    \item graphics rendering
    \item audio/video/data compression and decompression
    \item computations
    \item etc.
  \end{itemize}
\end{itemize}

\subsubsection{Multitasking on a multi-core processor}

\begin{itemize}
    \item Multitasking on a multi-core processor refers to the ability of the CPU to perform multiple tasks at the same time by utilizing multiple cores within a single CPU unit.
    \item Each core can independently run a task, allowing for real parallel execution of processes. This improves the overall efficiency and performance of the system, especially for multi-threaded applications.
    \item In the illustrated example, five tasks (task 0 to task 4) are distributed across four CPU cores (CPU 0 to CPU 3). This distribution allows each task to execute in parallel:
    \begin{itemize}
        \item Task 0 runs on CPU 0
        \item Task 1 runs on CPU 1
        \item Task 2 runs on CPU 2
        \item Task 3 runs on CPU 3
        \item Task 4 could either be queued or run concurrently if one of the other tasks completes or is in a waiting state (not shown in the images).
    \end{itemize}
    \item This parallelism enables a multi-core processor to handle more tasks in the same amount of time compared to a single-core processor, which must switch contexts to give the illusion of parallelism.
\end{itemize}


\begin{itemize}
    \item From a hardware perspective:
    \begin{itemize}
        \item A CPU corresponds to a single integrated circuit (``IC'') package
        \item A computer can (rarely) have multiple CPUs
        \begin{itemize}
            \item Typically only found in datacenters, rarely more than 2
        \end{itemize}
        \item Each CPU can have multiple \textit{cores}
        \begin{itemize}
            \item generally 2-8 cores on laptops
            \item up to 128 on datacenter CPUs
        \end{itemize}
    \end{itemize}
    \item From a software perspective:
    \begin{itemize}
        \item Everything that can run a task is generally called a ``CPU''
        \item Only the kernel's scheduler will (sometimes) care about CPU vs. core
        \item All other software is unaware of the difference
    \end{itemize}

    \item A CPU can have multiple copies of some logic blocks
    \item very common for arithmetic and logic units (ALUs)
\end{itemize}


\subsection{Simultaneous Multithreading {SMT}}
    \begin{itemize}
        \item From a hardware perspective:
        \begin{itemize}
            \item With Simultaneous Multithreading (SMT) (a.k.a. Hyperthreading),
            \item each core can run multiple (generally 2) tasks (``threads'')
            \item but they share many logic blocks (in particular ALUs)
            \item SMT \textit{works well} when those logic blocks would otherwise be idle
            \item SMT is \textit{ineffective} when those logic blocks are the bottleneck
        \end{itemize}
        \item From a software perspective:
        \begin{itemize}
            \item Everything that can run a task is generally called a ``CPU''
            \item Only the kernel's scheduler will (sometimes) care about CPU vs. core vs. \textit{thread}
            \item All other software is unaware of the difference
            \item ``Thread'' has a different meaning in software
        \end{itemize}
    \end{itemize}

\subsection{SIMD}
\begin{itemize}
    \item SIMD stands for Single Instruction Multiple Data
    \item new, larger registers (in addition to the general purpose ones): ``vector registers''
\end{itemize}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
bits                   & 255..224             & 223..192             & 191..160             & 159..128             & 127..96              & 95..64               & 63..32               & 31..0                \\ \hline
\multirow{2}{*}{256}   & \multicolumn{8}{c|}{ymm0}                                                                                                                                  \\ \hline
64                     & fp64 \#3             &                      & fp64 \#2             &                      & fp64 \#1             &                      & fp64 \#0             &                      \\ \hline
32                     & fp32 \#7             & fp32 \#6             & fp32 \#5             & fp32 \#4             & fp32 \#3             & fp32 \#2             & fp32 \#1             & fp32 \#0             \\ \hline
16                     & \multicolumn{2}{c|}{}                      & \multicolumn{2}{c|}{}                      & \multicolumn{2}{c|}{}                      & \multicolumn{2}{c|}{}                      \\ \hline
8                      & \multicolumn{8}{c|}{}                                                                                                                                      \\ \hline
\end{tabular}

\begin{itemize}
    \item but
    \begin{itemize}
        \item SIMD registers cannot be treated as big integers
        \item individual ``lanes'' (8-, 16-, 32- or 64-bit parts) generally cannot be accessed individually
    \end{itemize}
\end{itemize}

\subsection{SIMD registers}
\begin{itemize}
    \item On Intel (and AMD) ISAs:
    \begin{itemize}
        \item SSE (ca. 1999): 8 128-bit registers xmm0 - xmm7
        \item AVX (ca. 2011): 16 256-bit registers ymm0 - ymm15
        \item AVX-512 (ca. 2016, but not yet generally available): 32 512-bit registers zmm0 - zmm31
    \end{itemize}
    \item On ARM:
    \begin{itemize}
        \item Neon (ca. 2005): 16 128-bit registers Q0 - Q15
    \end{itemize}
\end{itemize}

\subsection{Example}

The code example illustrates the concept of SIMD (Single Instruction, Multiple Data) in the context of vectorized operations for performance optimization in computing.

\begin{itemize}
    \item SIMD is a parallel computing architecture that allows a single processor instruction to perform the same operation on multiple data points simultaneously.
    \item In the given C function \texttt{add\_one}, each element of a 4-element float array is incremented by one. This operation, although simple, can be vectorized to exploit data-level parallelism provided by SIMD.
    \item The corresponding SIMD-enabled assembly instructions perform the addition in a parallel fashion:
    \begin{itemize}
        \item The \texttt{vbroadcastss} instruction replicates a single float value into all four elements of a SIMD register (\texttt{xmm0}), preparing the value \texttt{1.0} for parallel operations.
        \item The \texttt{vaddps} instruction performs parallel addition of the four \texttt{1.0} values in \texttt{xmm0} to the four elements of the input array pointed to by \texttt{rdi}, storing the result back into \texttt{xmm0}.
        \item The \texttt{vmovups} instruction writes the result from the SIMD register back to the memory location of the array, completing the parallel increment operation.
    \end{itemize}
    \item This SIMD approach is beneficial for computational efficiency, as it minimizes the number of sequential operations and leverages the processor's ability to handle multiple data points in a single instruction cycle.
\end{itemize}

\subsection{Counter-example}

The counter-example demonstrates a scenario where SIMD (Single Instruction, Multiple Data) instructions are not sufficient to handle all operations in a vectorized manner due to the dependency of each operation on the result of the previous one.

\begin{itemize}
    \item In the C function \texttt{many\_ops}, different arithmetic operations are applied to the elements of a float array, with each operation depending on the result of the previous one, which introduces data dependencies.
    \item The assembly code shows a sequence of SIMD instructions corresponding to the C code operations. However, due to the dependencies, each operation must be completed before the next can begin, preventing the full utilization of SIMD parallelism.
    \item The instructions include \texttt{vaddss} and \texttt{vdivss} for addition and division on scalar single-precision floating point values, respectively. These operations require the results from previous instructions, hence they cannot be parallelized.
    \item This example illustrates that while SIMD is powerful for operations that can be performed in parallel without dependencies, it is not universally applicable to all types of data processing, especially those with a sequence of dependent operations.
\end{itemize}

\subsection{How to use SIMD}
\begin{itemize}
    \item Rely on compilers (\textit{``autovectorization''})
    \item Write assembly code
    \item Use compiler \textit{``intrinsics''}
    \begin{itemize}
        \item Intrinsics look like C functions
        \item but the compiler knows how to translate them to specific assembly code
        \item \textbf{Intel intrinsics guide}
        \item \textbf{ARM intrinsics}
    \end{itemize}
\end{itemize}

$\implies \text{refer to the intrinsics guide}$
\subsection{Thread-level concurrency}

\subsection{Processes and threads}
\begin{itemize}
    \item When the OS runs an executable, it gets its own \textit{process}
    \item A single executable (if run multiple times) can have multiple independent processes
    \item Memory is virtualized: each process has its own view of the memory it owns
    \item A process can create (“spawn”) multiple \textit{threads}
    \item Like processes, each thread is an individual task from the point of view of the scheduler
    \item Within a process, threads share a same view of the process memory
\end{itemize}

\begin{itemize}
    \item Pro: Communication between threads is extremely efficient
    \begin{itemize}
        \item Just write something to memory,
        \item let other threads read it through the same pointer
    \end{itemize}
    
    \item Con: Because memory is shared, synchronizing threads is \textbf{very complex}
\end{itemize}

\subsubsection{Wrong code (1)}

The code attempts to implement a single-item buffer with:
\begin{itemize}
  \item A \texttt{push} function that stores a value in the buffer and sets a \texttt{ready} flag to indicate the buffer is full.
  \item A \texttt{pop} function that retrieves a value from the buffer and clears the \texttt{ready} flag to indicate the buffer is empty.
\end{itemize}
The \texttt{ready} flag is used for synchronization, ensuring each item is only pushed when the buffer is empty and popped when full.

\textbf{The C compiler is free to reorder this:}

\begin{lstlisting}
void push(int value)
{
    while (ready == 1) {
        // wait
    }
    buffer = value;
    ready = 1;
}
\end{lstlisting}

\textbf{into this:}

\begin{lstlisting}
void push(int value)
{
    buffer = value;
    while (ready == 1) {
        // wait
    }
    ready = 1;
}
\end{lstlisting}


\textbf{The C compiler is free to infer that this loop:}

\begin{lstlisting}
while (ready == 1) {
    // wait
}
\end{lstlisting}

has either zero or infinitely many iterations without side effects (UB); thus remove the loop!

\subsubsection{Wrong code (2)}

The code shows a flawed implementation for synchronizing a single-producer, single-consumer scenario using a shared buffer:
\begin{itemize}
  \item The \texttt{push} function writes to the buffer if it's not marked as ready, and sets it as ready afterward.
  \item The \texttt{pop} function reads from the buffer if it's marked as ready, and resets the flag afterward.
  \item The use of \texttt{volatile} suggests an attempt to prevent compiler optimization from reordering the access to the \texttt{ready} flag.
  \item However, this code is incorrect due to race conditions caused by lack of atomicity in flag checks and updates, potentially leading to simultaneous access to the buffer by both \texttt{push} and \texttt{pop}.
\end{itemize}

\subsubsection{Solution}

\begin{itemize}
    \item low-level: compiler intrinsics for ``atomic'' operations:
    combined operations that are performed as a single unit
    no thread will every see the memory in an intermediate state
    
    \item high-level: use libraries that correctly implement some primitives:
    locks, queues, etc.
    \begin{itemize}
        \item Posix threads (``pthreads''; Linux, MacOS)
        \item OpenMP (Open Multi-Processing; portable)
    \end{itemize}
\end{itemize}

\subsection{Distributed Computing}


\begin{itemize}
  \item In distributed computing, processes do not share memory
  \item They must communicate by explicitly sending data to each other (\texttt{send()}, \texttt{recv()}, etc.) typically over the network
\end{itemize}


\begin{itemize}
  \item \textbf{Con:} Communication is much slower than multithreading
  \item \textbf{Pros:}
  \begin{itemize}
    \item Easier to implement and reason about
    \item Scales to higher levels of parallelism
    \begin{itemize}
      \item As of today, off-the-shelf computers can have up to 2 processors $\times$ 128 cores $\times$ 2 SMT threads = 512 concurrent software threads
      \item With distributed computing, networked computers can work together in parallel
    \end{itemize}
  \end{itemize}
  \item \textbf{Libraries:}
  \begin{itemize}
    \item Message Passing Interface (MPI)
    \item \ldots % This represents an ellipsis where additional libraries might be listed
  \end{itemize}
\end{itemize}

\subsection{4. Hardware Acceleration}
\subsubsection{Graphics processing units (GPUs)}

\begin{itemize}
  \item GPUs were designed to perform the same simple, repetitive operations
  \begin{itemize}
      \item on many pixels (``fragment shaders''), or
      \item on many 3D coordinates (``vertex shaders'')
  \end{itemize}
\end{itemize}

\subsubsection{Examples (GLSL)}

\begin{lstlisting}
float box(in vec2 st, in vec2 size){
    size = vec2(0.5) - size*0.5;
    vec2 uv = smoothstep(size,
                         size+vec2(0.01),
                         st);
    uv *= smoothstep(size,
                     size+vec2(0.01),
                     vec2(1.0)-st);
    return uv.x*uv.y;
}
\end{lstlisting}


\subsubsection{Examples (CUDA)}

\begin{lstlisting}
inline __device__ float3 roundAndExpand(float3 v, ushort *w) {
    v.x = rintf(__saturatef(v.x) * 31.0f);
    v.y = rintf(__saturatef(v.y) * 63.0f);
    v.z = rintf(__saturatef(v.z) * 31.0f);
    
    *w = ((ushort)(v.x) << 11) | ((ushort)(v.y << 5)) | (ushort)v.z;
    // approximate integer bit expansion.
    v.x = 0.032258064516f;
    v.y = 0.015873015873f;
    v.z = 0.032258064516f;
    return v;
}
\end{lstlisting}

\textbullet\ GPUs were designed to perform the same simple, repetitive operations
\begin{itemize}
    \item on many pixels ("fragment shaders"), or
    \item on many 3D coordinates ("vertex shaders")
\end{itemize}
\textbullet\ they generally adopt a SIMT ("single instruction, multiple threads") model
\begin{itemize}
    \item hundreds of threads working on different sets of data
    \item but running the exact same instructions
\end{itemize}
\textbullet\ good fit for long loops performing repetitive operations \\
\textbullet\ bad fit for if/then/else

\subsubsection{How do we use GPUs?}

\textbullet\ GPUs are programmed in special-purpose languages

\textbullet\ Typically, all GPU code is compiled
\begin{itemize}
    \item during application startup,
    \item by the device driver
    \item for the specific GPU device installed (amount and subdivision of threads, memory, etc.)
\end{itemize}

\textbullet\ Two dominant players in the GPU market: nVidia and AMD

\textbullet\ Three major GPU programming languages:
\begin{itemize}
    \item CUDA (nVidia, proprietary),
    \item ROCm (AMD, open-source),
    \item OpenCL (cross-platform, open-source)
\end{itemize}
\subsubsection{Matrix Multiplication Tutorial}

\paragraph{N = 8192}
8192 $\times$ 8192 matrix multiplication \\
precision: fp64 (``double") \\
CPU: AMD Ryzen 7900 x3d

\begin{tabular}{l l l}
matmul\_1 & straightforward implementation & 2932.059 s \quad 1x \\
matmul\_2 & transpose B matrix & 357.569 s \quad 8x \\
matmul\_3 & block multiply & 67.105 s \quad 44x \\
matmul\_4 & same code as matmul\_3, SIMD & 32.876 s \quad 89x \\
matmul\_5 & OpenBLAS & 15.555 s \quad 188x 1x \\
matmul\_6 & OpenBLAS, 24 threads & 1.962 s \quad 1494x 8x \\
\end{tabular}

\paragraph{N = 32768}
32768 $\times$ 32768 matrix multiplication \\
precision: fp32 (``float") - total 4 GB per matrix \\
CPU Ryzen 7900 x3d (released Feb 2023)

\begin{tabular}{l l l}
matmul\_7 & OpenBLAS, 1 thread & 550.350 s \quad 1x \\
matmul\_8 & OpenBLAS, 24 threads & 50.577 s \quad 11x \\
matmul\_9 & cuBLAS, \textit{nVidia} A100 (Apr 2021) & 13.152 s \quad 42x \\
matmul\_9 & cuBLAS, \textit{nVidia} H100 (Mar 2022) & ? s \quad 84x? \\
\end{tabular}
\end{document}